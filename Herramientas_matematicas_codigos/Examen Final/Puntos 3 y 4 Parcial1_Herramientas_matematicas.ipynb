{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1de_JcYFjwUeuzdt0bLr4T6YLAQzSS5ZW","timestamp":1664238938215}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **DATAET PARA LAS PRACTICAS DE LOS PUNTO 3 Y 4 DEL EXAMEN FINAL**"],"metadata":{"id":"lBOY91b1lwEd"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import requests\n","import re \n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","\n","\n","datos = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00504/qsar_fish_toxicity.csv\",header=None,sep=';',names=['CIC0','SM1_Dz(Z)','GATS1i','NdsCH','NdssC','MLOGP','quantitative response'])\n","datos.head(9)"],"metadata":{"id":"SU_Yp6unlwdb","colab":{"base_uri":"https://localhost:8080/","height":331},"executionInfo":{"status":"ok","timestamp":1664746835308,"user_tz":300,"elapsed":1871,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"5e1e5895-7343-4cf9-c6a3-0f1b2e30c6fb"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    CIC0  SM1_Dz(Z)  GATS1i  NdsCH  NdssC  MLOGP  quantitative response\n","0  3.260      0.829   1.676      0      1  1.453                  3.770\n","1  2.189      0.580   0.863      0      0  1.348                  3.115\n","2  2.125      0.638   0.831      0      0  1.348                  3.531\n","3  3.027      0.331   1.472      1      0  1.807                  3.510\n","4  2.094      0.827   0.860      0      0  1.886                  5.390\n","5  3.222      0.331   2.177      0      0  0.706                  1.819\n","6  3.179      0.000   1.063      0      0  2.942                  3.947\n","7  3.000      0.000   0.938      1      0  2.851                  3.513\n","8  2.620      0.499   0.990      0      0  2.942                  4.402"],"text/html":["\n","  <div id=\"df-067cc355-8385-41a0-a9f9-c7a687e73252\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CIC0</th>\n","      <th>SM1_Dz(Z)</th>\n","      <th>GATS1i</th>\n","      <th>NdsCH</th>\n","      <th>NdssC</th>\n","      <th>MLOGP</th>\n","      <th>quantitative response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.260</td>\n","      <td>0.829</td>\n","      <td>1.676</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.453</td>\n","      <td>3.770</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.189</td>\n","      <td>0.580</td>\n","      <td>0.863</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.348</td>\n","      <td>3.115</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.125</td>\n","      <td>0.638</td>\n","      <td>0.831</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.348</td>\n","      <td>3.531</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.027</td>\n","      <td>0.331</td>\n","      <td>1.472</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1.807</td>\n","      <td>3.510</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.094</td>\n","      <td>0.827</td>\n","      <td>0.860</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.886</td>\n","      <td>5.390</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3.222</td>\n","      <td>0.331</td>\n","      <td>2.177</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.706</td>\n","      <td>1.819</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>3.179</td>\n","      <td>0.000</td>\n","      <td>1.063</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.942</td>\n","      <td>3.947</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3.000</td>\n","      <td>0.000</td>\n","      <td>0.938</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2.851</td>\n","      <td>3.513</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2.620</td>\n","      <td>0.499</td>\n","      <td>0.990</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.942</td>\n","      <td>4.402</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-067cc355-8385-41a0-a9f9-c7a687e73252')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-067cc355-8385-41a0-a9f9-c7a687e73252 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-067cc355-8385-41a0-a9f9-c7a687e73252');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# **PUNTO 3 EXAMEN FINAL HM_IA**"],"metadata":{"id":"V4B2DVwk4p4g"}},{"cell_type":"markdown","source":["### **FUNCIONES DESCENSO DE GRADIENTE**"],"metadata":{"id":"3rEbANlj98YR"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","#variables to store mean and standard deviation for each feature\n","mu = []\n","std = []\n","\n","def load_data(filename):\n","\tdf = pd.read_csv(filename, sep=\",\", index_col=False)\n","\tdf.columns = [\"housesize\", \"rooms\", \"price\"]\n","\tdata = np.array(df, dtype=float)\n","\tplot_data(data[:,:2], data[:, -1])\n","\tnormalize(data)\n","\treturn data[:,:2], data[:, -1]\n","\n","def plot_data(x, y):\n","\tplt.xlabel('house size')\n","\tplt.ylabel('price')\n","\tplt.plot(x[:,0], y, 'bo')\n","\tplt.show()\n","\n","def normalize(data):\n","\tfor i in range(0,data.shape[1]-1):\n","\t\tdata[:,i] = ((data[:,i] - np.mean(data[:,i]))/np.std(data[:, i]))\n","\t\tmu.append(np.mean(data[:,i]))\n","\t\tstd.append(np.std(data[:, i]))\n","\n","\n","def h(x,theta):\n","\treturn np.matmul(x, theta)\n","\n","def cost_function(x, y, theta):\n","\treturn ((h(x, theta)-y).T@(h(x, theta)-y))/(2*y.shape[0])\n","\n","def gradient_descent(x, y, theta, learning_rate=0.1, num_epochs=10):\n","\tm = x.shape[0]\n","\tJ_all = []\n","\t\n","\tfor _ in range(num_epochs):\n","\t\th_x = h(x, theta)\n","\t\tcost_ = (1/m)*(x.T@(h_x - y))\n","\t\ttheta = theta - (learning_rate)*cost_\n","\t\tJ_all.append(cost_function(x, y, theta))\n","\n","\treturn theta, J_all \n","\n","def plot_cost(J_all, num_epochs):\n","\tplt.xlabel('Epochs')\n","\tplt.ylabel('Cost')\n","\tplt.plot(num_epochs, J_all, 'm', linewidth = \"5\")\n","\tplt.show()\n","\n","def test(theta, x):\n","\n","\ty = theta[0] + theta[1]*x[0] + theta[2]*x[1]\n","\tprint(\"Price of house: \", y)\n","\n","def normalized(x, xmax, xmin):\n","  ymax = 1\n","  ymin = 0\n","\n","  m = (ymax - ymin) / (xmax - xmin)\n","  b = ymin - m*xmin\n","\n","  y = m*x+b\n","  \n","  return y"],"metadata":{"id":"3wP4gson96qq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **MODELO 1 ORIGINAL**"],"metadata":{"id":"EfXYixdEpwno"}},{"cell_type":"markdown","source":["### **CREACION DEL MODELO**"],"metadata":{"id":"qZ4wq-9QCX1k"}},{"cell_type":"code","source":["plt.rcParams['figure.figsize'] = [20, 5]\n","\n","x1,y1 = load_data(\"https://raw.githubusercontent.com/kumudlakara/Medium-codes/main/linear_regression/house_price_data.txt\")\n","y = np.reshape(y1, (46,1))\n","x = np.hstack((np.ones((x1.shape[0],1)), x1))\n","theta = np.zeros((x.shape[1], 1))\n","learning_rate = 0.1\n","num_epochs = 50\n","theta, J_all = gradient_descent(x, y, theta, learning_rate, num_epochs)\n","J = cost_function(x, y, theta)\n","print(\"Cost: \", J)\n","print(\"Parameters: \", theta)"],"metadata":{"id":"VVrhevUI9ovj","colab":{"base_uri":"https://localhost:8080/","height":403},"executionInfo":{"status":"ok","timestamp":1664239098515,"user_tz":300,"elapsed":16,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"66646c3a-ec97-4534-f547-7f80b193cf19"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Cost:  [[2.08475311e+09]]\n","Parameters:  [[ 3.37371711e+05]\n"," [ 1.04066024e+05]\n"," [-6.47874139e+01]]\n"]}]},{"cell_type":"markdown","source":["### **ENTRENAMIENTO DE LA FUNCION DE COSTO**"],"metadata":{"id":"4i76EQFSqk6J"}},{"cell_type":"code","source":["#for testing and plotting cost \n","n_epochs = []\n","jplot = []\n","count = 0\n","for i in J_all:\n","\tjplot.append(i[0][0])\n","\tn_epochs.append(count)\n","\tcount += 1\n","jplot = np.array(jplot)\n","n_epochs = np.array(n_epochs)\n","plot_cost(jplot, n_epochs)"],"metadata":{"id":"XEoKja93qSJy","colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"status":"ok","timestamp":1664239098515,"user_tz":300,"elapsed":14,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"0fa11193-222a-4c43-a286-6b41d36256ef"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### **VALIDACION MODELO 1 ORIGINAL**"],"metadata":{"id":"amJNDY96Vua7"}},{"cell_type":"code","source":["Y_pred_modelo1 = np.dot(x,theta)\n","y_n=np.asarray(normalized(y,np.max(y),np.min(y)))\n","Y_pred_modelo1_n=np.asarray(normalized(Y_pred_modelo1,np.max(Y_pred_modelo1),np.min(Y_pred_modelo1)))\n","\n","RMSE = (np.sqrt(mean_squared_error(y_n,Y_pred_modelo1_n)))\n","\n","regr = LinearRegression()\n","regr.fit(y_n,Y_pred_modelo1_n)\n","coef = regr.coef_\n","\n","print(RMSE)\n","print(coef)"],"metadata":{"id":"UamBIlv8VvDD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239098797,"user_tz":300,"elapsed":294,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"1a14ddd0-9ec3-473b-b9ed-461fa5522ef2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.12300623907344257\n","[[0.79716283]]\n"]}]},{"cell_type":"markdown","source":["## **MODELO ANTERIOR APLICADO AL DATASET SELECCIONADO**"],"metadata":{"id":"X3WkZqZTri7h"}},{"cell_type":"markdown","source":["### **CREACION DEL MODELO DE GRADIENTE DECENDENTE**"],"metadata":{"id":"oGFW7tM0r442"}},{"cell_type":"code","source":["#Extraccion de datos\n","X = datos.iloc[:,0:6]\n","Y = datos.iloc[:,6]\n","#Normalizacion de los datos, y amuento de dimensionalidad de X\n","X_n1=np.asarray(normalized(X,np.max(X),np.min(X)))\n","X_n = np.hstack((np.ones((X_n1.shape[0],1)), X_n1))\n","Y_n = Y.values\n","Y_n = Y_n.reshape(-1,1)\n","Y_n=np.asarray(normalized(Y_n,np.max(Y_n),np.min(Y_n)))\n","\n","#plot de graficas de cada entrada vs salida\n","plt.rcParams['figure.figsize'] = [20, 5]\n","\n","for i in range(0,(X.shape[1])):\n","  plt.subplot(2,3,i+1)\n","  plt.scatter(X_n1[:,i], Y_n)\n","\n","#variables constantes y calculos del modelo \n","theta = np.zeros((X_n.shape[1], 1))\n","learning_rate = 0.1\n","num_epochs = 900\n","theta, J_all = gradient_descent(X_n, Y_n, theta, learning_rate, num_epochs)\n","J = cost_function(X_n, Y_n, theta)\n","print(\"Cost: \", J)\n","print(\"Parameters: \", theta)"],"metadata":{"id":"_2amtW65cgBp","colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"status":"ok","timestamp":1664239099777,"user_tz":300,"elapsed":985,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"65ef3d63-0a7e-44a0-8867-ac9a257e272a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cost:  [[0.0049175]]\n","Parameters:  [[ 0.12072654]\n"," [ 0.18036226]\n"," [ 0.26590766]\n"," [-0.19484062]\n"," [ 0.14817678]\n"," [ 0.05989124]\n"," [ 0.37976587]]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x360 with 6 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### **ENTRENAMIENTO DE LA FUNCION DE COSTO**"],"metadata":{"id":"EHvjaqursa7D"}},{"cell_type":"code","source":["#for testing and plotting cost \n","n_epochs = []\n","jplot = []\n","count = 0\n","for i in J_all:\n","\tjplot.append(i[0][0])\n","\tn_epochs.append(count)\n","\tcount += 1\n","jplot = np.array(jplot)\n","n_epochs = np.array(n_epochs)\n","plot_cost(jplot, n_epochs)"],"metadata":{"id":"a8ECkpV67ezY","colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"status":"ok","timestamp":1664239100139,"user_tz":300,"elapsed":368,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"d09d1b28-b079-4960-ac3f-88c79509e3ed"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### **VALIDACION MODELO 1 DATASET ELEGIDO**"],"metadata":{"id":"oTkQdM0n0G4p"}},{"cell_type":"code","source":["Y_pred_modelo2 = np.dot(X_n,theta)\n","RMSE = (np.sqrt(mean_squared_error(Y_n,Y_pred_modelo2)))\n","\n","regr = LinearRegression()\n","regr.fit(Y_n,Y_pred_modelo2)\n","coef = regr.coef_\n","\n","print(RMSE)\n","print(coef)"],"metadata":{"id":"vjJWCvnE0HLJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239100140,"user_tz":300,"elapsed":26,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"68850ecf-7d8d-492b-8339-e988899481bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.09917152923080635\n","[[0.55369464]]\n"]}]},{"cell_type":"markdown","source":["## **MODELO UTILIZANDO LINEAR REGRESSION DE LA LIBRERIA SKLEARN**"],"metadata":{"id":"P-tEImGoTV9Y"}},{"cell_type":"markdown","source":["### **CREACION DEL MODELO DE REGRESION LINEAL**"],"metadata":{"id":"aBE4jhRCpSS5"}},{"cell_type":"code","source":["#Extraccion de datos\n","X = datos.iloc[:,0:6]\n","Y = datos.iloc[:,6]\n","\n","#Normalizacion de los datos\n","scaler1 = MinMaxScaler(feature_range=(-1, 1))\n","scaler1.fit(X)\n","X_n=scaler1.transform(X)\n","X_n = np.array(X_n)\n","\n","scaler2 = MinMaxScaler(feature_range=(-1, 1))\n","Y_n = Y.values\n","Y_n = Y_n.reshape(-1,1)\n","\n","scaler2.fit(Y_n)\n","Y_n=scaler2.transform(Y_n)\n","Y_n = np.array(Y_n)\n","\n","#Division del datoset normalizado\n","X_train, X_test, Y_train, Y_test = train_test_split(X_n, Y_n, test_size = 0.2, random_state=5)\n","\n","#Entrenamiento del modelo por LinearRegression\n","linear_model = LinearRegression()"],"metadata":{"id":"UjCotBMtpREP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **ENTRENAMIENTO DEL MODELO LINEAR_REGRESSION**"],"metadata":{"id":"w5OZhZBfIyh_"}},{"cell_type":"code","source":["linear_model.fit(X_train, Y_train)"],"metadata":{"id":"flGuSfcGIzJj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239100141,"user_tz":300,"elapsed":24,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"3875968c-eb75-4f10-c32d-87959bfd85d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearRegression()"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["### **VALIDACION MODELO LINEAR_REGRESSION CON EL DATASET ELEGIDO**"],"metadata":{"id":"JUUn7ZxHIf2V"}},{"cell_type":"code","source":["R2 = linear_model.score(X_test,Y_test)\n","y_test_predict = linear_model.predict(X_test)\n","RMSE = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n","\n","regr = LinearRegression()\n","regr.fit(Y_test,y_test_predict)\n","coef = regr.coef_\n","\n","print('Rmse ',RMSE)\n","print(coef)"],"metadata":{"id":"KUrbcuvVIgGI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239100142,"user_tz":300,"elapsed":22,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"3351d580-251a-4234-893a-9ff4d46047a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Rmse  0.19026400833792062\n","[[0.56235088]]\n"]}]},{"cell_type":"markdown","source":["## **MODELO SGD**"],"metadata":{"id":"ZxkiVZFRMbfo"}},{"cell_type":"markdown","source":["### **CREACION DEL MODELO SGD**"],"metadata":{"id":"P0vsR8wuN7Gq"}},{"cell_type":"code","source":["from sklearn.linear_model import SGDRegressor\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","reg = make_pipeline(StandardScaler(),\n","                    SGDRegressor(max_iter=10000, tol=1e-3))\n","#SVMreg = svm.SVR(kernel='linear')"],"metadata":{"id":"P5_5komHMtIr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gradient_descent(x, y, theta, learning_rate=0.1, num_epochs=10):\n","\tm = x.shape[0]\n","\tJ_all = []\n","\t\n","\tfor _ in range(num_epochs):\n","\t\th_x = h(x, theta)\n","\t\tcost_ = (1/m)*(x.T@(h_x - y))*(1/(math.sqrt((1/m)*(x.T@(h_x - y))+1)+1e-6))\n","\t\ttheta = theta - (learning_rate)*cost_\n","\t\tJ_all.append(cost_function(x, y, theta))\n","\n","\treturn theta, J_all "],"metadata":{"id":"9XJk7bne72Ql"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **ENTRENAMIENTO DEL MODELO SGD**"],"metadata":{"id":"GRx0aF41OCAH"}},{"cell_type":"code","source":["reg.fit(X_train, Y_train)"],"metadata":{"id":"1aAQe2r4N6TI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239100146,"user_tz":300,"elapsed":21,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"71cec66f-5ce1-493c-f854-c3852b0306a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('standardscaler', StandardScaler()),\n","                ('sgdregressor', SGDRegressor(max_iter=10000))])"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### **VALIDACION DEL MODELO SGD**"],"metadata":{"id":"MeHbey8jOGoO"}},{"cell_type":"code","source":["Y_pred_SGD = reg.predict(X_test)\n","\n","RMSE = (np.sqrt(mean_squared_error(Y_test, Y_pred_SGD)))\n","regr = LinearRegression()\n","regr.fit(Y_test,Y_pred_SGD)\n","coef = regr.coef_\n","\n","print('Rmse ',RMSE)\n","print(coef)"],"metadata":{"id":"6_V1Ef6UM_s3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239100147,"user_tz":300,"elapsed":20,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"1d76c24c-b2b4-47c1-ddc8-3ef88b99d8ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Rmse  0.19023169250678557\n","[0.56069107]\n"]}]},{"cell_type":"markdown","source":["# **PUNTO 4 EXAMEN FINAL HM_IA**"],"metadata":{"id":"8sRYaWgOpypd"}},{"cell_type":"markdown","source":["### **DEFINICION DE FUNCIONES PARA EL DESARROLLO DE LA MLP CON PROPAGRACION DE ERROR HACIA ATRAS**"],"metadata":{"id":"f6KzDk1D1L1_"}},{"cell_type":"code","source":["from sklearn.datasets import make_circles\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","import math\n","np.random.seed(0)\n","\n","# Find a small float to avoid division by zero\n","epsilon = np.finfo(float).eps\n","\n","# Sigmoid function and its differentiation\n","def sigmoid(z):\n","    return 1/(1+np.exp(-z.clip(-500, 500)))\n","def dsigmoid(z):\n","    s = sigmoid(z)\n","    return 2 * s * (1-s)\n","\n","# ReLU function and its differentiation\n","def relu(z):\n","    return np.maximum(0, z)\n","def drelu(z):\n","    return (z > 0).astype(float)\n","\n","# Loss function L(y, yhat) and its differentiation\n","def cross_entropy(y, yhat):\n","    \"\"\"Binary cross entropy function\n","        L = - y log yhat - (1-y) log (1-yhat)\n","\n","    Args:\n","        y, yhat (np.array): nx1 matrices which n are the number of data instances\n","    Returns:\n","        average cross entropy value of shape 1x1, averaging over the n instances\n","    \"\"\"\n","    return -(y.T @ np.log(yhat.clip(epsilon)) + (1-y.T) @ np.log((1-yhat).clip(epsilon))) / y.shape[1]\n","\n","def d_cross_entropy(y, yhat):\n","    \"\"\" dL/dyhat \"\"\"\n","    return - np.divide(y, yhat.clip(epsilon)) + np.divide(1-y, (1-yhat).clip(epsilon))\n","\n","class mlp:\n","    '''Multilayer perceptron using numpy\n","    '''\n","    def __init__(self, layersizes, activations, derivatives, lossderiv):\n","        \"\"\"remember config, then initialize array to hold NN parameters without init\"\"\"\n","        # hold NN config\n","        self.layersizes = tuple(layersizes)\n","        self.activations = tuple(activations)\n","        self.derivatives = tuple(derivatives)\n","        self.lossderiv = lossderiv\n","        assert len(self.layersizes)-1 == len(self.activations), \\\n","            \"number of layers and the number of activation functions does not match\"\n","        assert len(self.activations) == len(self.derivatives), \\\n","            \"number of activation functions and number of derivatives does not match\"\n","        assert all(isinstance(n, int) and n >= 1 for n in layersizes), \\\n","            \"Only positive integral number of perceptons is allowed in each layer\"\n","        # parameters, each is a 2D numpy array\n","        L = len(self.layersizes)\n","        self.z = [None] * L\n","        self.W = [None] * L\n","        self.b = [None] * L\n","        self.a = [None] * L\n","        self.dz = [None] * L\n","        self.dW = [None] * L\n","        self.db = [None] * L\n","        self.da = [None] * L\n","\n","    def initialize(self, seed=42):\n","        \"\"\"initialize the value of weight matrices and bias vectors with small random numbers.\"\"\"\n","        np.random.seed(seed)\n","        sigma = 0.1\n","        for l, (insize, outsize) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):\n","            self.W[l] = np.random.randn(insize, outsize) * sigma\n","            self.b[l] = np.random.randn(1, outsize) * sigma\n","\n","    def forward(self, x):\n","        \"\"\"Feed forward using existing `W` and `b`, and overwrite the result variables `a` and `z`\n","\n","        Args:\n","            x (numpy.ndarray): Input data to feed forward\n","        \"\"\"\n","        self.a[0] = x\n","        for l, func in enumerate(self.activations, 1):\n","            # z = W a + b, with `a` as output from previous layer\n","            # `W` is of size rxs and `a` the size sxn with n the number of data instances, `z` the size rxn\n","            # `b` is rx1 and broadcast to each column of `z`\n","            self.z[l] = (self.a[l-1] @ self.W[l]) + self.b[l]\n","            # a = g(z), with `a` as output of this layer, of size rxn\n","            self.a[l] = func(self.z[l])\n","        return self.a[-1]\n","\n","    def backward(self, y, yhat):\n","        \"\"\"back propagation using NN output yhat and the reference output y, generates dW, dz, db,\n","        da\n","        \"\"\"\n","        assert y.shape[1] == self.layersizes[-1], \"Output size doesn't match network output size\"\n","        assert y.shape == yhat.shape, \"Output size doesn't match reference\"\n","        # first `da`, at the output\n","        self.da[-1] = self.lossderiv(y, yhat)\n","        for l, func in reversed(list(enumerate(self.derivatives, 1))):\n","            # compute the differentials at this layer\n","            self.dz[l] = self.da[l] * func(self.z[l])\n","            self.dW[l] = self.a[l-1].T @ self.dz[l]\n","            self.db[l] = np.mean(self.dz[l], axis=0, keepdims=True)\n","            self.da[l-1] = self.dz[l] @ self.W[l].T\n","            assert self.z[l].shape == self.dz[l].shape\n","            assert self.W[l].shape == self.dW[l].shape\n","            assert self.b[l].shape == self.db[l].shape\n","            assert self.a[l].shape == self.da[l].shape\n","\n","    def update(self, eta):\n","        \"\"\"Updates W and b\n","\n","        Args:\n","            eta (float): Learning rate\n","        \"\"\"\n","        for l in range(1, len(self.W)):\n","            self.W[l] -= eta * self.dW[l]\n","            self.b[l] -= eta * self.db[l]\n"],"metadata":{"id":"cjpjUvZ63HBB","executionInfo":{"status":"ok","timestamp":1664746841124,"user_tz":300,"elapsed":249,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## **MODELO COPIADO DIRECTAMENTE DE LA PAGINA**"],"metadata":{"id":"j_2Tcf4k2Ban"}},{"cell_type":"markdown","source":["### **CREACION DEL MODELO**"],"metadata":{"id":"3wV1ITyp1g69"}},{"cell_type":"code","source":["# Make data: Two circles on x-y plane as a classification problem\n","X, y = make_circles(n_samples=908, factor=0.5, noise=0.1)\n","y = y.reshape(-1,1) # our model expects a 2D array of (n_sample, n_dim)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n","\n","# Build a model\n","model = mlp(layersizes=[2, 4, 3, 1],\n","            activations=[relu, relu, sigmoid],\n","            derivatives=[drelu, drelu, dsigmoid],\n","            lossderiv=d_cross_entropy)\n","model.initialize()\n","yhat = model.forward(X_train)\n","print(yhat.shape,X.shape)\n","loss = cross_entropy(Y_train, yhat)\n","print(\"Before training - loss value {} accuracy {}\".format(loss, accuracy_score(Y_train, (yhat > 0.5))))"],"metadata":{"id":"EXvbUPW5r-gk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664746844238,"user_tz":300,"elapsed":256,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"4357ec50-e21c-465e-f6c7-99fd44f01809"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["(726, 1) (908, 2)\n","Before training - loss value [[502.73019238]] accuracy 0.5192837465564738\n"]}]},{"cell_type":"markdown","source":["### **ENTRENAMIENTO DEL MODELO**"],"metadata":{"id":"8LyGxxSc1rVW"}},{"cell_type":"code","source":["# train for each epoch\n","n_epochs = 20000\n","learning_rate = 0.0005\n","for n in range(n_epochs):\n","    model.forward(X_train)\n","    yhat = model.a[-1]\n","    model.backward(Y_train, yhat)\n","    model.update(learning_rate)\n","    loss = cross_entropy(Y_train, yhat)\n","    print(\"Iteration {} - loss value {} accuracy {}\".format(n, loss, accuracy_score(Y_train, (yhat > 0.5))))"],"metadata":{"id":"D4niqP9Y1fZf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664746891368,"user_tz":300,"elapsed":44855,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"8b4d222f-346a-41b8-d7f3-a7eaaf90b763"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las ltimas 5000 lneas del flujo de salida.\u001b[0m\n","Iteration 15000 - loss value [[330.27247359]] accuracy 0.7837465564738292\n","Iteration 15001 - loss value [[333.83486996]] accuracy 0.7727272727272727\n","Iteration 15002 - loss value [[329.49976156]] accuracy 0.78099173553719\n","Iteration 15003 - loss value [[331.88478635]] accuracy 0.7713498622589532\n","Iteration 15004 - loss value [[331.13792869]] accuracy 0.7823691460055097\n","Iteration 15005 - loss value [[332.23451235]] accuracy 0.7754820936639119\n","Iteration 15006 - loss value [[329.64967511]] accuracy 0.7851239669421488\n","Iteration 15007 - loss value [[331.82063417]] accuracy 0.7727272727272727\n","Iteration 15008 - loss value [[329.67006582]] accuracy 0.7823691460055097\n","Iteration 15009 - loss value [[332.71922846]] accuracy 0.7754820936639119\n","Iteration 15010 - loss value [[328.3838659]] accuracy 0.7851239669421488\n","Iteration 15011 - loss value [[327.27125497]] accuracy 0.7837465564738292\n","Iteration 15012 - loss value [[329.34135827]] accuracy 0.78099173553719\n","Iteration 15013 - loss value [[328.94115532]] accuracy 0.7796143250688705\n","Iteration 15014 - loss value [[329.72711212]] accuracy 0.7837465564738292\n","Iteration 15015 - loss value [[328.98313289]] accuracy 0.7796143250688705\n","Iteration 15016 - loss value [[329.57039325]] accuracy 0.7823691460055097\n","Iteration 15017 - loss value [[327.64033241]] accuracy 0.7796143250688705\n","Iteration 15018 - loss value [[326.67839046]] accuracy 0.7823691460055097\n","Iteration 15019 - loss value [[326.15555193]] accuracy 0.7823691460055097\n","Iteration 15020 - loss value [[325.66450942]] accuracy 0.7878787878787878\n","Iteration 15021 - loss value [[325.94461351]] accuracy 0.7837465564738292\n","Iteration 15022 - loss value [[328.16287114]] accuracy 0.7796143250688705\n","Iteration 15023 - loss value [[332.79103423]] accuracy 0.7823691460055097\n","Iteration 15024 - loss value [[331.97039029]] accuracy 0.7796143250688705\n","Iteration 15025 - loss value [[336.39695357]] accuracy 0.78099173553719\n","Iteration 15026 - loss value [[347.08372719]] accuracy 0.7644628099173554\n","Iteration 15027 - loss value [[359.2053572]] accuracy 0.7672176308539945\n","Iteration 15028 - loss value [[407.3306123]] accuracy 0.6955922865013774\n","Iteration 15029 - loss value [[330.96732746]] accuracy 0.7754820936639119\n","Iteration 15030 - loss value [[326.66273144]] accuracy 0.78099173553719\n","Iteration 15031 - loss value [[327.16377266]] accuracy 0.7851239669421488\n","Iteration 15032 - loss value [[329.20678222]] accuracy 0.778236914600551\n","Iteration 15033 - loss value [[333.24079562]] accuracy 0.78099173553719\n","Iteration 15034 - loss value [[331.01649074]] accuracy 0.7796143250688705\n","Iteration 15035 - loss value [[334.30443256]] accuracy 0.7837465564738292\n","Iteration 15036 - loss value [[343.4782519]] accuracy 0.768595041322314\n","Iteration 15037 - loss value [[339.72964743]] accuracy 0.7768595041322314\n","Iteration 15038 - loss value [[358.5078372]] accuracy 0.7493112947658402\n","Iteration 15039 - loss value [[326.52332737]] accuracy 0.7851239669421488\n","Iteration 15040 - loss value [[325.75844804]] accuracy 0.7851239669421488\n","Iteration 15041 - loss value [[326.28635398]] accuracy 0.7851239669421488\n","Iteration 15042 - loss value [[328.74642796]] accuracy 0.7837465564738292\n","Iteration 15043 - loss value [[327.55223036]] accuracy 0.78099173553719\n","Iteration 15044 - loss value [[329.461313]] accuracy 0.7878787878787878\n","Iteration 15045 - loss value [[332.42656669]] accuracy 0.7727272727272727\n","Iteration 15046 - loss value [[327.34802162]] accuracy 0.7851239669421488\n","Iteration 15047 - loss value [[327.19557302]] accuracy 0.78099173553719\n","Iteration 15048 - loss value [[330.49150432]] accuracy 0.7823691460055097\n","Iteration 15049 - loss value [[330.70107382]] accuracy 0.7754820936639119\n","Iteration 15050 - loss value [[335.89373271]] accuracy 0.7837465564738292\n","Iteration 15051 - loss value [[353.72271719]] accuracy 0.7548209366391184\n","Iteration 15052 - loss value [[327.36216883]] accuracy 0.7837465564738292\n","Iteration 15053 - loss value [[331.411244]] accuracy 0.7741046831955923\n","Iteration 15054 - loss value [[330.30241566]] accuracy 0.7851239669421488\n","Iteration 15055 - loss value [[331.54091282]] accuracy 0.7768595041322314\n","Iteration 15056 - loss value [[328.2774671]] accuracy 0.7865013774104683\n","Iteration 15057 - loss value [[327.23389463]] accuracy 0.7823691460055097\n","Iteration 15058 - loss value [[329.18370693]] accuracy 0.78099173553719\n","Iteration 15059 - loss value [[328.68868631]] accuracy 0.7796143250688705\n","Iteration 15060 - loss value [[327.82054469]] accuracy 0.7837465564738292\n","Iteration 15061 - loss value [[327.02416709]] accuracy 0.78099173553719\n","Iteration 15062 - loss value [[327.37305627]] accuracy 0.7823691460055097\n","Iteration 15063 - loss value [[326.57955836]] accuracy 0.78099173553719\n","Iteration 15064 - loss value [[326.38424434]] accuracy 0.7837465564738292\n","Iteration 15065 - loss value [[326.17513976]] accuracy 0.7823691460055097\n","Iteration 15066 - loss value [[326.16392311]] accuracy 0.7865013774104683\n","Iteration 15067 - loss value [[327.71138797]] accuracy 0.7851239669421488\n","Iteration 15068 - loss value [[326.55278963]] accuracy 0.7837465564738292\n","Iteration 15069 - loss value [[327.15878836]] accuracy 0.7837465564738292\n","Iteration 15070 - loss value [[326.8972609]] accuracy 0.7837465564738292\n","Iteration 15071 - loss value [[328.48958384]] accuracy 0.78099173553719\n","Iteration 15072 - loss value [[327.42888245]] accuracy 0.7823691460055097\n","Iteration 15073 - loss value [[327.3383454]] accuracy 0.7837465564738292\n","Iteration 15074 - loss value [[326.13354163]] accuracy 0.7851239669421488\n","Iteration 15075 - loss value [[326.62152638]] accuracy 0.78099173553719\n","Iteration 15076 - loss value [[330.47799077]] accuracy 0.7823691460055097\n","Iteration 15077 - loss value [[332.62188463]] accuracy 0.7727272727272727\n","Iteration 15078 - loss value [[330.36068419]] accuracy 0.7823691460055097\n","Iteration 15079 - loss value [[328.37784463]] accuracy 0.778236914600551\n","Iteration 15080 - loss value [[327.4344039]] accuracy 0.7823691460055097\n","Iteration 15081 - loss value [[326.69658235]] accuracy 0.7837465564738292\n","Iteration 15082 - loss value [[326.19217681]] accuracy 0.78099173553719\n","Iteration 15083 - loss value [[326.73932267]] accuracy 0.7837465564738292\n","Iteration 15084 - loss value [[329.87533446]] accuracy 0.78099173553719\n","Iteration 15085 - loss value [[329.30485132]] accuracy 0.778236914600551\n","Iteration 15086 - loss value [[330.42877866]] accuracy 0.7851239669421488\n","Iteration 15087 - loss value [[327.49734085]] accuracy 0.7796143250688705\n","Iteration 15088 - loss value [[326.09703897]] accuracy 0.7837465564738292\n","Iteration 15089 - loss value [[326.33762418]] accuracy 0.7851239669421488\n","Iteration 15090 - loss value [[327.09342621]] accuracy 0.78099173553719\n","Iteration 15091 - loss value [[326.60447409]] accuracy 0.7851239669421488\n","Iteration 15092 - loss value [[327.45542577]] accuracy 0.78099173553719\n","Iteration 15093 - loss value [[325.97630021]] accuracy 0.7878787878787878\n","Iteration 15094 - loss value [[326.3116507]] accuracy 0.7823691460055097\n","Iteration 15095 - loss value [[326.10532131]] accuracy 0.7865013774104683\n","Iteration 15096 - loss value [[327.81177245]] accuracy 0.778236914600551\n","Iteration 15097 - loss value [[332.97941707]] accuracy 0.7796143250688705\n","Iteration 15098 - loss value [[330.79316803]] accuracy 0.78099173553719\n","Iteration 15099 - loss value [[332.17027262]] accuracy 0.7851239669421488\n","Iteration 15100 - loss value [[337.22973967]] accuracy 0.7741046831955923\n","Iteration 15101 - loss value [[336.82957224]] accuracy 0.7823691460055097\n","Iteration 15102 - loss value [[353.36196922]] accuracy 0.7548209366391184\n","Iteration 15103 - loss value [[326.13129058]] accuracy 0.7837465564738292\n","Iteration 15104 - loss value [[325.82436923]] accuracy 0.7851239669421488\n","Iteration 15105 - loss value [[326.28896934]] accuracy 0.7851239669421488\n","Iteration 15106 - loss value [[327.57363059]] accuracy 0.78099173553719\n","Iteration 15107 - loss value [[333.57682177]] accuracy 0.7768595041322314\n","Iteration 15108 - loss value [[342.89018386]] accuracy 0.7672176308539945\n","Iteration 15109 - loss value [[342.9052538]] accuracy 0.7768595041322314\n","Iteration 15110 - loss value [[368.0490699]] accuracy 0.7424242424242424\n","Iteration 15111 - loss value [[328.9481815]] accuracy 0.7796143250688705\n","Iteration 15112 - loss value [[330.45322352]] accuracy 0.7754820936639119\n","Iteration 15113 - loss value [[328.6943109]] accuracy 0.778236914600551\n","Iteration 15114 - loss value [[334.21670007]] accuracy 0.7727272727272727\n","Iteration 15115 - loss value [[331.15153136]] accuracy 0.7837465564738292\n","Iteration 15116 - loss value [[336.25582998]] accuracy 0.7754820936639119\n","Iteration 15117 - loss value [[331.27201406]] accuracy 0.78099173553719\n","Iteration 15118 - loss value [[332.09531572]] accuracy 0.7768595041322314\n","Iteration 15119 - loss value [[332.83852993]] accuracy 0.7851239669421488\n","Iteration 15120 - loss value [[334.54889213]] accuracy 0.778236914600551\n","Iteration 15121 - loss value [[329.60380499]] accuracy 0.7823691460055097\n","Iteration 15122 - loss value [[332.20569648]] accuracy 0.7741046831955923\n","Iteration 15123 - loss value [[328.62805829]] accuracy 0.7837465564738292\n","Iteration 15124 - loss value [[330.83075963]] accuracy 0.7713498622589532\n","Iteration 15125 - loss value [[329.57926222]] accuracy 0.7796143250688705\n","Iteration 15126 - loss value [[331.92870497]] accuracy 0.7741046831955923\n","Iteration 15127 - loss value [[330.63455439]] accuracy 0.7837465564738292\n","Iteration 15128 - loss value [[332.11370586]] accuracy 0.7727272727272727\n","Iteration 15129 - loss value [[329.7506471]] accuracy 0.7823691460055097\n","Iteration 15130 - loss value [[330.98826859]] accuracy 0.7741046831955923\n","Iteration 15131 - loss value [[331.83115681]] accuracy 0.78099173553719\n","Iteration 15132 - loss value [[333.23954183]] accuracy 0.7741046831955923\n","Iteration 15133 - loss value [[332.30746412]] accuracy 0.78099173553719\n","Iteration 15134 - loss value [[329.88698563]] accuracy 0.7796143250688705\n","Iteration 15135 - loss value [[328.70938569]] accuracy 0.7851239669421488\n","Iteration 15136 - loss value [[327.39938208]] accuracy 0.78099173553719\n","Iteration 15137 - loss value [[327.09200647]] accuracy 0.78099173553719\n","Iteration 15138 - loss value [[326.1897441]] accuracy 0.7823691460055097\n","Iteration 15139 - loss value [[326.28020329]] accuracy 0.7837465564738292\n","Iteration 15140 - loss value [[327.94442253]] accuracy 0.78099173553719\n","Iteration 15141 - loss value [[329.66811228]] accuracy 0.7851239669421488\n","Iteration 15142 - loss value [[332.2747933]] accuracy 0.7754820936639119\n","Iteration 15143 - loss value [[329.65878412]] accuracy 0.7878787878787878\n","Iteration 15144 - loss value [[331.81354129]] accuracy 0.7754820936639119\n","Iteration 15145 - loss value [[328.87770661]] accuracy 0.78099173553719\n","Iteration 15146 - loss value [[328.18886742]] accuracy 0.7796143250688705\n","Iteration 15147 - loss value [[327.76279938]] accuracy 0.7851239669421488\n","Iteration 15148 - loss value [[328.60468722]] accuracy 0.7768595041322314\n","Iteration 15149 - loss value [[328.86899282]] accuracy 0.78099173553719\n","Iteration 15150 - loss value [[331.39580869]] accuracy 0.7727272727272727\n","Iteration 15151 - loss value [[330.12576726]] accuracy 0.7796143250688705\n","Iteration 15152 - loss value [[331.8994688]] accuracy 0.7754820936639119\n","Iteration 15153 - loss value [[335.80991431]] accuracy 0.7837465564738292\n","Iteration 15154 - loss value [[349.151852]] accuracy 0.7603305785123967\n","Iteration 15155 - loss value [[332.11189572]] accuracy 0.7851239669421488\n","Iteration 15156 - loss value [[335.84710228]] accuracy 0.7768595041322314\n","Iteration 15157 - loss value [[332.65707712]] accuracy 0.7823691460055097\n","Iteration 15158 - loss value [[334.98421873]] accuracy 0.7754820936639119\n","Iteration 15159 - loss value [[334.27448359]] accuracy 0.7851239669421488\n","Iteration 15160 - loss value [[343.16094467]] accuracy 0.7658402203856749\n","Iteration 15161 - loss value [[346.69299291]] accuracy 0.7741046831955923\n","Iteration 15162 - loss value [[378.13447348]] accuracy 0.7327823691460055\n","Iteration 15163 - loss value [[336.31107099]] accuracy 0.778236914600551\n","Iteration 15164 - loss value [[338.68937772]] accuracy 0.7699724517906336\n","Iteration 15165 - loss value [[333.6506308]] accuracy 0.78099173553719\n","Iteration 15166 - loss value [[337.03496796]] accuracy 0.7699724517906336\n","Iteration 15167 - loss value [[338.20186478]] accuracy 0.78099173553719\n","Iteration 15168 - loss value [[353.19003291]] accuracy 0.7548209366391184\n","Iteration 15169 - loss value [[326.36741197]] accuracy 0.7823691460055097\n","Iteration 15170 - loss value [[326.25990034]] accuracy 0.7837465564738292\n","Iteration 15171 - loss value [[326.87269359]] accuracy 0.7837465564738292\n","Iteration 15172 - loss value [[328.52571888]] accuracy 0.7796143250688705\n","Iteration 15173 - loss value [[331.47432494]] accuracy 0.7823691460055097\n","Iteration 15174 - loss value [[336.50560618]] accuracy 0.7741046831955923\n","Iteration 15175 - loss value [[333.1003466]] accuracy 0.7837465564738292\n","Iteration 15176 - loss value [[332.5151659]] accuracy 0.778236914600551\n","Iteration 15177 - loss value [[342.5983029]] accuracy 0.7741046831955923\n","Iteration 15178 - loss value [[365.24862047]] accuracy 0.7493112947658402\n","Iteration 15179 - loss value [[326.46184451]] accuracy 0.7837465564738292\n","Iteration 15180 - loss value [[326.13148281]] accuracy 0.7837465564738292\n","Iteration 15181 - loss value [[326.31681352]] accuracy 0.7865013774104683\n","Iteration 15182 - loss value [[328.96820772]] accuracy 0.778236914600551\n","Iteration 15183 - loss value [[334.69192589]] accuracy 0.7768595041322314\n","Iteration 15184 - loss value [[332.24864189]] accuracy 0.7796143250688705\n","Iteration 15185 - loss value [[335.52998482]] accuracy 0.7796143250688705\n","Iteration 15186 - loss value [[347.76008152]] accuracy 0.7603305785123967\n","Iteration 15187 - loss value [[342.3958312]] accuracy 0.7796143250688705\n","Iteration 15188 - loss value [[356.04683564]] accuracy 0.7520661157024794\n","Iteration 15189 - loss value [[328.56279094]] accuracy 0.7741046831955923\n","Iteration 15190 - loss value [[329.50234781]] accuracy 0.78099173553719\n","Iteration 15191 - loss value [[327.96270623]] accuracy 0.778236914600551\n","Iteration 15192 - loss value [[333.66929829]] accuracy 0.78099173553719\n","Iteration 15193 - loss value [[333.60727097]] accuracy 0.7741046831955923\n","Iteration 15194 - loss value [[332.79189911]] accuracy 0.7865013774104683\n","Iteration 15195 - loss value [[337.63293707]] accuracy 0.7754820936639119\n","Iteration 15196 - loss value [[339.4829262]] accuracy 0.78099173553719\n","Iteration 15197 - loss value [[362.06481116]] accuracy 0.7424242424242424\n","Iteration 15198 - loss value [[326.49621696]] accuracy 0.7837465564738292\n","Iteration 15199 - loss value [[328.11200009]] accuracy 0.778236914600551\n","Iteration 15200 - loss value [[335.49104941]] accuracy 0.78099173553719\n","Iteration 15201 - loss value [[344.36937297]] accuracy 0.7699724517906336\n","Iteration 15202 - loss value [[335.14390543]] accuracy 0.78099173553719\n","Iteration 15203 - loss value [[343.64258628]] accuracy 0.7672176308539945\n","Iteration 15204 - loss value [[342.20727248]] accuracy 0.7768595041322314\n","Iteration 15205 - loss value [[364.25855746]] accuracy 0.7451790633608816\n","Iteration 15206 - loss value [[327.29021489]] accuracy 0.78099173553719\n","Iteration 15207 - loss value [[325.55615251]] accuracy 0.7865013774104683\n","Iteration 15208 - loss value [[325.63352673]] accuracy 0.7865013774104683\n","Iteration 15209 - loss value [[325.87930437]] accuracy 0.7851239669421488\n","Iteration 15210 - loss value [[326.6969963]] accuracy 0.7823691460055097\n","Iteration 15211 - loss value [[331.30181155]] accuracy 0.78099173553719\n","Iteration 15212 - loss value [[334.88324301]] accuracy 0.7741046831955923\n","Iteration 15213 - loss value [[331.05549703]] accuracy 0.7823691460055097\n","Iteration 15214 - loss value [[329.1115868]] accuracy 0.7796143250688705\n","Iteration 15215 - loss value [[330.08388865]] accuracy 0.7823691460055097\n","Iteration 15216 - loss value [[327.67325125]] accuracy 0.778236914600551\n","Iteration 15217 - loss value [[326.45195756]] accuracy 0.7851239669421488\n","Iteration 15218 - loss value [[325.71216688]] accuracy 0.7865013774104683\n","Iteration 15219 - loss value [[326.0980731]] accuracy 0.7865013774104683\n","Iteration 15220 - loss value [[328.08446799]] accuracy 0.778236914600551\n","Iteration 15221 - loss value [[335.07102362]] accuracy 0.7823691460055097\n","Iteration 15222 - loss value [[337.18035974]] accuracy 0.7699724517906336\n","Iteration 15223 - loss value [[334.64947348]] accuracy 0.7796143250688705\n","Iteration 15224 - loss value [[336.6479361]] accuracy 0.7713498622589532\n","Iteration 15225 - loss value [[330.80688453]] accuracy 0.7823691460055097\n","Iteration 15226 - loss value [[329.01395415]] accuracy 0.7796143250688705\n","Iteration 15227 - loss value [[327.48500035]] accuracy 0.7878787878787878\n","Iteration 15228 - loss value [[327.03825232]] accuracy 0.7823691460055097\n","Iteration 15229 - loss value [[327.32188082]] accuracy 0.7837465564738292\n","Iteration 15230 - loss value [[328.93673892]] accuracy 0.778236914600551\n","Iteration 15231 - loss value [[336.39987744]] accuracy 0.7796143250688705\n","Iteration 15232 - loss value [[338.9232707]] accuracy 0.7699724517906336\n","Iteration 15233 - loss value [[332.2028213]] accuracy 0.7823691460055097\n","Iteration 15234 - loss value [[329.35291588]] accuracy 0.778236914600551\n","Iteration 15235 - loss value [[328.73649018]] accuracy 0.7865013774104683\n","Iteration 15236 - loss value [[327.1762996]] accuracy 0.7796143250688705\n","Iteration 15237 - loss value [[326.75820297]] accuracy 0.7851239669421488\n","Iteration 15238 - loss value [[326.15105075]] accuracy 0.7837465564738292\n","Iteration 15239 - loss value [[327.01595105]] accuracy 0.7837465564738292\n","Iteration 15240 - loss value [[327.8513451]] accuracy 0.7796143250688705\n","Iteration 15241 - loss value [[333.16888052]] accuracy 0.7796143250688705\n","Iteration 15242 - loss value [[334.14137256]] accuracy 0.7741046831955923\n","Iteration 15243 - loss value [[344.69572443]] accuracy 0.7727272727272727\n","Iteration 15244 - loss value [[362.05586425]] accuracy 0.7506887052341598\n","Iteration 15245 - loss value [[326.46158616]] accuracy 0.7851239669421488\n","Iteration 15246 - loss value [[325.90171315]] accuracy 0.7865013774104683\n","Iteration 15247 - loss value [[327.21491127]] accuracy 0.7837465564738292\n","Iteration 15248 - loss value [[326.62060772]] accuracy 0.7837465564738292\n","Iteration 15249 - loss value [[328.30358991]] accuracy 0.7837465564738292\n","Iteration 15250 - loss value [[326.7201726]] accuracy 0.7837465564738292\n","Iteration 15251 - loss value [[327.08447218]] accuracy 0.78099173553719\n","Iteration 15252 - loss value [[328.73602516]] accuracy 0.778236914600551\n","Iteration 15253 - loss value [[332.31937655]] accuracy 0.7796143250688705\n","Iteration 15254 - loss value [[329.51766483]] accuracy 0.7796143250688705\n","Iteration 15255 - loss value [[328.03557849]] accuracy 0.7865013774104683\n","Iteration 15256 - loss value [[327.1399474]] accuracy 0.778236914600551\n","Iteration 15257 - loss value [[330.77248078]] accuracy 0.7837465564738292\n","Iteration 15258 - loss value [[332.73408483]] accuracy 0.7768595041322314\n","Iteration 15259 - loss value [[330.44769182]] accuracy 0.7796143250688705\n","Iteration 15260 - loss value [[328.16221274]] accuracy 0.78099173553719\n","Iteration 15261 - loss value [[327.01211655]] accuracy 0.7878787878787878\n","Iteration 15262 - loss value [[325.97018586]] accuracy 0.7837465564738292\n","Iteration 15263 - loss value [[325.52320175]] accuracy 0.7878787878787878\n","Iteration 15264 - loss value [[325.61196304]] accuracy 0.7851239669421488\n","Iteration 15265 - loss value [[325.88788443]] accuracy 0.7878787878787878\n","Iteration 15266 - loss value [[326.84568187]] accuracy 0.7823691460055097\n","Iteration 15267 - loss value [[329.37847063]] accuracy 0.7851239669421488\n","Iteration 15268 - loss value [[330.39018645]] accuracy 0.7768595041322314\n","Iteration 15269 - loss value [[334.20662146]] accuracy 0.7823691460055097\n","Iteration 15270 - loss value [[337.03889004]] accuracy 0.7713498622589532\n","Iteration 15271 - loss value [[332.13458793]] accuracy 0.7796143250688705\n","Iteration 15272 - loss value [[329.68500086]] accuracy 0.7796143250688705\n","Iteration 15273 - loss value [[330.50469808]] accuracy 0.7823691460055097\n","Iteration 15274 - loss value [[327.38405584]] accuracy 0.7796143250688705\n","Iteration 15275 - loss value [[326.47460106]] accuracy 0.7851239669421488\n","Iteration 15276 - loss value [[326.21334021]] accuracy 0.7837465564738292\n","Iteration 15277 - loss value [[327.65853811]] accuracy 0.7837465564738292\n","Iteration 15278 - loss value [[327.09841835]] accuracy 0.7823691460055097\n","Iteration 15279 - loss value [[328.38947977]] accuracy 0.7796143250688705\n","Iteration 15280 - loss value [[328.94528366]] accuracy 0.7768595041322314\n","Iteration 15281 - loss value [[330.57488714]] accuracy 0.7823691460055097\n","Iteration 15282 - loss value [[327.34315017]] accuracy 0.7796143250688705\n","Iteration 15283 - loss value [[326.06193162]] accuracy 0.7837465564738292\n","Iteration 15284 - loss value [[326.92213331]] accuracy 0.78099173553719\n","Iteration 15285 - loss value [[329.22105771]] accuracy 0.7865013774104683\n","Iteration 15286 - loss value [[330.45937641]] accuracy 0.7754820936639119\n","Iteration 15287 - loss value [[330.78486117]] accuracy 0.7796143250688705\n","Iteration 15288 - loss value [[328.35259996]] accuracy 0.7823691460055097\n","Iteration 15289 - loss value [[326.60836262]] accuracy 0.7837465564738292\n","Iteration 15290 - loss value [[325.94927183]] accuracy 0.7837465564738292\n","Iteration 15291 - loss value [[326.09755734]] accuracy 0.7837465564738292\n","Iteration 15292 - loss value [[327.70492846]] accuracy 0.7823691460055097\n","Iteration 15293 - loss value [[329.86023879]] accuracy 0.7768595041322314\n","Iteration 15294 - loss value [[329.84551851]] accuracy 0.7823691460055097\n","Iteration 15295 - loss value [[329.72865367]] accuracy 0.7754820936639119\n","Iteration 15296 - loss value [[341.24130462]] accuracy 0.7754820936639119\n","Iteration 15297 - loss value [[361.72614083]] accuracy 0.7451790633608816\n","Iteration 15298 - loss value [[327.12872241]] accuracy 0.7837465564738292\n","Iteration 15299 - loss value [[325.59996961]] accuracy 0.7878787878787878\n","Iteration 15300 - loss value [[326.37272912]] accuracy 0.7823691460055097\n","Iteration 15301 - loss value [[328.12866912]] accuracy 0.7837465564738292\n","Iteration 15302 - loss value [[327.55869496]] accuracy 0.7796143250688705\n","Iteration 15303 - loss value [[330.47047731]] accuracy 0.7851239669421488\n","Iteration 15304 - loss value [[334.43773164]] accuracy 0.7741046831955923\n","Iteration 15305 - loss value [[331.89912563]] accuracy 0.7823691460055097\n","Iteration 15306 - loss value [[338.54837548]] accuracy 0.7727272727272727\n","Iteration 15307 - loss value [[333.41708692]] accuracy 0.778236914600551\n","Iteration 15308 - loss value [[340.61915471]] accuracy 0.7699724517906336\n","Iteration 15309 - loss value [[341.5898819]] accuracy 0.7768595041322314\n","Iteration 15310 - loss value [[354.17571724]] accuracy 0.7520661157024794\n","Iteration 15311 - loss value [[326.62418116]] accuracy 0.7823691460055097\n","Iteration 15312 - loss value [[328.81310228]] accuracy 0.7796143250688705\n","Iteration 15313 - loss value [[336.28177299]] accuracy 0.7768595041322314\n","Iteration 15314 - loss value [[345.66862778]] accuracy 0.7617079889807162\n","Iteration 15315 - loss value [[335.51667715]] accuracy 0.7823691460055097\n","Iteration 15316 - loss value [[351.59218492]] accuracy 0.756198347107438\n","Iteration 15317 - loss value [[328.58241662]] accuracy 0.7837465564738292\n","Iteration 15318 - loss value [[328.36625907]] accuracy 0.778236914600551\n","Iteration 15319 - loss value [[330.07676346]] accuracy 0.78099173553719\n","Iteration 15320 - loss value [[327.75333385]] accuracy 0.778236914600551\n","Iteration 15321 - loss value [[327.96606233]] accuracy 0.7865013774104683\n","Iteration 15322 - loss value [[327.1135272]] accuracy 0.7823691460055097\n","Iteration 15323 - loss value [[327.9867698]] accuracy 0.7837465564738292\n","Iteration 15324 - loss value [[327.49878755]] accuracy 0.78099173553719\n","Iteration 15325 - loss value [[330.12964542]] accuracy 0.7851239669421488\n","Iteration 15326 - loss value [[332.54959034]] accuracy 0.7754820936639119\n","Iteration 15327 - loss value [[328.33901593]] accuracy 0.7823691460055097\n","Iteration 15328 - loss value [[327.78842378]] accuracy 0.7796143250688705\n","Iteration 15329 - loss value [[328.69111272]] accuracy 0.7865013774104683\n","Iteration 15330 - loss value [[329.95470665]] accuracy 0.7768595041322314\n","Iteration 15331 - loss value [[335.55393687]] accuracy 0.7837465564738292\n","Iteration 15332 - loss value [[350.32037342]] accuracy 0.7575757575757576\n","Iteration 15333 - loss value [[332.86777482]] accuracy 0.7823691460055097\n","Iteration 15334 - loss value [[336.53450384]] accuracy 0.7727272727272727\n","Iteration 15335 - loss value [[332.51722265]] accuracy 0.78099173553719\n","Iteration 15336 - loss value [[332.17934498]] accuracy 0.778236914600551\n","Iteration 15337 - loss value [[341.49568236]] accuracy 0.7768595041322314\n","Iteration 15338 - loss value [[364.96002795]] accuracy 0.7451790633608816\n","Iteration 15339 - loss value [[326.72362323]] accuracy 0.7837465564738292\n","Iteration 15340 - loss value [[325.92423141]] accuracy 0.7865013774104683\n","Iteration 15341 - loss value [[327.24009807]] accuracy 0.7851239669421488\n","Iteration 15342 - loss value [[327.11883726]] accuracy 0.7823691460055097\n","Iteration 15343 - loss value [[327.58013202]] accuracy 0.7837465564738292\n","Iteration 15344 - loss value [[330.20797085]] accuracy 0.7754820936639119\n","Iteration 15345 - loss value [[332.22383774]] accuracy 0.7837465564738292\n","Iteration 15346 - loss value [[329.43953111]] accuracy 0.78099173553719\n","Iteration 15347 - loss value [[329.86658363]] accuracy 0.78099173553719\n","Iteration 15348 - loss value [[327.90427501]] accuracy 0.7768595041322314\n","Iteration 15349 - loss value [[326.4820024]] accuracy 0.7837465564738292\n","Iteration 15350 - loss value [[325.7594921]] accuracy 0.7837465564738292\n","Iteration 15351 - loss value [[325.50966369]] accuracy 0.7865013774104683\n","Iteration 15352 - loss value [[325.60713567]] accuracy 0.7865013774104683\n","Iteration 15353 - loss value [[326.01638036]] accuracy 0.7865013774104683\n","Iteration 15354 - loss value [[327.54957014]] accuracy 0.7851239669421488\n","Iteration 15355 - loss value [[327.7274155]] accuracy 0.78099173553719\n","Iteration 15356 - loss value [[331.8890984]] accuracy 0.78099173553719\n","Iteration 15357 - loss value [[328.85746171]] accuracy 0.78099173553719\n","Iteration 15358 - loss value [[326.98540116]] accuracy 0.7837465564738292\n","Iteration 15359 - loss value [[326.2554391]] accuracy 0.7823691460055097\n","Iteration 15360 - loss value [[325.68592644]] accuracy 0.7851239669421488\n","Iteration 15361 - loss value [[325.58414663]] accuracy 0.7865013774104683\n","Iteration 15362 - loss value [[325.87222413]] accuracy 0.7865013774104683\n","Iteration 15363 - loss value [[327.2270946]] accuracy 0.7837465564738292\n","Iteration 15364 - loss value [[327.00177122]] accuracy 0.7823691460055097\n","Iteration 15365 - loss value [[327.48508085]] accuracy 0.7837465564738292\n","Iteration 15366 - loss value [[329.08629721]] accuracy 0.778236914600551\n","Iteration 15367 - loss value [[332.25450382]] accuracy 0.7865013774104683\n","Iteration 15368 - loss value [[333.03149516]] accuracy 0.778236914600551\n","Iteration 15369 - loss value [[332.0734607]] accuracy 0.7837465564738292\n","Iteration 15370 - loss value [[332.8590109]] accuracy 0.778236914600551\n","Iteration 15371 - loss value [[341.99720518]] accuracy 0.7754820936639119\n","Iteration 15372 - loss value [[367.39783239]] accuracy 0.7424242424242424\n","Iteration 15373 - loss value [[326.04289247]] accuracy 0.7851239669421488\n","Iteration 15374 - loss value [[325.54774694]] accuracy 0.7851239669421488\n","Iteration 15375 - loss value [[325.86490242]] accuracy 0.7878787878787878\n","Iteration 15376 - loss value [[327.22778983]] accuracy 0.7796143250688705\n","Iteration 15377 - loss value [[331.33121185]] accuracy 0.7837465564738292\n","Iteration 15378 - loss value [[335.96571793]] accuracy 0.7727272727272727\n","Iteration 15379 - loss value [[331.24076005]] accuracy 0.7837465564738292\n","Iteration 15380 - loss value [[334.27858585]] accuracy 0.7727272727272727\n","Iteration 15381 - loss value [[330.41951393]] accuracy 0.7851239669421488\n","Iteration 15382 - loss value [[334.16698224]] accuracy 0.7754820936639119\n","Iteration 15383 - loss value [[329.23911176]] accuracy 0.7823691460055097\n","Iteration 15384 - loss value [[330.05254176]] accuracy 0.7741046831955923\n","Iteration 15385 - loss value [[332.02152605]] accuracy 0.7837465564738292\n","Iteration 15386 - loss value [[336.24732356]] accuracy 0.7754820936639119\n","Iteration 15387 - loss value [[330.15299774]] accuracy 0.7796143250688705\n","Iteration 15388 - loss value [[332.11058084]] accuracy 0.7727272727272727\n","Iteration 15389 - loss value [[328.74618988]] accuracy 0.7837465564738292\n","Iteration 15390 - loss value [[327.0730083]] accuracy 0.7823691460055097\n","Iteration 15391 - loss value [[330.29712134]] accuracy 0.7837465564738292\n","Iteration 15392 - loss value [[331.45146331]] accuracy 0.7741046831955923\n","Iteration 15393 - loss value [[332.81076152]] accuracy 0.78099173553719\n","Iteration 15394 - loss value [[338.88595119]] accuracy 0.7727272727272727\n","Iteration 15395 - loss value [[336.687324]] accuracy 0.7796143250688705\n","Iteration 15396 - loss value [[345.87157094]] accuracy 0.7617079889807162\n","Iteration 15397 - loss value [[328.93955728]] accuracy 0.7768595041322314\n","Iteration 15398 - loss value [[326.2189361]] accuracy 0.7892561983471075\n","Iteration 15399 - loss value [[325.83684288]] accuracy 0.7851239669421488\n","Iteration 15400 - loss value [[326.10229185]] accuracy 0.7878787878787878\n","Iteration 15401 - loss value [[328.35780078]] accuracy 0.7768595041322314\n","Iteration 15402 - loss value [[330.62536594]] accuracy 0.7851239669421488\n","Iteration 15403 - loss value [[334.21587909]] accuracy 0.7727272727272727\n","Iteration 15404 - loss value [[329.7098656]] accuracy 0.7837465564738292\n","Iteration 15405 - loss value [[330.41143845]] accuracy 0.7754820936639119\n","Iteration 15406 - loss value [[333.76926808]] accuracy 0.7851239669421488\n","Iteration 15407 - loss value [[339.23965737]] accuracy 0.7727272727272727\n","Iteration 15408 - loss value [[335.86203201]] accuracy 0.78099173553719\n","Iteration 15409 - loss value [[345.86205586]] accuracy 0.7603305785123967\n","Iteration 15410 - loss value [[343.39534024]] accuracy 0.7768595041322314\n","Iteration 15411 - loss value [[370.29735779]] accuracy 0.7369146005509641\n","Iteration 15412 - loss value [[332.9052834]] accuracy 0.7823691460055097\n","Iteration 15413 - loss value [[331.61267615]] accuracy 0.7741046831955923\n","Iteration 15414 - loss value [[331.9022242]] accuracy 0.7823691460055097\n","Iteration 15415 - loss value [[334.09108422]] accuracy 0.7713498622589532\n","Iteration 15416 - loss value [[330.16037991]] accuracy 0.7837465564738292\n","Iteration 15417 - loss value [[331.85636006]] accuracy 0.7727272727272727\n","Iteration 15418 - loss value [[333.41144208]] accuracy 0.7823691460055097\n","Iteration 15419 - loss value [[338.98620757]] accuracy 0.7713498622589532\n","Iteration 15420 - loss value [[347.35889397]] accuracy 0.7741046831955923\n","Iteration 15421 - loss value [[379.67702917]] accuracy 0.7272727272727273\n","Iteration 15422 - loss value [[350.02813744]] accuracy 0.7658402203856749\n","Iteration 15423 - loss value [[368.36550231]] accuracy 0.7382920110192838\n","Iteration 15424 - loss value [[331.71754526]] accuracy 0.78099173553719\n","Iteration 15425 - loss value [[332.49046255]] accuracy 0.778236914600551\n","Iteration 15426 - loss value [[328.74059643]] accuracy 0.7823691460055097\n","Iteration 15427 - loss value [[332.704818]] accuracy 0.7713498622589532\n","Iteration 15428 - loss value [[332.89824456]] accuracy 0.7823691460055097\n","Iteration 15429 - loss value [[336.1662608]] accuracy 0.7727272727272727\n","Iteration 15430 - loss value [[328.98566449]] accuracy 0.7865013774104683\n","Iteration 15431 - loss value [[330.61403519]] accuracy 0.7768595041322314\n","Iteration 15432 - loss value [[327.64735793]] accuracy 0.7851239669421488\n","Iteration 15433 - loss value [[328.67537323]] accuracy 0.778236914600551\n","Iteration 15434 - loss value [[333.26115973]] accuracy 0.7796143250688705\n","Iteration 15435 - loss value [[335.99520869]] accuracy 0.7699724517906336\n","Iteration 15436 - loss value [[330.42158899]] accuracy 0.78099173553719\n","Iteration 15437 - loss value [[328.34562546]] accuracy 0.78099173553719\n","Iteration 15438 - loss value [[326.35440498]] accuracy 0.7837465564738292\n","Iteration 15439 - loss value [[325.7826301]] accuracy 0.7837465564738292\n","Iteration 15440 - loss value [[325.90122306]] accuracy 0.7865013774104683\n","Iteration 15441 - loss value [[327.66566817]] accuracy 0.7851239669421488\n","Iteration 15442 - loss value [[326.68799654]] accuracy 0.7823691460055097\n","Iteration 15443 - loss value [[327.94377974]] accuracy 0.7837465564738292\n","Iteration 15444 - loss value [[327.91522374]] accuracy 0.78099173553719\n","Iteration 15445 - loss value [[328.10692939]] accuracy 0.7823691460055097\n","Iteration 15446 - loss value [[328.13413676]] accuracy 0.778236914600551\n","Iteration 15447 - loss value [[333.23705548]] accuracy 0.7865013774104683\n","Iteration 15448 - loss value [[338.29219273]] accuracy 0.7713498622589532\n","Iteration 15449 - loss value [[336.95803312]] accuracy 0.7741046831955923\n","Iteration 15450 - loss value [[342.62001008]] accuracy 0.7658402203856749\n","Iteration 15451 - loss value [[336.80530005]] accuracy 0.778236914600551\n","Iteration 15452 - loss value [[336.39109982]] accuracy 0.7727272727272727\n","Iteration 15453 - loss value [[329.16311827]] accuracy 0.78099173553719\n","Iteration 15454 - loss value [[331.4343433]] accuracy 0.7741046831955923\n","Iteration 15455 - loss value [[330.17388263]] accuracy 0.7823691460055097\n","Iteration 15456 - loss value [[327.54599276]] accuracy 0.7796143250688705\n","Iteration 15457 - loss value [[326.69007582]] accuracy 0.7851239669421488\n","Iteration 15458 - loss value [[326.36942174]] accuracy 0.7837465564738292\n","Iteration 15459 - loss value [[326.06529932]] accuracy 0.7823691460055097\n","Iteration 15460 - loss value [[325.80317792]] accuracy 0.7865013774104683\n","Iteration 15461 - loss value [[326.97840784]] accuracy 0.7837465564738292\n","Iteration 15462 - loss value [[328.0933006]] accuracy 0.778236914600551\n","Iteration 15463 - loss value [[333.69190092]] accuracy 0.778236914600551\n","Iteration 15464 - loss value [[334.49530491]] accuracy 0.7754820936639119\n","Iteration 15465 - loss value [[334.99717238]] accuracy 0.78099173553719\n","Iteration 15466 - loss value [[345.17852213]] accuracy 0.7644628099173554\n","Iteration 15467 - loss value [[341.80669382]] accuracy 0.7796143250688705\n","Iteration 15468 - loss value [[362.80587724]] accuracy 0.743801652892562\n","Iteration 15469 - loss value [[326.59761408]] accuracy 0.78099173553719\n","Iteration 15470 - loss value [[326.32141006]] accuracy 0.7851239669421488\n","Iteration 15471 - loss value [[326.1729331]] accuracy 0.7837465564738292\n","Iteration 15472 - loss value [[327.24363067]] accuracy 0.7865013774104683\n","Iteration 15473 - loss value [[326.15323098]] accuracy 0.7823691460055097\n","Iteration 15474 - loss value [[326.91792476]] accuracy 0.7865013774104683\n","Iteration 15475 - loss value [[329.47277155]] accuracy 0.778236914600551\n","Iteration 15476 - loss value [[329.6418181]] accuracy 0.7878787878787878\n","Iteration 15477 - loss value [[332.54715687]] accuracy 0.7741046831955923\n","Iteration 15478 - loss value [[328.90627758]] accuracy 0.7865013774104683\n","Iteration 15479 - loss value [[330.79599582]] accuracy 0.7768595041322314\n","Iteration 15480 - loss value [[330.82288078]] accuracy 0.7823691460055097\n","Iteration 15481 - loss value [[329.41182582]] accuracy 0.7768595041322314\n","Iteration 15482 - loss value [[330.91926356]] accuracy 0.7851239669421488\n","Iteration 15483 - loss value [[330.45594677]] accuracy 0.7768595041322314\n","Iteration 15484 - loss value [[331.24235496]] accuracy 0.7823691460055097\n","Iteration 15485 - loss value [[330.46397131]] accuracy 0.7796143250688705\n","Iteration 15486 - loss value [[330.72217701]] accuracy 0.7851239669421488\n","Iteration 15487 - loss value [[331.00892577]] accuracy 0.7768595041322314\n","Iteration 15488 - loss value [[336.10510288]] accuracy 0.78099173553719\n","Iteration 15489 - loss value [[347.16310991]] accuracy 0.7617079889807162\n","Iteration 15490 - loss value [[344.8070918]] accuracy 0.7727272727272727\n","Iteration 15491 - loss value [[373.04080613]] accuracy 0.7369146005509641\n","Iteration 15492 - loss value [[334.41367431]] accuracy 0.7823691460055097\n","Iteration 15493 - loss value [[336.93042066]] accuracy 0.7699724517906336\n","Iteration 15494 - loss value [[334.84408626]] accuracy 0.78099173553719\n","Iteration 15495 - loss value [[339.40996077]] accuracy 0.7699724517906336\n","Iteration 15496 - loss value [[336.56086789]] accuracy 0.7754820936639119\n","Iteration 15497 - loss value [[334.74487963]] accuracy 0.7713498622589532\n","Iteration 15498 - loss value [[334.60820352]] accuracy 0.78099173553719\n","Iteration 15499 - loss value [[343.86679197]] accuracy 0.768595041322314\n","Iteration 15500 - loss value [[333.98567172]] accuracy 0.7837465564738292\n","Iteration 15501 - loss value [[338.76214788]] accuracy 0.7699724517906336\n","Iteration 15502 - loss value [[332.62841412]] accuracy 0.7865013774104683\n","Iteration 15503 - loss value [[333.10985056]] accuracy 0.778236914600551\n","Iteration 15504 - loss value [[334.45194051]] accuracy 0.7837465564738292\n","Iteration 15505 - loss value [[344.25606189]] accuracy 0.7672176308539945\n","Iteration 15506 - loss value [[344.17926175]] accuracy 0.7768595041322314\n","Iteration 15507 - loss value [[375.60570712]] accuracy 0.7355371900826446\n","Iteration 15508 - loss value [[330.31540138]] accuracy 0.7823691460055097\n","Iteration 15509 - loss value [[331.10504861]] accuracy 0.7741046831955923\n","Iteration 15510 - loss value [[329.31395949]] accuracy 0.7837465564738292\n","Iteration 15511 - loss value [[333.02679855]] accuracy 0.7741046831955923\n","Iteration 15512 - loss value [[327.24126124]] accuracy 0.7837465564738292\n","Iteration 15513 - loss value [[328.38346075]] accuracy 0.778236914600551\n","Iteration 15514 - loss value [[333.68541961]] accuracy 0.7837465564738292\n","Iteration 15515 - loss value [[333.31292895]] accuracy 0.7768595041322314\n","Iteration 15516 - loss value [[344.17421192]] accuracy 0.7741046831955923\n","Iteration 15517 - loss value [[367.04608942]] accuracy 0.743801652892562\n","Iteration 15518 - loss value [[326.42589872]] accuracy 0.7823691460055097\n","Iteration 15519 - loss value [[329.64538373]] accuracy 0.7768595041322314\n","Iteration 15520 - loss value [[327.30693979]] accuracy 0.7837465564738292\n","Iteration 15521 - loss value [[329.10434344]] accuracy 0.7796143250688705\n","Iteration 15522 - loss value [[331.62612342]] accuracy 0.78099173553719\n","Iteration 15523 - loss value [[327.87126541]] accuracy 0.7796143250688705\n","Iteration 15524 - loss value [[326.33854185]] accuracy 0.7851239669421488\n","Iteration 15525 - loss value [[326.00953045]] accuracy 0.7851239669421488\n","Iteration 15526 - loss value [[327.67950928]] accuracy 0.7851239669421488\n","Iteration 15527 - loss value [[326.51428169]] accuracy 0.7823691460055097\n","Iteration 15528 - loss value [[327.35820191]] accuracy 0.7865013774104683\n","Iteration 15529 - loss value [[326.58468435]] accuracy 0.7837465564738292\n","Iteration 15530 - loss value [[328.10126622]] accuracy 0.7823691460055097\n","Iteration 15531 - loss value [[327.06112347]] accuracy 0.7823691460055097\n","Iteration 15532 - loss value [[327.03230141]] accuracy 0.7837465564738292\n","Iteration 15533 - loss value [[327.95004719]] accuracy 0.778236914600551\n","Iteration 15534 - loss value [[331.23878864]] accuracy 0.7741046831955923\n","Iteration 15535 - loss value [[330.27305345]] accuracy 0.778236914600551\n","Iteration 15536 - loss value [[331.65374379]] accuracy 0.7754820936639119\n","Iteration 15537 - loss value [[328.07169165]] accuracy 0.7865013774104683\n","Iteration 15538 - loss value [[327.07870644]] accuracy 0.78099173553719\n","Iteration 15539 - loss value [[326.44632203]] accuracy 0.7837465564738292\n","Iteration 15540 - loss value [[326.88038647]] accuracy 0.78099173553719\n","Iteration 15541 - loss value [[330.65640064]] accuracy 0.78099173553719\n","Iteration 15542 - loss value [[332.58580702]] accuracy 0.7741046831955923\n","Iteration 15543 - loss value [[329.96595725]] accuracy 0.7823691460055097\n","Iteration 15544 - loss value [[330.76487359]] accuracy 0.7754820936639119\n","Iteration 15545 - loss value [[330.90472526]] accuracy 0.78099173553719\n","Iteration 15546 - loss value [[333.49826571]] accuracy 0.7741046831955923\n","Iteration 15547 - loss value [[332.02585646]] accuracy 0.78099173553719\n","Iteration 15548 - loss value [[329.62938947]] accuracy 0.7796143250688705\n","Iteration 15549 - loss value [[329.86888908]] accuracy 0.78099173553719\n","Iteration 15550 - loss value [[327.75387208]] accuracy 0.7796143250688705\n","Iteration 15551 - loss value [[326.45539329]] accuracy 0.7851239669421488\n","Iteration 15552 - loss value [[325.71827402]] accuracy 0.7837465564738292\n","Iteration 15553 - loss value [[325.61839233]] accuracy 0.7851239669421488\n","Iteration 15554 - loss value [[325.91135433]] accuracy 0.7837465564738292\n","Iteration 15555 - loss value [[325.6842538]] accuracy 0.7878787878787878\n","Iteration 15556 - loss value [[326.59116729]] accuracy 0.7851239669421488\n","Iteration 15557 - loss value [[327.03729463]] accuracy 0.78099173553719\n","Iteration 15558 - loss value [[330.99933786]] accuracy 0.78099173553719\n","Iteration 15559 - loss value [[328.29805526]] accuracy 0.7796143250688705\n","Iteration 15560 - loss value [[328.05897293]] accuracy 0.7865013774104683\n","Iteration 15561 - loss value [[328.50276771]] accuracy 0.7754820936639119\n","Iteration 15562 - loss value [[336.09237088]] accuracy 0.7754820936639119\n","Iteration 15563 - loss value [[334.71098107]] accuracy 0.7713498622589532\n","Iteration 15564 - loss value [[332.34774243]] accuracy 0.7837465564738292\n","Iteration 15565 - loss value [[334.12832921]] accuracy 0.778236914600551\n","Iteration 15566 - loss value [[333.03859536]] accuracy 0.7823691460055097\n","Iteration 15567 - loss value [[342.6815593]] accuracy 0.7699724517906336\n","Iteration 15568 - loss value [[335.63597952]] accuracy 0.7823691460055097\n","Iteration 15569 - loss value [[340.6648492]] accuracy 0.7727272727272727\n","Iteration 15570 - loss value [[332.11448446]] accuracy 0.7796143250688705\n","Iteration 15571 - loss value [[330.67052596]] accuracy 0.7796143250688705\n","Iteration 15572 - loss value [[331.39662715]] accuracy 0.7837465564738292\n","Iteration 15573 - loss value [[337.65842072]] accuracy 0.7713498622589532\n","Iteration 15574 - loss value [[339.04207691]] accuracy 0.7768595041322314\n","Iteration 15575 - loss value [[355.1594987]] accuracy 0.7534435261707989\n","Iteration 15576 - loss value [[326.08101008]] accuracy 0.7823691460055097\n","Iteration 15577 - loss value [[326.85572406]] accuracy 0.7823691460055097\n","Iteration 15578 - loss value [[327.91347952]] accuracy 0.78099173553719\n","Iteration 15579 - loss value [[332.41892414]] accuracy 0.7796143250688705\n","Iteration 15580 - loss value [[328.92755203]] accuracy 0.78099173553719\n","Iteration 15581 - loss value [[326.73869489]] accuracy 0.7851239669421488\n","Iteration 15582 - loss value [[325.9084524]] accuracy 0.7837465564738292\n","Iteration 15583 - loss value [[325.79342495]] accuracy 0.7865013774104683\n","Iteration 15584 - loss value [[327.14160769]] accuracy 0.7823691460055097\n","Iteration 15585 - loss value [[329.15408038]] accuracy 0.7741046831955923\n","Iteration 15586 - loss value [[332.04883027]] accuracy 0.7851239669421488\n","Iteration 15587 - loss value [[332.20769892]] accuracy 0.7768595041322314\n","Iteration 15588 - loss value [[340.64750257]] accuracy 0.778236914600551\n","Iteration 15589 - loss value [[364.00932268]] accuracy 0.7424242424242424\n","Iteration 15590 - loss value [[326.06872435]] accuracy 0.7865013774104683\n","Iteration 15591 - loss value [[327.30091431]] accuracy 0.7823691460055097\n","Iteration 15592 - loss value [[329.09025654]] accuracy 0.7768595041322314\n","Iteration 15593 - loss value [[333.09463398]] accuracy 0.7837465564738292\n","Iteration 15594 - loss value [[340.62998848]] accuracy 0.7699724517906336\n","Iteration 15595 - loss value [[332.779534]] accuracy 0.78099173553719\n","Iteration 15596 - loss value [[330.04238013]] accuracy 0.7796143250688705\n","Iteration 15597 - loss value [[329.22377728]] accuracy 0.7837465564738292\n","Iteration 15598 - loss value [[327.68815117]] accuracy 0.7796143250688705\n","Iteration 15599 - loss value [[325.96510548]] accuracy 0.7823691460055097\n","Iteration 15600 - loss value [[326.94776055]] accuracy 0.7823691460055097\n","Iteration 15601 - loss value [[330.35372839]] accuracy 0.7851239669421488\n","Iteration 15602 - loss value [[334.45418259]] accuracy 0.7727272727272727\n","Iteration 15603 - loss value [[328.54283776]] accuracy 0.7851239669421488\n","Iteration 15604 - loss value [[329.01406597]] accuracy 0.778236914600551\n","Iteration 15605 - loss value [[331.54943253]] accuracy 0.7796143250688705\n","Iteration 15606 - loss value [[329.05714462]] accuracy 0.7796143250688705\n","Iteration 15607 - loss value [[327.0650212]] accuracy 0.7837465564738292\n","Iteration 15608 - loss value [[326.9301444]] accuracy 0.7837465564738292\n","Iteration 15609 - loss value [[326.80035171]] accuracy 0.78099173553719\n","Iteration 15610 - loss value [[326.09233249]] accuracy 0.7823691460055097\n","Iteration 15611 - loss value [[326.4568199]] accuracy 0.7865013774104683\n","Iteration 15612 - loss value [[328.44365503]] accuracy 0.778236914600551\n","Iteration 15613 - loss value [[331.33957701]] accuracy 0.7754820936639119\n","Iteration 15614 - loss value [[330.43667394]] accuracy 0.7796143250688705\n","Iteration 15615 - loss value [[335.3553954]] accuracy 0.7741046831955923\n","Iteration 15616 - loss value [[327.29096267]] accuracy 0.7823691460055097\n","Iteration 15617 - loss value [[330.92354806]] accuracy 0.7754820936639119\n","Iteration 15618 - loss value [[329.04143755]] accuracy 0.7837465564738292\n","Iteration 15619 - loss value [[330.4689494]] accuracy 0.7768595041322314\n","Iteration 15620 - loss value [[327.71963914]] accuracy 0.7851239669421488\n","Iteration 15621 - loss value [[328.72965607]] accuracy 0.778236914600551\n","Iteration 15622 - loss value [[336.51094928]] accuracy 0.7768595041322314\n","Iteration 15623 - loss value [[339.10652249]] accuracy 0.7713498622589532\n","Iteration 15624 - loss value [[336.36425814]] accuracy 0.78099173553719\n","Iteration 15625 - loss value [[352.60347661]] accuracy 0.756198347107438\n","Iteration 15626 - loss value [[328.27320736]] accuracy 0.7823691460055097\n","Iteration 15627 - loss value [[328.51081615]] accuracy 0.778236914600551\n","Iteration 15628 - loss value [[335.43426313]] accuracy 0.78099173553719\n","Iteration 15629 - loss value [[350.06425856]] accuracy 0.7575757575757576\n","Iteration 15630 - loss value [[330.22467464]] accuracy 0.78099173553719\n","Iteration 15631 - loss value [[331.42620149]] accuracy 0.7741046831955923\n","Iteration 15632 - loss value [[327.98593823]] accuracy 0.7878787878787878\n","Iteration 15633 - loss value [[326.9561713]] accuracy 0.7796143250688705\n","Iteration 15634 - loss value [[329.54595555]] accuracy 0.7851239669421488\n","Iteration 15635 - loss value [[332.03395724]] accuracy 0.7741046831955923\n","Iteration 15636 - loss value [[331.61373346]] accuracy 0.7823691460055097\n","Iteration 15637 - loss value [[338.68716965]] accuracy 0.7727272727272727\n","Iteration 15638 - loss value [[338.0517181]] accuracy 0.778236914600551\n","Iteration 15639 - loss value [[348.68215557]] accuracy 0.7589531680440771\n","Iteration 15640 - loss value [[327.53127588]] accuracy 0.78099173553719\n","Iteration 15641 - loss value [[332.48732128]] accuracy 0.7754820936639119\n","Iteration 15642 - loss value [[328.79728471]] accuracy 0.7823691460055097\n","Iteration 15643 - loss value [[327.71204577]] accuracy 0.78099173553719\n","Iteration 15644 - loss value [[334.68540144]] accuracy 0.7837465564738292\n","Iteration 15645 - loss value [[342.02560865]] accuracy 0.7713498622589532\n","Iteration 15646 - loss value [[335.90361386]] accuracy 0.78099173553719\n","Iteration 15647 - loss value [[340.73240384]] accuracy 0.7699724517906336\n","Iteration 15648 - loss value [[334.03164192]] accuracy 0.7796143250688705\n","Iteration 15649 - loss value [[335.80817171]] accuracy 0.7727272727272727\n","Iteration 15650 - loss value [[334.2377909]] accuracy 0.778236914600551\n","Iteration 15651 - loss value [[338.16265591]] accuracy 0.768595041322314\n","Iteration 15652 - loss value [[336.10235079]] accuracy 0.7823691460055097\n","Iteration 15653 - loss value [[354.35359955]] accuracy 0.7548209366391184\n","Iteration 15654 - loss value [[328.07365919]] accuracy 0.7823691460055097\n","Iteration 15655 - loss value [[327.50524065]] accuracy 0.778236914600551\n","Iteration 15656 - loss value [[328.56501752]] accuracy 0.7851239669421488\n","Iteration 15657 - loss value [[330.18876293]] accuracy 0.7768595041322314\n","Iteration 15658 - loss value [[336.5402578]] accuracy 0.7851239669421488\n","Iteration 15659 - loss value [[353.54529389]] accuracy 0.756198347107438\n","Iteration 15660 - loss value [[325.95298284]] accuracy 0.7837465564738292\n","Iteration 15661 - loss value [[325.68906067]] accuracy 0.7851239669421488\n","Iteration 15662 - loss value [[326.45233714]] accuracy 0.7837465564738292\n","Iteration 15663 - loss value [[329.33454628]] accuracy 0.778236914600551\n","Iteration 15664 - loss value [[328.4661911]] accuracy 0.7837465564738292\n","Iteration 15665 - loss value [[329.52786343]] accuracy 0.7754820936639119\n","Iteration 15666 - loss value [[327.80404201]] accuracy 0.7823691460055097\n","Iteration 15667 - loss value [[329.10581864]] accuracy 0.778236914600551\n","Iteration 15668 - loss value [[327.07005387]] accuracy 0.7851239669421488\n","Iteration 15669 - loss value [[330.79007068]] accuracy 0.7727272727272727\n","Iteration 15670 - loss value [[327.59605795]] accuracy 0.7823691460055097\n","Iteration 15671 - loss value [[332.03744567]] accuracy 0.7727272727272727\n","Iteration 15672 - loss value [[327.350839]] accuracy 0.7851239669421488\n","Iteration 15673 - loss value [[331.23296946]] accuracy 0.7727272727272727\n","Iteration 15674 - loss value [[328.54620195]] accuracy 0.7851239669421488\n","Iteration 15675 - loss value [[330.02927065]] accuracy 0.7754820936639119\n","Iteration 15676 - loss value [[330.24491739]] accuracy 0.7796143250688705\n","Iteration 15677 - loss value [[333.43426231]] accuracy 0.7727272727272727\n","Iteration 15678 - loss value [[328.36714975]] accuracy 0.7851239669421488\n","Iteration 15679 - loss value [[330.73019664]] accuracy 0.7754820936639119\n","Iteration 15680 - loss value [[328.82878628]] accuracy 0.78099173553719\n","Iteration 15681 - loss value [[328.82053785]] accuracy 0.778236914600551\n","Iteration 15682 - loss value [[329.61360935]] accuracy 0.7823691460055097\n","Iteration 15683 - loss value [[333.54241376]] accuracy 0.7713498622589532\n","Iteration 15684 - loss value [[329.71611252]] accuracy 0.7865013774104683\n","Iteration 15685 - loss value [[332.68856707]] accuracy 0.7741046831955923\n","Iteration 15686 - loss value [[328.20042455]] accuracy 0.7823691460055097\n","Iteration 15687 - loss value [[327.41296598]] accuracy 0.7796143250688705\n","Iteration 15688 - loss value [[327.97475643]] accuracy 0.7837465564738292\n","Iteration 15689 - loss value [[327.63135011]] accuracy 0.78099173553719\n","Iteration 15690 - loss value [[331.55483362]] accuracy 0.78099173553719\n","Iteration 15691 - loss value [[336.11599851]] accuracy 0.7713498622589532\n","Iteration 15692 - loss value [[330.18282722]] accuracy 0.7837465564738292\n","Iteration 15693 - loss value [[332.95428774]] accuracy 0.7768595041322314\n","Iteration 15694 - loss value [[332.60480312]] accuracy 0.78099173553719\n","Iteration 15695 - loss value [[341.63602161]] accuracy 0.7672176308539945\n","Iteration 15696 - loss value [[341.48156819]] accuracy 0.7768595041322314\n","Iteration 15697 - loss value [[360.15280845]] accuracy 0.7465564738292011\n","Iteration 15698 - loss value [[326.65292444]] accuracy 0.78099173553719\n","Iteration 15699 - loss value [[326.00354813]] accuracy 0.7823691460055097\n","Iteration 15700 - loss value [[327.70936649]] accuracy 0.78099173553719\n","Iteration 15701 - loss value [[327.79742796]] accuracy 0.78099173553719\n","Iteration 15702 - loss value [[329.52349073]] accuracy 0.7823691460055097\n","Iteration 15703 - loss value [[327.19470682]] accuracy 0.7796143250688705\n","Iteration 15704 - loss value [[326.35232356]] accuracy 0.7823691460055097\n","Iteration 15705 - loss value [[325.89531927]] accuracy 0.7837465564738292\n","Iteration 15706 - loss value [[326.05880467]] accuracy 0.7837465564738292\n","Iteration 15707 - loss value [[327.26820199]] accuracy 0.7851239669421488\n","Iteration 15708 - loss value [[328.82085277]] accuracy 0.78099173553719\n","Iteration 15709 - loss value [[331.00939459]] accuracy 0.7823691460055097\n","Iteration 15710 - loss value [[327.36785093]] accuracy 0.7796143250688705\n","Iteration 15711 - loss value [[325.72446064]] accuracy 0.7823691460055097\n","Iteration 15712 - loss value [[326.0152034]] accuracy 0.7837465564738292\n","Iteration 15713 - loss value [[326.99904689]] accuracy 0.7851239669421488\n","Iteration 15714 - loss value [[327.2510332]] accuracy 0.78099173553719\n","Iteration 15715 - loss value [[332.04057048]] accuracy 0.7796143250688705\n","Iteration 15716 - loss value [[328.82160133]] accuracy 0.78099173553719\n","Iteration 15717 - loss value [[326.64990932]] accuracy 0.7837465564738292\n","Iteration 15718 - loss value [[326.43818934]] accuracy 0.78099173553719\n","Iteration 15719 - loss value [[325.76778757]] accuracy 0.7837465564738292\n","Iteration 15720 - loss value [[325.9015267]] accuracy 0.7878787878787878\n","Iteration 15721 - loss value [[327.38209686]] accuracy 0.7865013774104683\n","Iteration 15722 - loss value [[327.5851714]] accuracy 0.78099173553719\n","Iteration 15723 - loss value [[331.68340888]] accuracy 0.78099173553719\n","Iteration 15724 - loss value [[328.66046924]] accuracy 0.78099173553719\n","Iteration 15725 - loss value [[327.02361704]] accuracy 0.7851239669421488\n","Iteration 15726 - loss value [[325.69526264]] accuracy 0.7837465564738292\n","Iteration 15727 - loss value [[325.26406138]] accuracy 0.7865013774104683\n","Iteration 15728 - loss value [[325.25441302]] accuracy 0.7865013774104683\n","Iteration 15729 - loss value [[325.25623186]] accuracy 0.7865013774104683\n","Iteration 15730 - loss value [[325.26646757]] accuracy 0.7865013774104683\n","Iteration 15731 - loss value [[325.3143632]] accuracy 0.7878787878787878\n","Iteration 15732 - loss value [[325.52504876]] accuracy 0.7865013774104683\n","Iteration 15733 - loss value [[326.48116784]] accuracy 0.7851239669421488\n","Iteration 15734 - loss value [[326.32621448]] accuracy 0.78099173553719\n","Iteration 15735 - loss value [[328.30239941]] accuracy 0.7851239669421488\n","Iteration 15736 - loss value [[329.53314558]] accuracy 0.7754820936639119\n","Iteration 15737 - loss value [[336.40173696]] accuracy 0.7768595041322314\n","Iteration 15738 - loss value [[345.55909777]] accuracy 0.7658402203856749\n","Iteration 15739 - loss value [[347.26740454]] accuracy 0.7727272727272727\n","Iteration 15740 - loss value [[373.97268186]] accuracy 0.7355371900826446\n","Iteration 15741 - loss value [[329.77577373]] accuracy 0.7837465564738292\n","Iteration 15742 - loss value [[330.77067417]] accuracy 0.7741046831955923\n","Iteration 15743 - loss value [[328.74784622]] accuracy 0.7837465564738292\n","Iteration 15744 - loss value [[331.60385453]] accuracy 0.7741046831955923\n","Iteration 15745 - loss value [[330.92665261]] accuracy 0.7796143250688705\n","Iteration 15746 - loss value [[331.48357481]] accuracy 0.7727272727272727\n","Iteration 15747 - loss value [[332.13529285]] accuracy 0.7823691460055097\n","Iteration 15748 - loss value [[336.31225839]] accuracy 0.7727272727272727\n","Iteration 15749 - loss value [[336.55585186]] accuracy 0.7796143250688705\n","Iteration 15750 - loss value [[345.24166637]] accuracy 0.7630853994490359\n","Iteration 15751 - loss value [[337.5426844]] accuracy 0.7823691460055097\n","Iteration 15752 - loss value [[352.48430985]] accuracy 0.756198347107438\n","Iteration 15753 - loss value [[325.92177818]] accuracy 0.7837465564738292\n","Iteration 15754 - loss value [[325.58170078]] accuracy 0.7865013774104683\n","Iteration 15755 - loss value [[326.14865933]] accuracy 0.7837465564738292\n","Iteration 15756 - loss value [[327.61411823]] accuracy 0.778236914600551\n","Iteration 15757 - loss value [[334.89743815]] accuracy 0.7768595041322314\n","Iteration 15758 - loss value [[339.51597143]] accuracy 0.768595041322314\n","Iteration 15759 - loss value [[333.9593865]] accuracy 0.7823691460055097\n","Iteration 15760 - loss value [[346.20019871]] accuracy 0.7644628099173554\n","Iteration 15761 - loss value [[341.91960294]] accuracy 0.7754820936639119\n","Iteration 15762 - loss value [[365.0602711]] accuracy 0.7465564738292011\n","Iteration 15763 - loss value [[327.50288767]] accuracy 0.7796143250688705\n","Iteration 15764 - loss value [[326.68220923]] accuracy 0.7851239669421488\n","Iteration 15765 - loss value [[326.07934425]] accuracy 0.7837465564738292\n","Iteration 15766 - loss value [[327.29474586]] accuracy 0.7851239669421488\n","Iteration 15767 - loss value [[327.28042211]] accuracy 0.7823691460055097\n","Iteration 15768 - loss value [[328.41234609]] accuracy 0.7865013774104683\n","Iteration 15769 - loss value [[327.34231068]] accuracy 0.7823691460055097\n","Iteration 15770 - loss value [[329.531956]] accuracy 0.78099173553719\n","Iteration 15771 - loss value [[328.85724392]] accuracy 0.7796143250688705\n","Iteration 15772 - loss value [[327.13885346]] accuracy 0.7851239669421488\n","Iteration 15773 - loss value [[326.00836712]] accuracy 0.7837465564738292\n","Iteration 15774 - loss value [[326.00811792]] accuracy 0.7837465564738292\n","Iteration 15775 - loss value [[329.11615035]] accuracy 0.7768595041322314\n","Iteration 15776 - loss value [[331.13650099]] accuracy 0.7837465564738292\n","Iteration 15777 - loss value [[333.46889258]] accuracy 0.7754820936639119\n","Iteration 15778 - loss value [[327.75904398]] accuracy 0.7837465564738292\n","Iteration 15779 - loss value [[326.62010548]] accuracy 0.7837465564738292\n","Iteration 15780 - loss value [[328.22478588]] accuracy 0.7837465564738292\n","Iteration 15781 - loss value [[326.78496496]] accuracy 0.7837465564738292\n","Iteration 15782 - loss value [[327.72579491]] accuracy 0.7837465564738292\n","Iteration 15783 - loss value [[327.14923001]] accuracy 0.7837465564738292\n","Iteration 15784 - loss value [[328.49095474]] accuracy 0.78099173553719\n","Iteration 15785 - loss value [[327.3798667]] accuracy 0.7823691460055097\n","Iteration 15786 - loss value [[325.87924111]] accuracy 0.7837465564738292\n","Iteration 15787 - loss value [[326.5431561]] accuracy 0.7823691460055097\n","Iteration 15788 - loss value [[328.31491612]] accuracy 0.7851239669421488\n","Iteration 15789 - loss value [[331.23050292]] accuracy 0.7741046831955923\n","Iteration 15790 - loss value [[326.57384401]] accuracy 0.7837465564738292\n","Iteration 15791 - loss value [[326.29870581]] accuracy 0.78099173553719\n","Iteration 15792 - loss value [[328.2866326]] accuracy 0.7837465564738292\n","Iteration 15793 - loss value [[329.17482707]] accuracy 0.7768595041322314\n","Iteration 15794 - loss value [[337.10768578]] accuracy 0.7754820936639119\n","Iteration 15795 - loss value [[348.84371401]] accuracy 0.7603305785123967\n","Iteration 15796 - loss value [[342.76777013]] accuracy 0.7768595041322314\n","Iteration 15797 - loss value [[354.81371204]] accuracy 0.7506887052341598\n","Iteration 15798 - loss value [[328.13226168]] accuracy 0.78099173553719\n","Iteration 15799 - loss value [[327.26660358]] accuracy 0.7837465564738292\n","Iteration 15800 - loss value [[327.6184841]] accuracy 0.78099173553719\n","Iteration 15801 - loss value [[331.73889272]] accuracy 0.78099173553719\n","Iteration 15802 - loss value [[330.12515687]] accuracy 0.778236914600551\n","Iteration 15803 - loss value [[331.8496072]] accuracy 0.7837465564738292\n","Iteration 15804 - loss value [[331.42732723]] accuracy 0.778236914600551\n","Iteration 15805 - loss value [[335.43975467]] accuracy 0.7837465564738292\n","Iteration 15806 - loss value [[351.37664097]] accuracy 0.7575757575757576\n","Iteration 15807 - loss value [[329.54087417]] accuracy 0.7865013774104683\n","Iteration 15808 - loss value [[331.10824438]] accuracy 0.7754820936639119\n","Iteration 15809 - loss value [[327.76217055]] accuracy 0.7851239669421488\n","Iteration 15810 - loss value [[326.4108152]] accuracy 0.7837465564738292\n","Iteration 15811 - loss value [[327.9375611]] accuracy 0.7837465564738292\n","Iteration 15812 - loss value [[326.96781158]] accuracy 0.7823691460055097\n","Iteration 15813 - loss value [[326.55043195]] accuracy 0.7851239669421488\n","Iteration 15814 - loss value [[326.87882777]] accuracy 0.7796143250688705\n","Iteration 15815 - loss value [[326.68751558]] accuracy 0.78099173553719\n","Iteration 15816 - loss value [[331.50736436]] accuracy 0.778236914600551\n","Iteration 15817 - loss value [[333.64934822]] accuracy 0.7754820936639119\n","Iteration 15818 - loss value [[327.85506149]] accuracy 0.7851239669421488\n","Iteration 15819 - loss value [[328.05434081]] accuracy 0.78099173553719\n","Iteration 15820 - loss value [[334.43896135]] accuracy 0.7796143250688705\n","Iteration 15821 - loss value [[338.14342858]] accuracy 0.7713498622589532\n","Iteration 15822 - loss value [[331.56621148]] accuracy 0.78099173553719\n","Iteration 15823 - loss value [[335.34409487]] accuracy 0.7741046831955923\n","Iteration 15824 - loss value [[333.34654362]] accuracy 0.78099173553719\n","Iteration 15825 - loss value [[336.96154824]] accuracy 0.7727272727272727\n","Iteration 15826 - loss value [[339.26134793]] accuracy 0.7727272727272727\n","Iteration 15827 - loss value [[362.77264958]] accuracy 0.7479338842975206\n","Iteration 15828 - loss value [[327.52174364]] accuracy 0.7837465564738292\n","Iteration 15829 - loss value [[329.11990151]] accuracy 0.78099173553719\n","Iteration 15830 - loss value [[328.89737715]] accuracy 0.7768595041322314\n","Iteration 15831 - loss value [[336.93979404]] accuracy 0.78099173553719\n","Iteration 15832 - loss value [[354.25053853]] accuracy 0.7548209366391184\n","Iteration 15833 - loss value [[325.71924976]] accuracy 0.7837465564738292\n","Iteration 15834 - loss value [[325.62671008]] accuracy 0.7865013774104683\n","Iteration 15835 - loss value [[326.62749695]] accuracy 0.7837465564738292\n","Iteration 15836 - loss value [[329.98457554]] accuracy 0.7754820936639119\n","Iteration 15837 - loss value [[328.88863855]] accuracy 0.7837465564738292\n","Iteration 15838 - loss value [[331.20735727]] accuracy 0.7713498622589532\n","Iteration 15839 - loss value [[330.20049911]] accuracy 0.7837465564738292\n","Iteration 15840 - loss value [[331.99084806]] accuracy 0.7754820936639119\n","Iteration 15841 - loss value [[330.33004703]] accuracy 0.7837465564738292\n","Iteration 15842 - loss value [[326.88424453]] accuracy 0.7823691460055097\n","Iteration 15843 - loss value [[327.36960965]] accuracy 0.7865013774104683\n","Iteration 15844 - loss value [[326.59554158]] accuracy 0.7823691460055097\n","Iteration 15845 - loss value [[326.87252784]] accuracy 0.7837465564738292\n","Iteration 15846 - loss value [[330.44918809]] accuracy 0.7741046831955923\n","Iteration 15847 - loss value [[328.03410566]] accuracy 0.7837465564738292\n","Iteration 15848 - loss value [[329.85332005]] accuracy 0.7741046831955923\n","Iteration 15849 - loss value [[330.96768669]] accuracy 0.7837465564738292\n","Iteration 15850 - loss value [[335.29336934]] accuracy 0.7727272727272727\n","Iteration 15851 - loss value [[334.62627259]] accuracy 0.7768595041322314\n","Iteration 15852 - loss value [[331.05013637]] accuracy 0.78099173553719\n","Iteration 15853 - loss value [[332.03818431]] accuracy 0.7823691460055097\n","Iteration 15854 - loss value [[329.55691534]] accuracy 0.78099173553719\n","Iteration 15855 - loss value [[328.55623594]] accuracy 0.7823691460055097\n","Iteration 15856 - loss value [[327.19933285]] accuracy 0.78099173553719\n","Iteration 15857 - loss value [[326.66554438]] accuracy 0.7823691460055097\n","Iteration 15858 - loss value [[326.15530166]] accuracy 0.7837465564738292\n","Iteration 15859 - loss value [[325.9056106]] accuracy 0.7837465564738292\n","Iteration 15860 - loss value [[325.3757928]] accuracy 0.7878787878787878\n","Iteration 15861 - loss value [[325.39498874]] accuracy 0.7851239669421488\n","Iteration 15862 - loss value [[325.89739585]] accuracy 0.7878787878787878\n","Iteration 15863 - loss value [[327.88685433]] accuracy 0.7768595041322314\n","Iteration 15864 - loss value [[333.92798746]] accuracy 0.78099173553719\n","Iteration 15865 - loss value [[334.07741481]] accuracy 0.7768595041322314\n","Iteration 15866 - loss value [[332.39273635]] accuracy 0.7851239669421488\n","Iteration 15867 - loss value [[333.51617584]] accuracy 0.7754820936639119\n","Iteration 15868 - loss value [[333.03849068]] accuracy 0.7823691460055097\n","Iteration 15869 - loss value [[340.36045943]] accuracy 0.7699724517906336\n","Iteration 15870 - loss value [[334.24185838]] accuracy 0.78099173553719\n","Iteration 15871 - loss value [[338.77449528]] accuracy 0.768595041322314\n","Iteration 15872 - loss value [[346.69128931]] accuracy 0.7699724517906336\n","Iteration 15873 - loss value [[379.72256641]] accuracy 0.7258953168044077\n","Iteration 15874 - loss value [[334.07917045]] accuracy 0.7851239669421488\n","Iteration 15875 - loss value [[331.06372656]] accuracy 0.7741046831955923\n","Iteration 15876 - loss value [[335.66990534]] accuracy 0.78099173553719\n","Iteration 15877 - loss value [[348.24972538]] accuracy 0.7575757575757576\n","Iteration 15878 - loss value [[332.07956035]] accuracy 0.78099173553719\n","Iteration 15879 - loss value [[331.96829999]] accuracy 0.7754820936639119\n","Iteration 15880 - loss value [[331.43866452]] accuracy 0.78099173553719\n","Iteration 15881 - loss value [[327.44402947]] accuracy 0.78099173553719\n","Iteration 15882 - loss value [[326.33413935]] accuracy 0.7823691460055097\n","Iteration 15883 - loss value [[326.27750393]] accuracy 0.7837465564738292\n","Iteration 15884 - loss value [[326.59389237]] accuracy 0.7837465564738292\n","Iteration 15885 - loss value [[328.57681608]] accuracy 0.778236914600551\n","Iteration 15886 - loss value [[337.58406155]] accuracy 0.7768595041322314\n","Iteration 15887 - loss value [[339.84741906]] accuracy 0.7713498622589532\n","Iteration 15888 - loss value [[331.83013612]] accuracy 0.7837465564738292\n","Iteration 15889 - loss value [[329.49400403]] accuracy 0.778236914600551\n","Iteration 15890 - loss value [[330.83669805]] accuracy 0.7865013774104683\n","Iteration 15891 - loss value [[330.32517945]] accuracy 0.7768595041322314\n","Iteration 15892 - loss value [[331.05725211]] accuracy 0.7851239669421488\n","Iteration 15893 - loss value [[329.31071289]] accuracy 0.7796143250688705\n","Iteration 15894 - loss value [[327.56971111]] accuracy 0.7865013774104683\n","Iteration 15895 - loss value [[327.66899177]] accuracy 0.778236914600551\n","Iteration 15896 - loss value [[331.2782177]] accuracy 0.78099173553719\n","Iteration 15897 - loss value [[327.61673497]] accuracy 0.778236914600551\n","Iteration 15898 - loss value [[326.76056366]] accuracy 0.7837465564738292\n","Iteration 15899 - loss value [[325.73690159]] accuracy 0.7837465564738292\n","Iteration 15900 - loss value [[325.44598445]] accuracy 0.7851239669421488\n","Iteration 15901 - loss value [[325.76351451]] accuracy 0.7823691460055097\n","Iteration 15902 - loss value [[325.96578367]] accuracy 0.7851239669421488\n","Iteration 15903 - loss value [[328.88058437]] accuracy 0.778236914600551\n","Iteration 15904 - loss value [[327.63149732]] accuracy 0.78099173553719\n","Iteration 15905 - loss value [[328.59616157]] accuracy 0.78099173553719\n","Iteration 15906 - loss value [[330.72677043]] accuracy 0.7851239669421488\n","Iteration 15907 - loss value [[327.32784181]] accuracy 0.778236914600551\n","Iteration 15908 - loss value [[325.94907624]] accuracy 0.7837465564738292\n","Iteration 15909 - loss value [[326.85362053]] accuracy 0.78099173553719\n","Iteration 15910 - loss value [[329.76014114]] accuracy 0.7878787878787878\n","Iteration 15911 - loss value [[331.75838526]] accuracy 0.7754820936639119\n","Iteration 15912 - loss value [[329.30888365]] accuracy 0.7837465564738292\n","Iteration 15913 - loss value [[330.13490007]] accuracy 0.778236914600551\n","Iteration 15914 - loss value [[332.81347701]] accuracy 0.7837465564738292\n","Iteration 15915 - loss value [[330.97817703]] accuracy 0.7796143250688705\n","Iteration 15916 - loss value [[330.4879907]] accuracy 0.7837465564738292\n","Iteration 15917 - loss value [[328.42667103]] accuracy 0.7768595041322314\n","Iteration 15918 - loss value [[327.08739631]] accuracy 0.7837465564738292\n","Iteration 15919 - loss value [[326.08380813]] accuracy 0.78099173553719\n","Iteration 15920 - loss value [[325.9178987]] accuracy 0.7851239669421488\n","Iteration 15921 - loss value [[327.22465465]] accuracy 0.78099173553719\n","Iteration 15922 - loss value [[333.47449077]] accuracy 0.7865013774104683\n","Iteration 15923 - loss value [[337.2838756]] accuracy 0.7713498622589532\n","Iteration 15924 - loss value [[342.55329353]] accuracy 0.7754820936639119\n","Iteration 15925 - loss value [[362.51887389]] accuracy 0.743801652892562\n","Iteration 15926 - loss value [[326.40749562]] accuracy 0.7851239669421488\n","Iteration 15927 - loss value [[326.00922942]] accuracy 0.7865013774104683\n","Iteration 15928 - loss value [[327.96727467]] accuracy 0.7768595041322314\n","Iteration 15929 - loss value [[333.70757631]] accuracy 0.7796143250688705\n","Iteration 15930 - loss value [[333.49889077]] accuracy 0.7768595041322314\n","Iteration 15931 - loss value [[342.52754226]] accuracy 0.7754820936639119\n","Iteration 15932 - loss value [[360.44612884]] accuracy 0.7493112947658402\n","Iteration 15933 - loss value [[326.45848805]] accuracy 0.7823691460055097\n","Iteration 15934 - loss value [[329.43053896]] accuracy 0.778236914600551\n","Iteration 15935 - loss value [[328.31335933]] accuracy 0.7865013774104683\n","Iteration 15936 - loss value [[327.18150012]] accuracy 0.7837465564738292\n","Iteration 15937 - loss value [[329.27494123]] accuracy 0.78099173553719\n","Iteration 15938 - loss value [[328.99137821]] accuracy 0.778236914600551\n","Iteration 15939 - loss value [[329.38660119]] accuracy 0.7837465564738292\n","Iteration 15940 - loss value [[327.5207314]] accuracy 0.7768595041322314\n","Iteration 15941 - loss value [[326.63307284]] accuracy 0.7837465564738292\n","Iteration 15942 - loss value [[325.88541781]] accuracy 0.7851239669421488\n","Iteration 15943 - loss value [[326.57727732]] accuracy 0.7823691460055097\n","Iteration 15944 - loss value [[326.9428705]] accuracy 0.7823691460055097\n","Iteration 15945 - loss value [[327.87693055]] accuracy 0.7851239669421488\n","Iteration 15946 - loss value [[327.55776769]] accuracy 0.7823691460055097\n","Iteration 15947 - loss value [[328.10205783]] accuracy 0.7851239669421488\n","Iteration 15948 - loss value [[328.03949085]] accuracy 0.7796143250688705\n","Iteration 15949 - loss value [[327.44479484]] accuracy 0.7823691460055097\n","Iteration 15950 - loss value [[326.68392132]] accuracy 0.78099173553719\n","Iteration 15951 - loss value [[327.16212803]] accuracy 0.7837465564738292\n","Iteration 15952 - loss value [[326.01082569]] accuracy 0.78099173553719\n","Iteration 15953 - loss value [[325.5809429]] accuracy 0.7865013774104683\n","Iteration 15954 - loss value [[326.20085214]] accuracy 0.7837465564738292\n","Iteration 15955 - loss value [[328.91642945]] accuracy 0.7823691460055097\n","Iteration 15956 - loss value [[331.6453344]] accuracy 0.7713498622589532\n","Iteration 15957 - loss value [[330.72738215]] accuracy 0.7823691460055097\n","Iteration 15958 - loss value [[327.34311104]] accuracy 0.778236914600551\n","Iteration 15959 - loss value [[326.87722837]] accuracy 0.7837465564738292\n","Iteration 15960 - loss value [[326.17157649]] accuracy 0.78099173553719\n","Iteration 15961 - loss value [[325.42687724]] accuracy 0.7851239669421488\n","Iteration 15962 - loss value [[325.4543527]] accuracy 0.7865013774104683\n","Iteration 15963 - loss value [[326.19609753]] accuracy 0.7837465564738292\n","Iteration 15964 - loss value [[327.7116566]] accuracy 0.7851239669421488\n","Iteration 15965 - loss value [[328.91960111]] accuracy 0.7754820936639119\n","Iteration 15966 - loss value [[338.69705546]] accuracy 0.7741046831955923\n","Iteration 15967 - loss value [[345.43113764]] accuracy 0.7672176308539945\n","Iteration 15968 - loss value [[343.67425333]] accuracy 0.7754820936639119\n","Iteration 15969 - loss value [[362.89997777]] accuracy 0.743801652892562\n","Iteration 15970 - loss value [[326.82053175]] accuracy 0.7823691460055097\n","Iteration 15971 - loss value [[325.99419337]] accuracy 0.7837465564738292\n","Iteration 15972 - loss value [[325.78258001]] accuracy 0.7837465564738292\n","Iteration 15973 - loss value [[326.54577735]] accuracy 0.7851239669421488\n","Iteration 15974 - loss value [[326.34778196]] accuracy 0.7796143250688705\n","Iteration 15975 - loss value [[325.64238685]] accuracy 0.7878787878787878\n","Iteration 15976 - loss value [[325.93048451]] accuracy 0.78099173553719\n","Iteration 15977 - loss value [[325.66648928]] accuracy 0.7865013774104683\n","Iteration 15978 - loss value [[326.94952901]] accuracy 0.7851239669421488\n","Iteration 15979 - loss value [[326.52494535]] accuracy 0.7823691460055097\n","Iteration 15980 - loss value [[327.88933318]] accuracy 0.7865013774104683\n","Iteration 15981 - loss value [[326.88073456]] accuracy 0.7823691460055097\n","Iteration 15982 - loss value [[326.04605003]] accuracy 0.7823691460055097\n","Iteration 15983 - loss value [[326.64649702]] accuracy 0.7851239669421488\n","Iteration 15984 - loss value [[325.87629754]] accuracy 0.7837465564738292\n","Iteration 15985 - loss value [[326.09207447]] accuracy 0.7851239669421488\n","Iteration 15986 - loss value [[329.58143639]] accuracy 0.7754820936639119\n","Iteration 15987 - loss value [[329.297013]] accuracy 0.78099173553719\n","Iteration 15988 - loss value [[328.30556099]] accuracy 0.7796143250688705\n","Iteration 15989 - loss value [[328.42002581]] accuracy 0.7865013774104683\n","Iteration 15990 - loss value [[330.68836669]] accuracy 0.7768595041322314\n","Iteration 15991 - loss value [[330.92707999]] accuracy 0.78099173553719\n","Iteration 15992 - loss value [[327.75185206]] accuracy 0.7796143250688705\n","Iteration 15993 - loss value [[326.82630999]] accuracy 0.7837465564738292\n","Iteration 15994 - loss value [[326.62468509]] accuracy 0.7837465564738292\n","Iteration 15995 - loss value [[326.26035798]] accuracy 0.7837465564738292\n","Iteration 15996 - loss value [[326.02369452]] accuracy 0.7823691460055097\n","Iteration 15997 - loss value [[325.5960439]] accuracy 0.7865013774104683\n","Iteration 15998 - loss value [[327.04659731]] accuracy 0.78099173553719\n","Iteration 15999 - loss value [[330.6189668]] accuracy 0.7823691460055097\n","Iteration 16000 - loss value [[334.21742018]] accuracy 0.7727272727272727\n","Iteration 16001 - loss value [[329.75673427]] accuracy 0.7851239669421488\n","Iteration 16002 - loss value [[331.73235032]] accuracy 0.7741046831955923\n","Iteration 16003 - loss value [[326.49439543]] accuracy 0.7823691460055097\n","Iteration 16004 - loss value [[327.12248796]] accuracy 0.78099173553719\n","Iteration 16005 - loss value [[331.80382351]] accuracy 0.7796143250688705\n","Iteration 16006 - loss value [[328.79150839]] accuracy 0.78099173553719\n","Iteration 16007 - loss value [[326.91684098]] accuracy 0.7837465564738292\n","Iteration 16008 - loss value [[326.24434156]] accuracy 0.7823691460055097\n","Iteration 16009 - loss value [[325.90658931]] accuracy 0.7837465564738292\n","Iteration 16010 - loss value [[327.01959619]] accuracy 0.78099173553719\n","Iteration 16011 - loss value [[329.9143123]] accuracy 0.7768595041322314\n","Iteration 16012 - loss value [[327.75929157]] accuracy 0.7837465564738292\n","Iteration 16013 - loss value [[328.94594286]] accuracy 0.7796143250688705\n","Iteration 16014 - loss value [[327.01684464]] accuracy 0.7851239669421488\n","Iteration 16015 - loss value [[330.75997464]] accuracy 0.7741046831955923\n","Iteration 16016 - loss value [[327.77285714]] accuracy 0.7823691460055097\n","Iteration 16017 - loss value [[329.24439926]] accuracy 0.7768595041322314\n","Iteration 16018 - loss value [[332.80742656]] accuracy 0.78099173553719\n","Iteration 16019 - loss value [[334.4422085]] accuracy 0.7754820936639119\n","Iteration 16020 - loss value [[339.18220915]] accuracy 0.7823691460055097\n","Iteration 16021 - loss value [[358.24512862]] accuracy 0.7479338842975206\n","Iteration 16022 - loss value [[327.59255691]] accuracy 0.7823691460055097\n","Iteration 16023 - loss value [[328.78837143]] accuracy 0.778236914600551\n","Iteration 16024 - loss value [[339.34992792]] accuracy 0.7741046831955923\n","Iteration 16025 - loss value [[351.77716125]] accuracy 0.7589531680440771\n","Iteration 16026 - loss value [[328.09243825]] accuracy 0.7823691460055097\n","Iteration 16027 - loss value [[329.76596869]] accuracy 0.7768595041322314\n","Iteration 16028 - loss value [[327.49278003]] accuracy 0.7823691460055097\n","Iteration 16029 - loss value [[330.01595097]] accuracy 0.7741046831955923\n","Iteration 16030 - loss value [[328.84216583]] accuracy 0.7851239669421488\n","Iteration 16031 - loss value [[328.63535845]] accuracy 0.7754820936639119\n","Iteration 16032 - loss value [[335.52565934]] accuracy 0.7837465564738292\n","Iteration 16033 - loss value [[349.5340979]] accuracy 0.7589531680440771\n","Iteration 16034 - loss value [[331.09861522]] accuracy 0.7823691460055097\n","Iteration 16035 - loss value [[333.8036215]] accuracy 0.7754820936639119\n","Iteration 16036 - loss value [[328.41958046]] accuracy 0.7823691460055097\n","Iteration 16037 - loss value [[328.31276077]] accuracy 0.7796143250688705\n","Iteration 16038 - loss value [[330.57417331]] accuracy 0.78099173553719\n","Iteration 16039 - loss value [[328.3948927]] accuracy 0.7796143250688705\n","Iteration 16040 - loss value [[326.65811345]] accuracy 0.7823691460055097\n","Iteration 16041 - loss value [[326.57040564]] accuracy 0.7837465564738292\n","Iteration 16042 - loss value [[325.89644386]] accuracy 0.7837465564738292\n","Iteration 16043 - loss value [[326.36188081]] accuracy 0.7823691460055097\n","Iteration 16044 - loss value [[326.4100243]] accuracy 0.7837465564738292\n","Iteration 16045 - loss value [[330.17799967]] accuracy 0.7768595041322314\n","Iteration 16046 - loss value [[327.29731428]] accuracy 0.7851239669421488\n","Iteration 16047 - loss value [[327.68013598]] accuracy 0.778236914600551\n","Iteration 16048 - loss value [[333.95279879]] accuracy 0.78099173553719\n","Iteration 16049 - loss value [[336.30510735]] accuracy 0.7727272727272727\n","Iteration 16050 - loss value [[333.97306387]] accuracy 0.7837465564738292\n","Iteration 16051 - loss value [[339.38524587]] accuracy 0.7713498622589532\n","Iteration 16052 - loss value [[331.48899673]] accuracy 0.78099173553719\n","Iteration 16053 - loss value [[329.70245479]] accuracy 0.7768595041322314\n","Iteration 16054 - loss value [[331.87751494]] accuracy 0.7823691460055097\n","Iteration 16055 - loss value [[338.26344946]] accuracy 0.7741046831955923\n","Iteration 16056 - loss value [[332.59936751]] accuracy 0.7851239669421488\n","Iteration 16057 - loss value [[337.81817597]] accuracy 0.7741046831955923\n","Iteration 16058 - loss value [[338.86895073]] accuracy 0.7823691460055097\n","Iteration 16059 - loss value [[358.22131063]] accuracy 0.7479338842975206\n","Iteration 16060 - loss value [[327.77486547]] accuracy 0.7837465564738292\n","Iteration 16061 - loss value [[328.31377763]] accuracy 0.778236914600551\n","Iteration 16062 - loss value [[333.43696056]] accuracy 0.778236914600551\n","Iteration 16063 - loss value [[335.56958457]] accuracy 0.7741046831955923\n","Iteration 16064 - loss value [[332.33955502]] accuracy 0.7851239669421488\n","Iteration 16065 - loss value [[331.97602603]] accuracy 0.78099173553719\n","Iteration 16066 - loss value [[335.22979146]] accuracy 0.78099173553719\n","Iteration 16067 - loss value [[348.44498313]] accuracy 0.7603305785123967\n","Iteration 16068 - loss value [[340.57895869]] accuracy 0.78099173553719\n","Iteration 16069 - loss value [[353.78809026]] accuracy 0.7534435261707989\n","Iteration 16070 - loss value [[326.87879466]] accuracy 0.7796143250688705\n","Iteration 16071 - loss value [[328.14457676]] accuracy 0.7796143250688705\n","Iteration 16072 - loss value [[329.52817343]] accuracy 0.78099173553719\n","Iteration 16073 - loss value [[332.9561138]] accuracy 0.7754820936639119\n","Iteration 16074 - loss value [[331.41245424]] accuracy 0.78099173553719\n","Iteration 16075 - loss value [[329.25382519]] accuracy 0.778236914600551\n","Iteration 16076 - loss value [[329.88648704]] accuracy 0.7837465564738292\n","Iteration 16077 - loss value [[327.94555156]] accuracy 0.7768595041322314\n","Iteration 16078 - loss value [[326.86553871]] accuracy 0.7851239669421488\n","Iteration 16079 - loss value [[325.81402957]] accuracy 0.78099173553719\n","Iteration 16080 - loss value [[325.43391978]] accuracy 0.7878787878787878\n","Iteration 16081 - loss value [[326.70179632]] accuracy 0.78099173553719\n","Iteration 16082 - loss value [[330.94920494]] accuracy 0.7823691460055097\n","Iteration 16083 - loss value [[329.33271939]] accuracy 0.778236914600551\n","Iteration 16084 - loss value [[330.43150399]] accuracy 0.7823691460055097\n","Iteration 16085 - loss value [[328.87475037]] accuracy 0.7796143250688705\n","Iteration 16086 - loss value [[328.74523674]] accuracy 0.7823691460055097\n","Iteration 16087 - loss value [[326.79144458]] accuracy 0.7796143250688705\n","Iteration 16088 - loss value [[326.09197853]] accuracy 0.7837465564738292\n","Iteration 16089 - loss value [[325.34761254]] accuracy 0.7865013774104683\n","Iteration 16090 - loss value [[325.5352155]] accuracy 0.7851239669421488\n","Iteration 16091 - loss value [[326.92997872]] accuracy 0.7796143250688705\n","Iteration 16092 - loss value [[332.47873596]] accuracy 0.7865013774104683\n","Iteration 16093 - loss value [[332.09924394]] accuracy 0.778236914600551\n","Iteration 16094 - loss value [[336.36795647]] accuracy 0.7796143250688705\n","Iteration 16095 - loss value [[347.063792]] accuracy 0.7630853994490359\n","Iteration 16096 - loss value [[346.50219857]] accuracy 0.7754820936639119\n","Iteration 16097 - loss value [[371.78631086]] accuracy 0.7369146005509641\n","Iteration 16098 - loss value [[330.1413792]] accuracy 0.78099173553719\n","Iteration 16099 - loss value [[330.94819093]] accuracy 0.7754820936639119\n","Iteration 16100 - loss value [[329.89359516]] accuracy 0.7823691460055097\n","Iteration 16101 - loss value [[328.21469878]] accuracy 0.7796143250688705\n","Iteration 16102 - loss value [[331.86224939]] accuracy 0.7837465564738292\n","Iteration 16103 - loss value [[330.39162932]] accuracy 0.778236914600551\n","Iteration 16104 - loss value [[331.63897158]] accuracy 0.7823691460055097\n","Iteration 16105 - loss value [[330.47948556]] accuracy 0.778236914600551\n","Iteration 16106 - loss value [[330.21215094]] accuracy 0.7837465564738292\n","Iteration 16107 - loss value [[329.71920018]] accuracy 0.7768595041322314\n","Iteration 16108 - loss value [[330.74220069]] accuracy 0.7837465564738292\n","Iteration 16109 - loss value [[330.2828607]] accuracy 0.7768595041322314\n","Iteration 16110 - loss value [[332.73171159]] accuracy 0.78099173553719\n","Iteration 16111 - loss value [[333.84146904]] accuracy 0.7741046831955923\n","Iteration 16112 - loss value [[332.3088048]] accuracy 0.7851239669421488\n","Iteration 16113 - loss value [[335.26791839]] accuracy 0.7727272727272727\n","Iteration 16114 - loss value [[334.27514665]] accuracy 0.7865013774104683\n","Iteration 16115 - loss value [[349.24820647]] accuracy 0.7603305785123967\n","Iteration 16116 - loss value [[337.58467299]] accuracy 0.78099173553719\n","Iteration 16117 - loss value [[353.19602956]] accuracy 0.7548209366391184\n","Iteration 16118 - loss value [[325.65018607]] accuracy 0.7851239669421488\n","Iteration 16119 - loss value [[325.68702677]] accuracy 0.7851239669421488\n","Iteration 16120 - loss value [[327.51392262]] accuracy 0.7837465564738292\n","Iteration 16121 - loss value [[326.76556924]] accuracy 0.7823691460055097\n","Iteration 16122 - loss value [[327.67265066]] accuracy 0.78099173553719\n","Iteration 16123 - loss value [[327.80991942]] accuracy 0.7823691460055097\n","Iteration 16124 - loss value [[328.47458644]] accuracy 0.7837465564738292\n","Iteration 16125 - loss value [[329.76313784]] accuracy 0.7768595041322314\n","Iteration 16126 - loss value [[332.08445312]] accuracy 0.7837465564738292\n","Iteration 16127 - loss value [[329.29647323]] accuracy 0.778236914600551\n","Iteration 16128 - loss value [[329.51211618]] accuracy 0.7823691460055097\n","Iteration 16129 - loss value [[327.43740615]] accuracy 0.7768595041322314\n","Iteration 16130 - loss value [[325.98525373]] accuracy 0.7837465564738292\n","Iteration 16131 - loss value [[325.68531472]] accuracy 0.7823691460055097\n","Iteration 16132 - loss value [[325.25554395]] accuracy 0.7878787878787878\n","Iteration 16133 - loss value [[325.54261676]] accuracy 0.7851239669421488\n","Iteration 16134 - loss value [[326.64523244]] accuracy 0.78099173553719\n","Iteration 16135 - loss value [[331.34961744]] accuracy 0.7837465564738292\n","Iteration 16136 - loss value [[330.40366179]] accuracy 0.778236914600551\n","Iteration 16137 - loss value [[334.26143162]] accuracy 0.7823691460055097\n","Iteration 16138 - loss value [[343.69037916]] accuracy 0.7672176308539945\n","Iteration 16139 - loss value [[339.73490928]] accuracy 0.778236914600551\n","Iteration 16140 - loss value [[356.90522007]] accuracy 0.7493112947658402\n","Iteration 16141 - loss value [[325.70630834]] accuracy 0.7851239669421488\n","Iteration 16142 - loss value [[326.07383752]] accuracy 0.7851239669421488\n","Iteration 16143 - loss value [[328.34839446]] accuracy 0.7837465564738292\n","Iteration 16144 - loss value [[328.24523211]] accuracy 0.78099173553719\n","Iteration 16145 - loss value [[328.14683701]] accuracy 0.7851239669421488\n","Iteration 16146 - loss value [[329.22550512]] accuracy 0.7768595041322314\n","Iteration 16147 - loss value [[335.91633844]] accuracy 0.7823691460055097\n","Iteration 16148 - loss value [[350.39570543]] accuracy 0.7575757575757576\n","Iteration 16149 - loss value [[332.12807811]] accuracy 0.78099173553719\n","Iteration 16150 - loss value [[336.73981019]] accuracy 0.7699724517906336\n","Iteration 16151 - loss value [[332.28085653]] accuracy 0.78099173553719\n","Iteration 16152 - loss value [[332.78789276]] accuracy 0.7727272727272727\n","Iteration 16153 - loss value [[332.02177969]] accuracy 0.78099173553719\n","Iteration 16154 - loss value [[333.57084941]] accuracy 0.7754820936639119\n","Iteration 16155 - loss value [[329.71412525]] accuracy 0.7823691460055097\n","Iteration 16156 - loss value [[329.74015561]] accuracy 0.7741046831955923\n","Iteration 16157 - loss value [[328.03574054]] accuracy 0.7851239669421488\n","Iteration 16158 - loss value [[328.2479832]] accuracy 0.7768595041322314\n","Iteration 16159 - loss value [[335.20733461]] accuracy 0.778236914600551\n","Iteration 16160 - loss value [[337.20679796]] accuracy 0.7713498622589532\n","Iteration 16161 - loss value [[330.86884774]] accuracy 0.7823691460055097\n","Iteration 16162 - loss value [[328.01756364]] accuracy 0.78099173553719\n","Iteration 16163 - loss value [[327.62458221]] accuracy 0.7865013774104683\n","Iteration 16164 - loss value [[326.67653165]] accuracy 0.78099173553719\n","Iteration 16165 - loss value [[329.23839997]] accuracy 0.7837465564738292\n","Iteration 16166 - loss value [[331.95198238]] accuracy 0.7754820936639119\n","Iteration 16167 - loss value [[327.19460682]] accuracy 0.7865013774104683\n","Iteration 16168 - loss value [[326.86096679]] accuracy 0.78099173553719\n","Iteration 16169 - loss value [[331.5082672]] accuracy 0.78099173553719\n","Iteration 16170 - loss value [[329.09491002]] accuracy 0.7796143250688705\n","Iteration 16171 - loss value [[328.83239091]] accuracy 0.7823691460055097\n","Iteration 16172 - loss value [[326.85207768]] accuracy 0.7796143250688705\n","Iteration 16173 - loss value [[326.49016841]] accuracy 0.7823691460055097\n","Iteration 16174 - loss value [[326.34104497]] accuracy 0.7823691460055097\n","Iteration 16175 - loss value [[325.99699432]] accuracy 0.7823691460055097\n","Iteration 16176 - loss value [[328.34057607]] accuracy 0.778236914600551\n","Iteration 16177 - loss value [[332.99995896]] accuracy 0.7768595041322314\n","Iteration 16178 - loss value [[332.81224501]] accuracy 0.7768595041322314\n","Iteration 16179 - loss value [[333.95207217]] accuracy 0.7837465564738292\n","Iteration 16180 - loss value [[337.26323769]] accuracy 0.7741046831955923\n","Iteration 16181 - loss value [[337.09222342]] accuracy 0.7796143250688705\n","Iteration 16182 - loss value [[355.29623452]] accuracy 0.7520661157024794\n","Iteration 16183 - loss value [[326.32524189]] accuracy 0.7837465564738292\n","Iteration 16184 - loss value [[328.92928564]] accuracy 0.778236914600551\n","Iteration 16185 - loss value [[331.6428019]] accuracy 0.7837465564738292\n","Iteration 16186 - loss value [[329.88348006]] accuracy 0.778236914600551\n","Iteration 16187 - loss value [[331.14559945]] accuracy 0.7851239669421488\n","Iteration 16188 - loss value [[330.19793902]] accuracy 0.7796143250688705\n","Iteration 16189 - loss value [[328.9219938]] accuracy 0.7823691460055097\n","Iteration 16190 - loss value [[327.40621487]] accuracy 0.7796143250688705\n","Iteration 16191 - loss value [[325.65655695]] accuracy 0.7837465564738292\n","Iteration 16192 - loss value [[326.40184109]] accuracy 0.7823691460055097\n","Iteration 16193 - loss value [[329.63829074]] accuracy 0.7851239669421488\n","Iteration 16194 - loss value [[331.32685792]] accuracy 0.7768595041322314\n","Iteration 16195 - loss value [[331.76126716]] accuracy 0.7865013774104683\n","Iteration 16196 - loss value [[332.07186283]] accuracy 0.7754820936639119\n","Iteration 16197 - loss value [[340.5157562]] accuracy 0.7754820936639119\n","Iteration 16198 - loss value [[361.56894626]] accuracy 0.7451790633608816\n","Iteration 16199 - loss value [[326.1888935]] accuracy 0.7823691460055097\n","Iteration 16200 - loss value [[325.87657493]] accuracy 0.7837465564738292\n","Iteration 16201 - loss value [[326.35008494]] accuracy 0.7823691460055097\n","Iteration 16202 - loss value [[325.81668815]] accuracy 0.7865013774104683\n","Iteration 16203 - loss value [[327.38685545]] accuracy 0.7796143250688705\n","Iteration 16204 - loss value [[331.88060376]] accuracy 0.7796143250688705\n","Iteration 16205 - loss value [[327.78941964]] accuracy 0.7796143250688705\n","Iteration 16206 - loss value [[326.1589947]] accuracy 0.7851239669421488\n","Iteration 16207 - loss value [[325.43733038]] accuracy 0.7837465564738292\n","Iteration 16208 - loss value [[325.27303522]] accuracy 0.7865013774104683\n","Iteration 16209 - loss value [[325.51260865]] accuracy 0.7851239669421488\n","Iteration 16210 - loss value [[326.01154046]] accuracy 0.7837465564738292\n","Iteration 16211 - loss value [[328.08536842]] accuracy 0.78099173553719\n","Iteration 16212 - loss value [[328.854856]] accuracy 0.7754820936639119\n","Iteration 16213 - loss value [[331.23930067]] accuracy 0.7851239669421488\n","Iteration 16214 - loss value [[333.29804052]] accuracy 0.7727272727272727\n","Iteration 16215 - loss value [[339.46623185]] accuracy 0.7768595041322314\n","Iteration 16216 - loss value [[364.89611526]] accuracy 0.743801652892562\n","Iteration 16217 - loss value [[326.80218323]] accuracy 0.78099173553719\n","Iteration 16218 - loss value [[331.34023609]] accuracy 0.7823691460055097\n","Iteration 16219 - loss value [[329.27564575]] accuracy 0.778236914600551\n","Iteration 16220 - loss value [[329.62618821]] accuracy 0.7837465564738292\n","Iteration 16221 - loss value [[327.43285604]] accuracy 0.778236914600551\n","Iteration 16222 - loss value [[326.30107421]] accuracy 0.7865013774104683\n","Iteration 16223 - loss value [[325.43463493]] accuracy 0.7837465564738292\n","Iteration 16224 - loss value [[325.44367673]] accuracy 0.7837465564738292\n","Iteration 16225 - loss value [[325.84918705]] accuracy 0.7837465564738292\n","Iteration 16226 - loss value [[326.09853473]] accuracy 0.7837465564738292\n","Iteration 16227 - loss value [[329.01812136]] accuracy 0.7837465564738292\n","Iteration 16228 - loss value [[331.80211002]] accuracy 0.7754820936639119\n","Iteration 16229 - loss value [[330.93619324]] accuracy 0.7865013774104683\n","Iteration 16230 - loss value [[335.37314476]] accuracy 0.7741046831955923\n","Iteration 16231 - loss value [[330.28171168]] accuracy 0.7865013774104683\n","Iteration 16232 - loss value [[333.39332219]] accuracy 0.7727272727272727\n","Iteration 16233 - loss value [[331.22701719]] accuracy 0.7823691460055097\n","Iteration 16234 - loss value [[333.69736514]] accuracy 0.7754820936639119\n","Iteration 16235 - loss value [[328.17238453]] accuracy 0.7851239669421488\n","Iteration 16236 - loss value [[328.93911283]] accuracy 0.778236914600551\n","Iteration 16237 - loss value [[330.19428754]] accuracy 0.7823691460055097\n","Iteration 16238 - loss value [[327.99778625]] accuracy 0.778236914600551\n","Iteration 16239 - loss value [[326.46029602]] accuracy 0.7837465564738292\n","Iteration 16240 - loss value [[325.60709146]] accuracy 0.7837465564738292\n","Iteration 16241 - loss value [[325.14238303]] accuracy 0.7878787878787878\n","Iteration 16242 - loss value [[325.31161214]] accuracy 0.7865013774104683\n","Iteration 16243 - loss value [[326.22465088]] accuracy 0.7837465564738292\n","Iteration 16244 - loss value [[326.23936922]] accuracy 0.7837465564738292\n","Iteration 16245 - loss value [[328.09243376]] accuracy 0.7837465564738292\n","Iteration 16246 - loss value [[328.16708542]] accuracy 0.7823691460055097\n","Iteration 16247 - loss value [[327.96404711]] accuracy 0.7892561983471075\n","Iteration 16248 - loss value [[328.36945905]] accuracy 0.778236914600551\n","Iteration 16249 - loss value [[331.6453298]] accuracy 0.7796143250688705\n","Iteration 16250 - loss value [[329.01935239]] accuracy 0.78099173553719\n","Iteration 16251 - loss value [[326.83297143]] accuracy 0.7837465564738292\n","Iteration 16252 - loss value [[326.04339139]] accuracy 0.78099173553719\n","Iteration 16253 - loss value [[325.6655743]] accuracy 0.7851239669421488\n","Iteration 16254 - loss value [[326.24980418]] accuracy 0.7823691460055097\n","Iteration 16255 - loss value [[329.54624316]] accuracy 0.7878787878787878\n","Iteration 16256 - loss value [[332.2458842]] accuracy 0.7754820936639119\n","Iteration 16257 - loss value [[327.59703079]] accuracy 0.7837465564738292\n","Iteration 16258 - loss value [[326.7660999]] accuracy 0.7837465564738292\n","Iteration 16259 - loss value [[329.46388818]] accuracy 0.7865013774104683\n","Iteration 16260 - loss value [[331.95794156]] accuracy 0.7754820936639119\n","Iteration 16261 - loss value [[327.06455051]] accuracy 0.7837465564738292\n","Iteration 16262 - loss value [[325.86686098]] accuracy 0.78099173553719\n","Iteration 16263 - loss value [[325.62118818]] accuracy 0.7851239669421488\n","Iteration 16264 - loss value [[326.27268433]] accuracy 0.7823691460055097\n","Iteration 16265 - loss value [[330.56320071]] accuracy 0.7837465564738292\n","Iteration 16266 - loss value [[334.08048482]] accuracy 0.7768595041322314\n","Iteration 16267 - loss value [[332.40285224]] accuracy 0.7837465564738292\n","Iteration 16268 - loss value [[336.30968794]] accuracy 0.7741046831955923\n","Iteration 16269 - loss value [[337.89497993]] accuracy 0.778236914600551\n","Iteration 16270 - loss value [[355.99246096]] accuracy 0.7520661157024794\n","Iteration 16271 - loss value [[327.04267976]] accuracy 0.7837465564738292\n","Iteration 16272 - loss value [[329.96503827]] accuracy 0.778236914600551\n","Iteration 16273 - loss value [[327.17930605]] accuracy 0.7851239669421488\n","Iteration 16274 - loss value [[328.47490196]] accuracy 0.778236914600551\n","Iteration 16275 - loss value [[333.8790219]] accuracy 0.7796143250688705\n","Iteration 16276 - loss value [[335.74736225]] accuracy 0.7713498622589532\n","Iteration 16277 - loss value [[329.04359194]] accuracy 0.7823691460055097\n","Iteration 16278 - loss value [[326.84851703]] accuracy 0.7796143250688705\n","Iteration 16279 - loss value [[326.0896513]] accuracy 0.7823691460055097\n","Iteration 16280 - loss value [[325.93143098]] accuracy 0.7837465564738292\n","Iteration 16281 - loss value [[325.57453565]] accuracy 0.7837465564738292\n","Iteration 16282 - loss value [[325.1395026]] accuracy 0.7878787878787878\n","Iteration 16283 - loss value [[325.26686922]] accuracy 0.7865013774104683\n","Iteration 16284 - loss value [[325.79776581]] accuracy 0.7851239669421488\n","Iteration 16285 - loss value [[327.6997103]] accuracy 0.7837465564738292\n","Iteration 16286 - loss value [[327.14991404]] accuracy 0.778236914600551\n","Iteration 16287 - loss value [[330.25334025]] accuracy 0.7823691460055097\n","Iteration 16288 - loss value [[331.57814595]] accuracy 0.7713498622589532\n","Iteration 16289 - loss value [[331.22821083]] accuracy 0.7865013774104683\n","Iteration 16290 - loss value [[328.97496147]] accuracy 0.78099173553719\n","Iteration 16291 - loss value [[328.60802589]] accuracy 0.78099173553719\n","Iteration 16292 - loss value [[326.70906673]] accuracy 0.7796143250688705\n","Iteration 16293 - loss value [[326.11917872]] accuracy 0.7823691460055097\n","Iteration 16294 - loss value [[325.7188461]] accuracy 0.7823691460055097\n","Iteration 16295 - loss value [[325.93479339]] accuracy 0.7823691460055097\n","Iteration 16296 - loss value [[325.51134155]] accuracy 0.7837465564738292\n","Iteration 16297 - loss value [[326.14274852]] accuracy 0.7851239669421488\n","Iteration 16298 - loss value [[328.69369008]] accuracy 0.778236914600551\n","Iteration 16299 - loss value [[333.71663573]] accuracy 0.7837465564738292\n","Iteration 16300 - loss value [[336.67597706]] accuracy 0.7741046831955923\n","Iteration 16301 - loss value [[335.40998625]] accuracy 0.7823691460055097\n","Iteration 16302 - loss value [[348.21805488]] accuracy 0.7589531680440771\n","Iteration 16303 - loss value [[348.65877081]] accuracy 0.7713498622589532\n","Iteration 16304 - loss value [[386.36358231]] accuracy 0.7203856749311295\n","Iteration 16305 - loss value [[339.78001566]] accuracy 0.778236914600551\n","Iteration 16306 - loss value [[349.51069242]] accuracy 0.7548209366391184\n","Iteration 16307 - loss value [[328.24774845]] accuracy 0.7768595041322314\n","Iteration 16308 - loss value [[325.19023767]] accuracy 0.7878787878787878\n","Iteration 16309 - loss value [[325.25997169]] accuracy 0.7865013774104683\n","Iteration 16310 - loss value [[326.10401256]] accuracy 0.7837465564738292\n","Iteration 16311 - loss value [[326.12982651]] accuracy 0.7837465564738292\n","Iteration 16312 - loss value [[328.11622014]] accuracy 0.78099173553719\n","Iteration 16313 - loss value [[327.00456752]] accuracy 0.7823691460055097\n","Iteration 16314 - loss value [[327.75262255]] accuracy 0.7837465564738292\n","Iteration 16315 - loss value [[326.48043541]] accuracy 0.78099173553719\n","Iteration 16316 - loss value [[326.99538129]] accuracy 0.7851239669421488\n","Iteration 16317 - loss value [[325.61997229]] accuracy 0.7837465564738292\n","Iteration 16318 - loss value [[326.3069943]] accuracy 0.7851239669421488\n","Iteration 16319 - loss value [[329.72112842]] accuracy 0.7768595041322314\n","Iteration 16320 - loss value [[331.07081647]] accuracy 0.7851239669421488\n","Iteration 16321 - loss value [[334.66171531]] accuracy 0.7727272727272727\n","Iteration 16322 - loss value [[328.58839228]] accuracy 0.7851239669421488\n","Iteration 16323 - loss value [[327.24761905]] accuracy 0.7768595041322314\n","Iteration 16324 - loss value [[331.43478375]] accuracy 0.7837465564738292\n","Iteration 16325 - loss value [[329.23071447]] accuracy 0.778236914600551\n","Iteration 16326 - loss value [[329.18947096]] accuracy 0.7837465564738292\n","Iteration 16327 - loss value [[327.57179341]] accuracy 0.7754820936639119\n","Iteration 16328 - loss value [[326.16280905]] accuracy 0.7837465564738292\n","Iteration 16329 - loss value [[325.43113869]] accuracy 0.7837465564738292\n","Iteration 16330 - loss value [[325.32199381]] accuracy 0.7851239669421488\n","Iteration 16331 - loss value [[325.81377338]] accuracy 0.7851239669421488\n","Iteration 16332 - loss value [[326.99958194]] accuracy 0.7796143250688705\n","Iteration 16333 - loss value [[332.87942533]] accuracy 0.78099173553719\n","Iteration 16334 - loss value [[336.57886221]] accuracy 0.7699724517906336\n","Iteration 16335 - loss value [[337.58312045]] accuracy 0.7796143250688705\n","Iteration 16336 - loss value [[359.54252539]] accuracy 0.7479338842975206\n","Iteration 16337 - loss value [[326.85857792]] accuracy 0.778236914600551\n","Iteration 16338 - loss value [[328.97663419]] accuracy 0.7796143250688705\n","Iteration 16339 - loss value [[330.16313424]] accuracy 0.7823691460055097\n","Iteration 16340 - loss value [[331.87242187]] accuracy 0.7754820936639119\n","Iteration 16341 - loss value [[329.82705307]] accuracy 0.7837465564738292\n","Iteration 16342 - loss value [[329.67236303]] accuracy 0.7754820936639119\n","Iteration 16343 - loss value [[332.04924413]] accuracy 0.7837465564738292\n","Iteration 16344 - loss value [[330.55900882]] accuracy 0.7768595041322314\n","Iteration 16345 - loss value [[334.0805624]] accuracy 0.7837465564738292\n","Iteration 16346 - loss value [[347.30131731]] accuracy 0.7603305785123967\n","Iteration 16347 - loss value [[357.217938]] accuracy 0.7672176308539945\n","Iteration 16348 - loss value [[397.90154207]] accuracy 0.7024793388429752\n","Iteration 16349 - loss value [[331.36668286]] accuracy 0.7713498622589532\n","Iteration 16350 - loss value [[326.92641719]] accuracy 0.7796143250688705\n","Iteration 16351 - loss value [[325.17466881]] accuracy 0.7865013774104683\n","Iteration 16352 - loss value [[325.24881447]] accuracy 0.7837465564738292\n","Iteration 16353 - loss value [[325.44723237]] accuracy 0.7865013774104683\n","Iteration 16354 - loss value [[326.6702016]] accuracy 0.7823691460055097\n","Iteration 16355 - loss value [[330.03053603]] accuracy 0.7851239669421488\n","Iteration 16356 - loss value [[334.06411402]] accuracy 0.7713498622589532\n","Iteration 16357 - loss value [[327.79054508]] accuracy 0.7851239669421488\n","Iteration 16358 - loss value [[327.68753904]] accuracy 0.7796143250688705\n","Iteration 16359 - loss value [[327.77191166]] accuracy 0.7823691460055097\n","Iteration 16360 - loss value [[327.4216895]] accuracy 0.778236914600551\n","Iteration 16361 - loss value [[328.16566016]] accuracy 0.7851239669421488\n","Iteration 16362 - loss value [[329.59772367]] accuracy 0.7754820936639119\n","Iteration 16363 - loss value [[330.73432121]] accuracy 0.7851239669421488\n","Iteration 16364 - loss value [[331.09617937]] accuracy 0.778236914600551\n","Iteration 16365 - loss value [[336.05463373]] accuracy 0.7837465564738292\n","Iteration 16366 - loss value [[353.73654844]] accuracy 0.7548209366391184\n","Iteration 16367 - loss value [[325.74853123]] accuracy 0.7837465564738292\n","Iteration 16368 - loss value [[326.45780256]] accuracy 0.78099173553719\n","Iteration 16369 - loss value [[330.53647573]] accuracy 0.7837465564738292\n","Iteration 16370 - loss value [[327.15063764]] accuracy 0.778236914600551\n","Iteration 16371 - loss value [[325.85269597]] accuracy 0.7837465564738292\n","Iteration 16372 - loss value [[325.41333044]] accuracy 0.7851239669421488\n","Iteration 16373 - loss value [[326.44531199]] accuracy 0.78099173553719\n","Iteration 16374 - loss value [[327.32546125]] accuracy 0.7796143250688705\n","Iteration 16375 - loss value [[329.43194131]] accuracy 0.7865013774104683\n","Iteration 16376 - loss value [[332.52398019]] accuracy 0.7741046831955923\n","Iteration 16377 - loss value [[330.2239286]] accuracy 0.78099173553719\n","Iteration 16378 - loss value [[329.10349113]] accuracy 0.7768595041322314\n","Iteration 16379 - loss value [[331.52364183]] accuracy 0.7837465564738292\n","Iteration 16380 - loss value [[332.37493966]] accuracy 0.7768595041322314\n","Iteration 16381 - loss value [[333.20905648]] accuracy 0.7851239669421488\n","Iteration 16382 - loss value [[341.23792341]] accuracy 0.7713498622589532\n","Iteration 16383 - loss value [[340.69191609]] accuracy 0.7754820936639119\n","Iteration 16384 - loss value [[360.08940787]] accuracy 0.7465564738292011\n","Iteration 16385 - loss value [[326.11052852]] accuracy 0.7837465564738292\n","Iteration 16386 - loss value [[325.53124297]] accuracy 0.7837465564738292\n","Iteration 16387 - loss value [[326.63007319]] accuracy 0.7865013774104683\n","Iteration 16388 - loss value [[327.62666876]] accuracy 0.7837465564738292\n","Iteration 16389 - loss value [[327.64380283]] accuracy 0.7837465564738292\n","Iteration 16390 - loss value [[326.92722055]] accuracy 0.7837465564738292\n","Iteration 16391 - loss value [[328.87443669]] accuracy 0.7823691460055097\n","Iteration 16392 - loss value [[328.41131726]] accuracy 0.778236914600551\n","Iteration 16393 - loss value [[328.88853999]] accuracy 0.78099173553719\n","Iteration 16394 - loss value [[328.18821301]] accuracy 0.7796143250688705\n","Iteration 16395 - loss value [[329.24141771]] accuracy 0.78099173553719\n","Iteration 16396 - loss value [[326.60722981]] accuracy 0.78099173553719\n","Iteration 16397 - loss value [[326.39465496]] accuracy 0.78099173553719\n","Iteration 16398 - loss value [[325.69329506]] accuracy 0.7837465564738292\n","Iteration 16399 - loss value [[325.95764549]] accuracy 0.7851239669421488\n","Iteration 16400 - loss value [[326.96890219]] accuracy 0.7796143250688705\n","Iteration 16401 - loss value [[329.13558376]] accuracy 0.7796143250688705\n","Iteration 16402 - loss value [[330.55239032]] accuracy 0.7823691460055097\n","Iteration 16403 - loss value [[330.04897583]] accuracy 0.7727272727272727\n","Iteration 16404 - loss value [[333.32041284]] accuracy 0.78099173553719\n","Iteration 16405 - loss value [[338.61883071]] accuracy 0.768595041322314\n","Iteration 16406 - loss value [[333.36926518]] accuracy 0.778236914600551\n","Iteration 16407 - loss value [[334.10270203]] accuracy 0.7713498622589532\n","Iteration 16408 - loss value [[337.05626752]] accuracy 0.7823691460055097\n","Iteration 16409 - loss value [[354.92520932]] accuracy 0.7520661157024794\n","Iteration 16410 - loss value [[325.70685227]] accuracy 0.7837465564738292\n","Iteration 16411 - loss value [[326.47594533]] accuracy 0.7837465564738292\n","Iteration 16412 - loss value [[330.40844009]] accuracy 0.7823691460055097\n","Iteration 16413 - loss value [[327.25245137]] accuracy 0.7796143250688705\n","Iteration 16414 - loss value [[326.34987407]] accuracy 0.7865013774104683\n","Iteration 16415 - loss value [[325.65666731]] accuracy 0.7837465564738292\n","Iteration 16416 - loss value [[326.02166865]] accuracy 0.7851239669421488\n","Iteration 16417 - loss value [[328.18614915]] accuracy 0.778236914600551\n","Iteration 16418 - loss value [[332.82109003]] accuracy 0.778236914600551\n","Iteration 16419 - loss value [[330.07093415]] accuracy 0.7796143250688705\n","Iteration 16420 - loss value [[331.30777489]] accuracy 0.7837465564738292\n","Iteration 16421 - loss value [[334.69875391]] accuracy 0.7741046831955923\n","Iteration 16422 - loss value [[339.38229808]] accuracy 0.7768595041322314\n","Iteration 16423 - loss value [[361.17050159]] accuracy 0.7451790633608816\n","Iteration 16424 - loss value [[326.82494449]] accuracy 0.7823691460055097\n","Iteration 16425 - loss value [[325.46992188]] accuracy 0.7865013774104683\n","Iteration 16426 - loss value [[326.33006108]] accuracy 0.78099173553719\n","Iteration 16427 - loss value [[330.50866183]] accuracy 0.7837465564738292\n","Iteration 16428 - loss value [[332.38523695]] accuracy 0.7754820936639119\n","Iteration 16429 - loss value [[329.88054819]] accuracy 0.78099173553719\n","Iteration 16430 - loss value [[326.92849833]] accuracy 0.7768595041322314\n","Iteration 16431 - loss value [[326.46176539]] accuracy 0.7823691460055097\n","Iteration 16432 - loss value [[325.78121899]] accuracy 0.7837465564738292\n","Iteration 16433 - loss value [[326.27748897]] accuracy 0.7851239669421488\n","Iteration 16434 - loss value [[326.10165127]] accuracy 0.7823691460055097\n","Iteration 16435 - loss value [[325.39815236]] accuracy 0.7878787878787878\n","Iteration 16436 - loss value [[325.79774028]] accuracy 0.7837465564738292\n","Iteration 16437 - loss value [[326.6051732]] accuracy 0.7865013774104683\n","Iteration 16438 - loss value [[326.7727859]] accuracy 0.78099173553719\n","Iteration 16439 - loss value [[330.83057826]] accuracy 0.7796143250688705\n","Iteration 16440 - loss value [[327.86361888]] accuracy 0.78099173553719\n","Iteration 16441 - loss value [[326.09881122]] accuracy 0.7823691460055097\n","Iteration 16442 - loss value [[325.55074049]] accuracy 0.7851239669421488\n","Iteration 16443 - loss value [[325.87545197]] accuracy 0.7823691460055097\n","Iteration 16444 - loss value [[328.22037234]] accuracy 0.7837465564738292\n","Iteration 16445 - loss value [[328.11902545]] accuracy 0.7768595041322314\n","Iteration 16446 - loss value [[331.02613125]] accuracy 0.7851239669421488\n","Iteration 16447 - loss value [[330.62769892]] accuracy 0.778236914600551\n","Iteration 16448 - loss value [[332.40879425]] accuracy 0.78099173553719\n","Iteration 16449 - loss value [[340.27027915]] accuracy 0.7672176308539945\n","Iteration 16450 - loss value [[337.58478942]] accuracy 0.7768595041322314\n","Iteration 16451 - loss value [[353.88775169]] accuracy 0.7520661157024794\n","Iteration 16452 - loss value [[326.14402231]] accuracy 0.7837465564738292\n","Iteration 16453 - loss value [[327.29124075]] accuracy 0.7823691460055097\n","Iteration 16454 - loss value [[327.60569748]] accuracy 0.7851239669421488\n","Iteration 16455 - loss value [[326.80872824]] accuracy 0.78099173553719\n","Iteration 16456 - loss value [[328.82507793]] accuracy 0.7837465564738292\n","Iteration 16457 - loss value [[328.32424566]] accuracy 0.7796143250688705\n","Iteration 16458 - loss value [[330.97189825]] accuracy 0.7796143250688705\n","Iteration 16459 - loss value [[327.95642991]] accuracy 0.7823691460055097\n","Iteration 16460 - loss value [[326.43343521]] accuracy 0.7823691460055097\n","Iteration 16461 - loss value [[325.55673426]] accuracy 0.7823691460055097\n","Iteration 16462 - loss value [[325.00857555]] accuracy 0.7878787878787878\n","Iteration 16463 - loss value [[325.08600128]] accuracy 0.7865013774104683\n","Iteration 16464 - loss value [[325.46625118]] accuracy 0.7865013774104683\n","Iteration 16465 - loss value [[327.06691234]] accuracy 0.7851239669421488\n","Iteration 16466 - loss value [[327.33192571]] accuracy 0.78099173553719\n","Iteration 16467 - loss value [[331.43697344]] accuracy 0.7851239669421488\n","Iteration 16468 - loss value [[328.84232485]] accuracy 0.78099173553719\n","Iteration 16469 - loss value [[328.72377007]] accuracy 0.78099173553719\n","Iteration 16470 - loss value [[326.76559708]] accuracy 0.78099173553719\n","Iteration 16471 - loss value [[326.39311727]] accuracy 0.7823691460055097\n","Iteration 16472 - loss value [[326.16912553]] accuracy 0.7837465564738292\n","Iteration 16473 - loss value [[326.18950503]] accuracy 0.7837465564738292\n","Iteration 16474 - loss value [[325.90181026]] accuracy 0.7823691460055097\n","Iteration 16475 - loss value [[325.80602619]] accuracy 0.7851239669421488\n","Iteration 16476 - loss value [[325.53544475]] accuracy 0.7796143250688705\n","Iteration 16477 - loss value [[325.23484818]] accuracy 0.7878787878787878\n","Iteration 16478 - loss value [[325.70677874]] accuracy 0.7837465564738292\n","Iteration 16479 - loss value [[327.69461916]] accuracy 0.7796143250688705\n","Iteration 16480 - loss value [[329.8211447]] accuracy 0.7865013774104683\n","Iteration 16481 - loss value [[333.36074923]] accuracy 0.7741046831955923\n","Iteration 16482 - loss value [[327.85047717]] accuracy 0.7851239669421488\n","Iteration 16483 - loss value [[329.55205552]] accuracy 0.7768595041322314\n","Iteration 16484 - loss value [[331.89442731]] accuracy 0.778236914600551\n","Iteration 16485 - loss value [[328.01121024]] accuracy 0.7823691460055097\n","Iteration 16486 - loss value [[326.31886719]] accuracy 0.7823691460055097\n","Iteration 16487 - loss value [[325.65715799]] accuracy 0.7823691460055097\n","Iteration 16488 - loss value [[325.5975961]] accuracy 0.7851239669421488\n","Iteration 16489 - loss value [[327.60010486]] accuracy 0.7837465564738292\n","Iteration 16490 - loss value [[327.21301189]] accuracy 0.7823691460055097\n","Iteration 16491 - loss value [[326.48031258]] accuracy 0.7837465564738292\n","Iteration 16492 - loss value [[328.64266074]] accuracy 0.7796143250688705\n","Iteration 16493 - loss value [[330.44318676]] accuracy 0.7823691460055097\n","Iteration 16494 - loss value [[327.04698095]] accuracy 0.7796143250688705\n","Iteration 16495 - loss value [[326.30700307]] accuracy 0.7851239669421488\n","Iteration 16496 - loss value [[325.75827558]] accuracy 0.7823691460055097\n","Iteration 16497 - loss value [[325.92093075]] accuracy 0.7865013774104683\n","Iteration 16498 - loss value [[326.62073705]] accuracy 0.78099173553719\n","Iteration 16499 - loss value [[325.57490838]] accuracy 0.7865013774104683\n","Iteration 16500 - loss value [[327.11214931]] accuracy 0.7823691460055097\n","Iteration 16501 - loss value [[326.93466941]] accuracy 0.78099173553719\n","Iteration 16502 - loss value [[329.47145392]] accuracy 0.7837465564738292\n","Iteration 16503 - loss value [[329.80792792]] accuracy 0.7754820936639119\n","Iteration 16504 - loss value [[330.25721312]] accuracy 0.78099173553719\n","Iteration 16505 - loss value [[327.99550323]] accuracy 0.7796143250688705\n","Iteration 16506 - loss value [[326.37517378]] accuracy 0.7837465564738292\n","Iteration 16507 - loss value [[325.74503192]] accuracy 0.7837465564738292\n","Iteration 16508 - loss value [[325.69233286]] accuracy 0.7851239669421488\n","Iteration 16509 - loss value [[327.88519499]] accuracy 0.7851239669421488\n","Iteration 16510 - loss value [[328.36567307]] accuracy 0.778236914600551\n","Iteration 16511 - loss value [[328.29890119]] accuracy 0.7851239669421488\n","Iteration 16512 - loss value [[329.49449086]] accuracy 0.7768595041322314\n","Iteration 16513 - loss value [[331.97143033]] accuracy 0.7837465564738292\n","Iteration 16514 - loss value [[329.3950716]] accuracy 0.778236914600551\n","Iteration 16515 - loss value [[329.89678871]] accuracy 0.7851239669421488\n","Iteration 16516 - loss value [[328.10300835]] accuracy 0.778236914600551\n","Iteration 16517 - loss value [[327.50200182]] accuracy 0.7851239669421488\n","Iteration 16518 - loss value [[327.60343795]] accuracy 0.7796143250688705\n","Iteration 16519 - loss value [[331.79539066]] accuracy 0.7823691460055097\n","Iteration 16520 - loss value [[332.51153876]] accuracy 0.7768595041322314\n","Iteration 16521 - loss value [[333.36410851]] accuracy 0.7837465564738292\n","Iteration 16522 - loss value [[340.03908585]] accuracy 0.768595041322314\n","Iteration 16523 - loss value [[337.11780004]] accuracy 0.7768595041322314\n","Iteration 16524 - loss value [[353.45785464]] accuracy 0.7548209366391184\n","Iteration 16525 - loss value [[325.35669621]] accuracy 0.7851239669421488\n","Iteration 16526 - loss value [[325.43648946]] accuracy 0.7851239669421488\n","Iteration 16527 - loss value [[326.96706315]] accuracy 0.7823691460055097\n","Iteration 16528 - loss value [[329.35072565]] accuracy 0.7754820936639119\n","Iteration 16529 - loss value [[333.77574709]] accuracy 0.7837465564738292\n","Iteration 16530 - loss value [[340.68353386]] accuracy 0.768595041322314\n","Iteration 16531 - loss value [[337.09003012]] accuracy 0.78099173553719\n","Iteration 16532 - loss value [[346.94681529]] accuracy 0.7575757575757576\n","Iteration 16533 - loss value [[335.02092446]] accuracy 0.78099173553719\n","Iteration 16534 - loss value [[333.8627172]] accuracy 0.7727272727272727\n","Iteration 16535 - loss value [[335.96637215]] accuracy 0.7796143250688705\n","Iteration 16536 - loss value [[352.34343172]] accuracy 0.7520661157024794\n","Iteration 16537 - loss value [[333.15984333]] accuracy 0.78099173553719\n","Iteration 16538 - loss value [[338.57064077]] accuracy 0.7713498622589532\n","Iteration 16539 - loss value [[339.77237147]] accuracy 0.7727272727272727\n","Iteration 16540 - loss value [[359.50199961]] accuracy 0.7479338842975206\n","Iteration 16541 - loss value [[326.40837283]] accuracy 0.7823691460055097\n","Iteration 16542 - loss value [[325.19650545]] accuracy 0.7865013774104683\n","Iteration 16543 - loss value [[326.46455589]] accuracy 0.7796143250688705\n","Iteration 16544 - loss value [[331.46083264]] accuracy 0.7823691460055097\n","Iteration 16545 - loss value [[328.32662018]] accuracy 0.7796143250688705\n","Iteration 16546 - loss value [[326.57260123]] accuracy 0.7851239669421488\n","Iteration 16547 - loss value [[325.55708726]] accuracy 0.7851239669421488\n","Iteration 16548 - loss value [[325.29944451]] accuracy 0.7865013774104683\n","Iteration 16549 - loss value [[326.26390339]] accuracy 0.78099173553719\n","Iteration 16550 - loss value [[328.47875639]] accuracy 0.7837465564738292\n","Iteration 16551 - loss value [[330.25804808]] accuracy 0.7768595041322314\n","Iteration 16552 - loss value [[329.8240034]] accuracy 0.7796143250688705\n","Iteration 16553 - loss value [[327.67195865]] accuracy 0.7796143250688705\n","Iteration 16554 - loss value [[327.37393061]] accuracy 0.7837465564738292\n","Iteration 16555 - loss value [[325.74780811]] accuracy 0.78099173553719\n","Iteration 16556 - loss value [[325.03048201]] accuracy 0.7865013774104683\n","Iteration 16557 - loss value [[325.27238315]] accuracy 0.7865013774104683\n","Iteration 16558 - loss value [[326.43136229]] accuracy 0.7851239669421488\n","Iteration 16559 - loss value [[328.45080263]] accuracy 0.78099173553719\n","Iteration 16560 - loss value [[330.13198096]] accuracy 0.7837465564738292\n","Iteration 16561 - loss value [[327.73666212]] accuracy 0.7823691460055097\n","Iteration 16562 - loss value [[328.24706457]] accuracy 0.7851239669421488\n","Iteration 16563 - loss value [[327.77571736]] accuracy 0.7823691460055097\n","Iteration 16564 - loss value [[329.6857658]] accuracy 0.7823691460055097\n","Iteration 16565 - loss value [[331.75923438]] accuracy 0.7741046831955923\n","Iteration 16566 - loss value [[330.23260436]] accuracy 0.7823691460055097\n","Iteration 16567 - loss value [[327.83503538]] accuracy 0.7796143250688705\n","Iteration 16568 - loss value [[326.78377622]] accuracy 0.7851239669421488\n","Iteration 16569 - loss value [[325.65776906]] accuracy 0.7837465564738292\n","Iteration 16570 - loss value [[325.67485393]] accuracy 0.7851239669421488\n","Iteration 16571 - loss value [[328.0339662]] accuracy 0.7768595041322314\n","Iteration 16572 - loss value [[330.09415661]] accuracy 0.7837465564738292\n","Iteration 16573 - loss value [[332.98250874]] accuracy 0.7754820936639119\n","Iteration 16574 - loss value [[329.1194744]] accuracy 0.7851239669421488\n","Iteration 16575 - loss value [[331.60745945]] accuracy 0.7741046831955923\n","Iteration 16576 - loss value [[332.56091831]] accuracy 0.7796143250688705\n","Iteration 16577 - loss value [[342.21432611]] accuracy 0.7658402203856749\n","Iteration 16578 - loss value [[334.07868426]] accuracy 0.778236914600551\n","Iteration 16579 - loss value [[341.71899477]] accuracy 0.7617079889807162\n","Iteration 16580 - loss value [[338.34841672]] accuracy 0.7768595041322314\n","Iteration 16581 - loss value [[347.3388136]] accuracy 0.7589531680440771\n","Iteration 16582 - loss value [[326.90194795]] accuracy 0.7768595041322314\n","Iteration 16583 - loss value [[327.28933299]] accuracy 0.7851239669421488\n","Iteration 16584 - loss value [[325.97810685]] accuracy 0.7837465564738292\n","Iteration 16585 - loss value [[326.91257699]] accuracy 0.7851239669421488\n","Iteration 16586 - loss value [[326.80198545]] accuracy 0.7796143250688705\n","Iteration 16587 - loss value [[329.9325578]] accuracy 0.7837465564738292\n","Iteration 16588 - loss value [[332.14053752]] accuracy 0.7741046831955923\n","Iteration 16589 - loss value [[327.96376125]] accuracy 0.7823691460055097\n","Iteration 16590 - loss value [[327.66471081]] accuracy 0.7796143250688705\n","Iteration 16591 - loss value [[331.74834126]] accuracy 0.7823691460055097\n","Iteration 16592 - loss value [[329.65717526]] accuracy 0.7768595041322314\n","Iteration 16593 - loss value [[330.99884129]] accuracy 0.7823691460055097\n","Iteration 16594 - loss value [[328.10002966]] accuracy 0.7796143250688705\n","Iteration 16595 - loss value [[326.31907949]] accuracy 0.7851239669421488\n","Iteration 16596 - loss value [[325.44038079]] accuracy 0.7823691460055097\n","Iteration 16597 - loss value [[324.98178337]] accuracy 0.7878787878787878\n","Iteration 16598 - loss value [[325.20146197]] accuracy 0.7865013774104683\n","Iteration 16599 - loss value [[326.1737338]] accuracy 0.7823691460055097\n","Iteration 16600 - loss value [[330.28332894]] accuracy 0.7851239669421488\n","Iteration 16601 - loss value [[334.43759569]] accuracy 0.7713498622589532\n","Iteration 16602 - loss value [[328.6889372]] accuracy 0.7851239669421488\n","Iteration 16603 - loss value [[330.38420396]] accuracy 0.7768595041322314\n","Iteration 16604 - loss value [[331.18337368]] accuracy 0.7851239669421488\n","Iteration 16605 - loss value [[328.68698608]] accuracy 0.78099173553719\n","Iteration 16606 - loss value [[328.44015849]] accuracy 0.7823691460055097\n","Iteration 16607 - loss value [[326.76825063]] accuracy 0.78099173553719\n","Iteration 16608 - loss value [[326.04012682]] accuracy 0.78099173553719\n","Iteration 16609 - loss value [[325.4469193]] accuracy 0.7837465564738292\n","Iteration 16610 - loss value [[326.45101826]] accuracy 0.7823691460055097\n","Iteration 16611 - loss value [[329.18393281]] accuracy 0.7865013774104683\n","Iteration 16612 - loss value [[332.52153769]] accuracy 0.7741046831955923\n","Iteration 16613 - loss value [[328.09682071]] accuracy 0.78099173553719\n","Iteration 16614 - loss value [[327.78240352]] accuracy 0.7796143250688705\n","Iteration 16615 - loss value [[328.74468144]] accuracy 0.7837465564738292\n","Iteration 16616 - loss value [[328.52444855]] accuracy 0.7796143250688705\n","Iteration 16617 - loss value [[332.64908008]] accuracy 0.7796143250688705\n","Iteration 16618 - loss value [[332.10228648]] accuracy 0.778236914600551\n","Iteration 16619 - loss value [[335.03289476]] accuracy 0.7837465564738292\n","Iteration 16620 - loss value [[350.07902832]] accuracy 0.7575757575757576\n","Iteration 16621 - loss value [[330.04054574]] accuracy 0.7851239669421488\n","Iteration 16622 - loss value [[333.67823763]] accuracy 0.7741046831955923\n","Iteration 16623 - loss value [[329.26026342]] accuracy 0.7837465564738292\n","Iteration 16624 - loss value [[331.09991061]] accuracy 0.7727272727272727\n","Iteration 16625 - loss value [[330.89344903]] accuracy 0.7837465564738292\n","Iteration 16626 - loss value [[327.68495828]] accuracy 0.778236914600551\n","Iteration 16627 - loss value [[326.77803144]] accuracy 0.7851239669421488\n","Iteration 16628 - loss value [[326.08319537]] accuracy 0.78099173553719\n","Iteration 16629 - loss value [[325.36731027]] accuracy 0.7837465564738292\n","Iteration 16630 - loss value [[325.14194289]] accuracy 0.7851239669421488\n","Iteration 16631 - loss value [[325.37929855]] accuracy 0.7851239669421488\n","Iteration 16632 - loss value [[325.61481877]] accuracy 0.7851239669421488\n","Iteration 16633 - loss value [[326.24642043]] accuracy 0.78099173553719\n","Iteration 16634 - loss value [[327.56919175]] accuracy 0.78099173553719\n","Iteration 16635 - loss value [[328.83319914]] accuracy 0.78099173553719\n","Iteration 16636 - loss value [[329.72683837]] accuracy 0.7754820936639119\n","Iteration 16637 - loss value [[333.76384244]] accuracy 0.7851239669421488\n","Iteration 16638 - loss value [[348.56453662]] accuracy 0.7575757575757576\n","Iteration 16639 - loss value [[333.00373006]] accuracy 0.7796143250688705\n","Iteration 16640 - loss value [[334.07572286]] accuracy 0.7741046831955923\n","Iteration 16641 - loss value [[338.80556437]] accuracy 0.7741046831955923\n","Iteration 16642 - loss value [[362.14177863]] accuracy 0.7451790633608816\n","Iteration 16643 - loss value [[325.445185]] accuracy 0.7865013774104683\n","Iteration 16644 - loss value [[325.44658748]] accuracy 0.7851239669421488\n","Iteration 16645 - loss value [[326.29160591]] accuracy 0.7796143250688705\n","Iteration 16646 - loss value [[331.00232131]] accuracy 0.78099173553719\n","Iteration 16647 - loss value [[336.40813688]] accuracy 0.7741046831955923\n","Iteration 16648 - loss value [[334.06027762]] accuracy 0.778236914600551\n","Iteration 16649 - loss value [[337.18741734]] accuracy 0.7699724517906336\n","Iteration 16650 - loss value [[333.67639086]] accuracy 0.7837465564738292\n","Iteration 16651 - loss value [[348.76115156]] accuracy 0.7603305785123967\n","Iteration 16652 - loss value [[346.18931539]] accuracy 0.7741046831955923\n","Iteration 16653 - loss value [[375.53166409]] accuracy 0.7341597796143251\n","Iteration 16654 - loss value [[329.34817946]] accuracy 0.7837465564738292\n","Iteration 16655 - loss value [[329.44807429]] accuracy 0.7754820936639119\n","Iteration 16656 - loss value [[328.24721626]] accuracy 0.7823691460055097\n","Iteration 16657 - loss value [[326.64510136]] accuracy 0.778236914600551\n","Iteration 16658 - loss value [[328.24459781]] accuracy 0.7837465564738292\n","Iteration 16659 - loss value [[326.60953724]] accuracy 0.78099173553719\n","Iteration 16660 - loss value [[329.09075949]] accuracy 0.7823691460055097\n","Iteration 16661 - loss value [[327.52686009]] accuracy 0.78099173553719\n","Iteration 16662 - loss value [[328.46579871]] accuracy 0.7837465564738292\n","Iteration 16663 - loss value [[329.07435248]] accuracy 0.778236914600551\n","Iteration 16664 - loss value [[331.02403212]] accuracy 0.78099173553719\n","Iteration 16665 - loss value [[334.02033256]] accuracy 0.7754820936639119\n","Iteration 16666 - loss value [[334.95679716]] accuracy 0.7823691460055097\n","Iteration 16667 - loss value [[348.71488495]] accuracy 0.7589531680440771\n","Iteration 16668 - loss value [[337.60806423]] accuracy 0.7796143250688705\n","Iteration 16669 - loss value [[353.26814024]] accuracy 0.756198347107438\n","Iteration 16670 - loss value [[325.69253066]] accuracy 0.7837465564738292\n","Iteration 16671 - loss value [[326.27524262]] accuracy 0.7823691460055097\n","Iteration 16672 - loss value [[330.06737495]] accuracy 0.7837465564738292\n","Iteration 16673 - loss value [[327.25289202]] accuracy 0.78099173553719\n","Iteration 16674 - loss value [[326.63284457]] accuracy 0.7837465564738292\n","Iteration 16675 - loss value [[325.65031127]] accuracy 0.7837465564738292\n","Iteration 16676 - loss value [[325.64594765]] accuracy 0.7837465564738292\n","Iteration 16677 - loss value [[326.97416903]] accuracy 0.78099173553719\n","Iteration 16678 - loss value [[329.95797937]] accuracy 0.7754820936639119\n","Iteration 16679 - loss value [[328.55385483]] accuracy 0.7837465564738292\n","Iteration 16680 - loss value [[331.61331332]] accuracy 0.7727272727272727\n","Iteration 16681 - loss value [[334.67013103]] accuracy 0.7796143250688705\n","Iteration 16682 - loss value [[346.38739027]] accuracy 0.7603305785123967\n","Iteration 16683 - loss value [[331.60143826]] accuracy 0.7865013774104683\n","Iteration 16684 - loss value [[335.19626149]] accuracy 0.7768595041322314\n","Iteration 16685 - loss value [[329.1091802]] accuracy 0.7837465564738292\n","Iteration 16686 - loss value [[331.21717324]] accuracy 0.7741046831955923\n","Iteration 16687 - loss value [[329.50659925]] accuracy 0.7837465564738292\n","Iteration 16688 - loss value [[331.80215017]] accuracy 0.7754820936639119\n","Iteration 16689 - loss value [[327.14764413]] accuracy 0.7878787878787878\n","Iteration 16690 - loss value [[328.45039283]] accuracy 0.7754820936639119\n","Iteration 16691 - loss value [[334.04812475]] accuracy 0.78099173553719\n","Iteration 16692 - loss value [[335.10497017]] accuracy 0.7727272727272727\n","Iteration 16693 - loss value [[332.03537027]] accuracy 0.7851239669421488\n","Iteration 16694 - loss value [[333.21271741]] accuracy 0.778236914600551\n","Iteration 16695 - loss value [[345.15130141]] accuracy 0.7699724517906336\n","Iteration 16696 - loss value [[371.77569319]] accuracy 0.7424242424242424\n","Iteration 16697 - loss value [[325.76428895]] accuracy 0.7837465564738292\n","Iteration 16698 - loss value [[325.58868802]] accuracy 0.7823691460055097\n","Iteration 16699 - loss value [[326.23303177]] accuracy 0.7851239669421488\n","Iteration 16700 - loss value [[326.88901702]] accuracy 0.7796143250688705\n","Iteration 16701 - loss value [[328.2533765]] accuracy 0.7837465564738292\n","Iteration 16702 - loss value [[329.57657101]] accuracy 0.778236914600551\n","Iteration 16703 - loss value [[330.85719064]] accuracy 0.7796143250688705\n","Iteration 16704 - loss value [[327.65893049]] accuracy 0.7823691460055097\n","Iteration 16705 - loss value [[326.69127297]] accuracy 0.7837465564738292\n","Iteration 16706 - loss value [[325.7414965]] accuracy 0.7837465564738292\n","Iteration 16707 - loss value [[325.86212099]] accuracy 0.7865013774104683\n","Iteration 16708 - loss value [[326.59324154]] accuracy 0.78099173553719\n","Iteration 16709 - loss value [[325.04891789]] accuracy 0.7878787878787878\n","Iteration 16710 - loss value [[325.49809492]] accuracy 0.7837465564738292\n","Iteration 16711 - loss value [[327.06743734]] accuracy 0.7796143250688705\n","Iteration 16712 - loss value [[333.95196261]] accuracy 0.7851239669421488\n","Iteration 16713 - loss value [[339.33049551]] accuracy 0.7699724517906336\n","Iteration 16714 - loss value [[334.36992288]] accuracy 0.7823691460055097\n","Iteration 16715 - loss value [[342.70204753]] accuracy 0.7658402203856749\n","Iteration 16716 - loss value [[344.35521707]] accuracy 0.7754820936639119\n","Iteration 16717 - loss value [[373.76590963]] accuracy 0.7382920110192838\n","Iteration 16718 - loss value [[332.84408596]] accuracy 0.7823691460055097\n","Iteration 16719 - loss value [[335.41460914]] accuracy 0.7713498622589532\n","Iteration 16720 - loss value [[333.06866185]] accuracy 0.78099173553719\n","Iteration 16721 - loss value [[329.81722197]] accuracy 0.778236914600551\n","Iteration 16722 - loss value [[330.53247191]] accuracy 0.7851239669421488\n","Iteration 16723 - loss value [[329.56142013]] accuracy 0.7768595041322314\n","Iteration 16724 - loss value [[331.36493098]] accuracy 0.7823691460055097\n","Iteration 16725 - loss value [[329.95160583]] accuracy 0.778236914600551\n","Iteration 16726 - loss value [[329.47374669]] accuracy 0.7851239669421488\n","Iteration 16727 - loss value [[328.01308259]] accuracy 0.7768595041322314\n","Iteration 16728 - loss value [[326.49915015]] accuracy 0.7823691460055097\n","Iteration 16729 - loss value [[325.87268277]] accuracy 0.7823691460055097\n","Iteration 16730 - loss value [[325.76858961]] accuracy 0.7837465564738292\n","Iteration 16731 - loss value [[325.63995999]] accuracy 0.7851239669421488\n","Iteration 16732 - loss value [[327.2467684]] accuracy 0.7837465564738292\n","Iteration 16733 - loss value [[326.59030074]] accuracy 0.7823691460055097\n","Iteration 16734 - loss value [[327.91928089]] accuracy 0.78099173553719\n","Iteration 16735 - loss value [[328.60835141]] accuracy 0.778236914600551\n","Iteration 16736 - loss value [[330.2305116]] accuracy 0.7823691460055097\n","Iteration 16737 - loss value [[327.81706257]] accuracy 0.7796143250688705\n","Iteration 16738 - loss value [[327.68770116]] accuracy 0.7851239669421488\n","Iteration 16739 - loss value [[328.2658105]] accuracy 0.778236914600551\n","Iteration 16740 - loss value [[332.35594131]] accuracy 0.7851239669421488\n","Iteration 16741 - loss value [[335.30223749]] accuracy 0.7768595041322314\n","Iteration 16742 - loss value [[337.26792771]] accuracy 0.78099173553719\n","Iteration 16743 - loss value [[354.91602209]] accuracy 0.7548209366391184\n","Iteration 16744 - loss value [[325.22451749]] accuracy 0.7865013774104683\n","Iteration 16745 - loss value [[325.49520985]] accuracy 0.7851239669421488\n","Iteration 16746 - loss value [[327.17619348]] accuracy 0.7837465564738292\n","Iteration 16747 - loss value [[326.38323697]] accuracy 0.7837465564738292\n","Iteration 16748 - loss value [[327.60946263]] accuracy 0.7837465564738292\n","Iteration 16749 - loss value [[327.60635229]] accuracy 0.7796143250688705\n","Iteration 16750 - loss value [[327.5684239]] accuracy 0.7823691460055097\n","Iteration 16751 - loss value [[326.72748757]] accuracy 0.7837465564738292\n","Iteration 16752 - loss value [[327.63996249]] accuracy 0.7823691460055097\n","Iteration 16753 - loss value [[327.27943521]] accuracy 0.7823691460055097\n","Iteration 16754 - loss value [[327.48361471]] accuracy 0.7823691460055097\n","Iteration 16755 - loss value [[326.62351483]] accuracy 0.78099173553719\n","Iteration 16756 - loss value [[327.49295429]] accuracy 0.7837465564738292\n","Iteration 16757 - loss value [[326.67147308]] accuracy 0.78099173553719\n","Iteration 16758 - loss value [[327.55862652]] accuracy 0.7837465564738292\n","Iteration 16759 - loss value [[326.73794681]] accuracy 0.78099173553719\n","Iteration 16760 - loss value [[326.81373055]] accuracy 0.7837465564738292\n","Iteration 16761 - loss value [[326.01835791]] accuracy 0.78099173553719\n","Iteration 16762 - loss value [[325.67914865]] accuracy 0.7837465564738292\n","Iteration 16763 - loss value [[326.58558209]] accuracy 0.78099173553719\n","Iteration 16764 - loss value [[329.60676086]] accuracy 0.7851239669421488\n","Iteration 16765 - loss value [[331.70137948]] accuracy 0.7741046831955923\n","Iteration 16766 - loss value [[327.30815343]] accuracy 0.7851239669421488\n","Iteration 16767 - loss value [[325.55004168]] accuracy 0.78099173553719\n","Iteration 16768 - loss value [[325.38291597]] accuracy 0.7865013774104683\n","Iteration 16769 - loss value [[327.0058644]] accuracy 0.78099173553719\n","Iteration 16770 - loss value [[334.27957883]] accuracy 0.7851239669421488\n","Iteration 16771 - loss value [[340.21061039]] accuracy 0.7713498622589532\n","Iteration 16772 - loss value [[332.65699244]] accuracy 0.78099173553719\n","Iteration 16773 - loss value [[329.16109249]] accuracy 0.78099173553719\n","Iteration 16774 - loss value [[328.60216693]] accuracy 0.78099173553719\n","Iteration 16775 - loss value [[326.75711936]] accuracy 0.78099173553719\n","Iteration 16776 - loss value [[326.57694944]] accuracy 0.78099173553719\n","Iteration 16777 - loss value [[326.4920769]] accuracy 0.7837465564738292\n","Iteration 16778 - loss value [[326.40725267]] accuracy 0.7796143250688705\n","Iteration 16779 - loss value [[325.58105627]] accuracy 0.7823691460055097\n","Iteration 16780 - loss value [[325.03817766]] accuracy 0.7878787878787878\n","Iteration 16781 - loss value [[325.35721895]] accuracy 0.7851239669421488\n","Iteration 16782 - loss value [[326.41107444]] accuracy 0.7823691460055097\n","Iteration 16783 - loss value [[331.03420367]] accuracy 0.7837465564738292\n","Iteration 16784 - loss value [[329.17636325]] accuracy 0.7796143250688705\n","Iteration 16785 - loss value [[329.17546475]] accuracy 0.7851239669421488\n","Iteration 16786 - loss value [[327.48455846]] accuracy 0.778236914600551\n","Iteration 16787 - loss value [[326.41908991]] accuracy 0.7851239669421488\n","Iteration 16788 - loss value [[325.80091592]] accuracy 0.7823691460055097\n","Iteration 16789 - loss value [[326.49474417]] accuracy 0.7865013774104683\n","Iteration 16790 - loss value [[327.80081316]] accuracy 0.778236914600551\n","Iteration 16791 - loss value [[331.35003673]] accuracy 0.7823691460055097\n","Iteration 16792 - loss value [[327.61826266]] accuracy 0.778236914600551\n","Iteration 16793 - loss value [[326.69166032]] accuracy 0.7837465564738292\n","Iteration 16794 - loss value [[326.09045527]] accuracy 0.78099173553719\n","Iteration 16795 - loss value [[325.12400253]] accuracy 0.7851239669421488\n","Iteration 16796 - loss value [[324.87917705]] accuracy 0.7865013774104683\n","Iteration 16797 - loss value [[324.88070666]] accuracy 0.7865013774104683\n","Iteration 16798 - loss value [[324.92039986]] accuracy 0.7865013774104683\n","Iteration 16799 - loss value [[325.08177304]] accuracy 0.7865013774104683\n","Iteration 16800 - loss value [[325.5103087]] accuracy 0.7851239669421488\n","Iteration 16801 - loss value [[327.18546617]] accuracy 0.7837465564738292\n","Iteration 16802 - loss value [[325.80025022]] accuracy 0.7823691460055097\n","Iteration 16803 - loss value [[325.4350674]] accuracy 0.7851239669421488\n","Iteration 16804 - loss value [[325.61871774]] accuracy 0.7837465564738292\n","Iteration 16805 - loss value [[326.55571335]] accuracy 0.7837465564738292\n","Iteration 16806 - loss value [[329.07965478]] accuracy 0.7768595041322314\n","Iteration 16807 - loss value [[325.97722714]] accuracy 0.7851239669421488\n","Iteration 16808 - loss value [[328.40937669]] accuracy 0.778236914600551\n","Iteration 16809 - loss value [[326.11356715]] accuracy 0.7865013774104683\n","Iteration 16810 - loss value [[328.8131023]] accuracy 0.7768595041322314\n","Iteration 16811 - loss value [[329.26610133]] accuracy 0.7823691460055097\n","Iteration 16812 - loss value [[328.50933745]] accuracy 0.7796143250688705\n","Iteration 16813 - loss value [[332.92466263]] accuracy 0.778236914600551\n","Iteration 16814 - loss value [[334.02952854]] accuracy 0.7754820936639119\n","Iteration 16815 - loss value [[332.62438644]] accuracy 0.7823691460055097\n","Iteration 16816 - loss value [[336.49324546]] accuracy 0.7741046831955923\n","Iteration 16817 - loss value [[330.47432284]] accuracy 0.7837465564738292\n","Iteration 16818 - loss value [[328.13985326]] accuracy 0.7796143250688705\n","Iteration 16819 - loss value [[326.47692309]] accuracy 0.7851239669421488\n","Iteration 16820 - loss value [[326.03010684]] accuracy 0.78099173553719\n","Iteration 16821 - loss value [[325.04863823]] accuracy 0.7851239669421488\n","Iteration 16822 - loss value [[325.13612245]] accuracy 0.7865013774104683\n","Iteration 16823 - loss value [[325.45299867]] accuracy 0.7823691460055097\n","Iteration 16824 - loss value [[325.24971559]] accuracy 0.7851239669421488\n","Iteration 16825 - loss value [[326.41434626]] accuracy 0.7823691460055097\n","Iteration 16826 - loss value [[329.43110269]] accuracy 0.7837465564738292\n","Iteration 16827 - loss value [[333.04368361]] accuracy 0.7727272727272727\n","Iteration 16828 - loss value [[329.19317406]] accuracy 0.7851239669421488\n","Iteration 16829 - loss value [[331.18355193]] accuracy 0.7754820936639119\n","Iteration 16830 - loss value [[326.04924049]] accuracy 0.7851239669421488\n","Iteration 16831 - loss value [[326.2058765]] accuracy 0.78099173553719\n","Iteration 16832 - loss value [[327.90332297]] accuracy 0.7878787878787878\n","Iteration 16833 - loss value [[328.14777084]] accuracy 0.7768595041322314\n","Iteration 16834 - loss value [[333.91476234]] accuracy 0.7796143250688705\n","Iteration 16835 - loss value [[332.6061824]] accuracy 0.7727272727272727\n","Iteration 16836 - loss value [[340.72437272]] accuracy 0.7754820936639119\n","Iteration 16837 - loss value [[361.9791622]] accuracy 0.7451790633608816\n","Iteration 16838 - loss value [[326.14757497]] accuracy 0.7837465564738292\n","Iteration 16839 - loss value [[326.68874816]] accuracy 0.778236914600551\n","Iteration 16840 - loss value [[331.06531433]] accuracy 0.7837465564738292\n","Iteration 16841 - loss value [[328.37717186]] accuracy 0.7796143250688705\n","Iteration 16842 - loss value [[327.04836519]] accuracy 0.7865013774104683\n","Iteration 16843 - loss value [[326.84446775]] accuracy 0.7823691460055097\n","Iteration 16844 - loss value [[327.43930127]] accuracy 0.7865013774104683\n","Iteration 16845 - loss value [[327.32054369]] accuracy 0.7796143250688705\n","Iteration 16846 - loss value [[328.43588957]] accuracy 0.7837465564738292\n","Iteration 16847 - loss value [[326.57676878]] accuracy 0.78099173553719\n","Iteration 16848 - loss value [[326.66572418]] accuracy 0.7851239669421488\n","Iteration 16849 - loss value [[326.18240254]] accuracy 0.78099173553719\n","Iteration 16850 - loss value [[325.50026317]] accuracy 0.7837465564738292\n","Iteration 16851 - loss value [[326.29376829]] accuracy 0.7823691460055097\n","Iteration 16852 - loss value [[329.82232786]] accuracy 0.7837465564738292\n","Iteration 16853 - loss value [[328.17468923]] accuracy 0.7796143250688705\n","Iteration 16854 - loss value [[329.11233288]] accuracy 0.78099173553719\n","Iteration 16855 - loss value [[326.78258212]] accuracy 0.78099173553719\n","Iteration 16856 - loss value [[326.35657778]] accuracy 0.7851239669421488\n","Iteration 16857 - loss value [[325.48151976]] accuracy 0.7851239669421488\n","Iteration 16858 - loss value [[325.11169475]] accuracy 0.7865013774104683\n","Iteration 16859 - loss value [[325.70235439]] accuracy 0.7837465564738292\n","Iteration 16860 - loss value [[326.80182832]] accuracy 0.7837465564738292\n","Iteration 16861 - loss value [[326.48520361]] accuracy 0.78099173553719\n","Iteration 16862 - loss value [[330.49959313]] accuracy 0.7796143250688705\n","Iteration 16863 - loss value [[327.03833604]] accuracy 0.7796143250688705\n","Iteration 16864 - loss value [[325.30660757]] accuracy 0.7823691460055097\n","Iteration 16865 - loss value [[325.66553011]] accuracy 0.7837465564738292\n","Iteration 16866 - loss value [[326.5327099]] accuracy 0.7837465564738292\n","Iteration 16867 - loss value [[327.70770458]] accuracy 0.778236914600551\n","Iteration 16868 - loss value [[335.2433825]] accuracy 0.7768595041322314\n","Iteration 16869 - loss value [[338.40853269]] accuracy 0.7713498622589532\n","Iteration 16870 - loss value [[331.74434339]] accuracy 0.7865013774104683\n","Iteration 16871 - loss value [[333.03439827]] accuracy 0.7768595041322314\n","Iteration 16872 - loss value [[338.54485104]] accuracy 0.7837465564738292\n","Iteration 16873 - loss value [[361.68890978]] accuracy 0.7451790633608816\n","Iteration 16874 - loss value [[325.54050427]] accuracy 0.7837465564738292\n","Iteration 16875 - loss value [[325.62382997]] accuracy 0.7837465564738292\n","Iteration 16876 - loss value [[327.49361495]] accuracy 0.7851239669421488\n","Iteration 16877 - loss value [[327.40475375]] accuracy 0.7823691460055097\n","Iteration 16878 - loss value [[329.78285953]] accuracy 0.7837465564738292\n","Iteration 16879 - loss value [[330.14179336]] accuracy 0.778236914600551\n","Iteration 16880 - loss value [[332.52605986]] accuracy 0.7865013774104683\n","Iteration 16881 - loss value [[336.32624902]] accuracy 0.7741046831955923\n","Iteration 16882 - loss value [[330.60236944]] accuracy 0.7823691460055097\n","Iteration 16883 - loss value [[328.59248041]] accuracy 0.7796143250688705\n","Iteration 16884 - loss value [[326.68375371]] accuracy 0.7865013774104683\n","Iteration 16885 - loss value [[325.86353641]] accuracy 0.78099173553719\n","Iteration 16886 - loss value [[325.32448135]] accuracy 0.7837465564738292\n","Iteration 16887 - loss value [[325.93218805]] accuracy 0.7823691460055097\n","Iteration 16888 - loss value [[328.80366727]] accuracy 0.7878787878787878\n","Iteration 16889 - loss value [[330.87255125]] accuracy 0.778236914600551\n","Iteration 16890 - loss value [[329.6754761]] accuracy 0.7823691460055097\n","Iteration 16891 - loss value [[327.36441859]] accuracy 0.7754820936639119\n","Iteration 16892 - loss value [[325.96174966]] accuracy 0.7837465564738292\n","Iteration 16893 - loss value [[325.23327408]] accuracy 0.7837465564738292\n","Iteration 16894 - loss value [[325.09788644]] accuracy 0.7851239669421488\n","Iteration 16895 - loss value [[325.53784189]] accuracy 0.7851239669421488\n","Iteration 16896 - loss value [[326.52624743]] accuracy 0.7796143250688705\n","Iteration 16897 - loss value [[331.28803683]] accuracy 0.7823691460055097\n","Iteration 16898 - loss value [[336.25668869]] accuracy 0.7741046831955923\n","Iteration 16899 - loss value [[330.57291301]] accuracy 0.7823691460055097\n","Iteration 16900 - loss value [[328.7677179]] accuracy 0.7823691460055097\n","Iteration 16901 - loss value [[327.17524217]] accuracy 0.7851239669421488\n","Iteration 16902 - loss value [[326.55931854]] accuracy 0.7823691460055097\n","Iteration 16903 - loss value [[326.76496893]] accuracy 0.7851239669421488\n","Iteration 16904 - loss value [[325.8204746]] accuracy 0.7823691460055097\n","Iteration 16905 - loss value [[325.28170377]] accuracy 0.7823691460055097\n","Iteration 16906 - loss value [[325.46828991]] accuracy 0.7851239669421488\n","Iteration 16907 - loss value [[326.24671931]] accuracy 0.7837465564738292\n","Iteration 16908 - loss value [[328.21798942]] accuracy 0.778236914600551\n","Iteration 16909 - loss value [[326.66218868]] accuracy 0.7837465564738292\n","Iteration 16910 - loss value [[331.1532174]] accuracy 0.7713498622589532\n","Iteration 16911 - loss value [[329.44138649]] accuracy 0.7837465564738292\n","Iteration 16912 - loss value [[330.37743772]] accuracy 0.7741046831955923\n","Iteration 16913 - loss value [[333.34609687]] accuracy 0.7796143250688705\n","Iteration 16914 - loss value [[335.85139038]] accuracy 0.7727272727272727\n","Iteration 16915 - loss value [[335.42655971]] accuracy 0.7754820936639119\n","Iteration 16916 - loss value [[346.35027571]] accuracy 0.7589531680440771\n","Iteration 16917 - loss value [[331.31360235]] accuracy 0.7823691460055097\n","Iteration 16918 - loss value [[334.32790569]] accuracy 0.7727272727272727\n","Iteration 16919 - loss value [[336.04071064]] accuracy 0.7837465564738292\n","Iteration 16920 - loss value [[356.2108465]] accuracy 0.7506887052341598\n","Iteration 16921 - loss value [[326.41163365]] accuracy 0.7837465564738292\n","Iteration 16922 - loss value [[329.14824793]] accuracy 0.7754820936639119\n","Iteration 16923 - loss value [[327.36541036]] accuracy 0.7837465564738292\n","Iteration 16924 - loss value [[327.16710308]] accuracy 0.78099173553719\n","Iteration 16925 - loss value [[332.16874264]] accuracy 0.7796143250688705\n","Iteration 16926 - loss value [[341.85846351]] accuracy 0.7672176308539945\n","Iteration 16927 - loss value [[341.02961035]] accuracy 0.7768595041322314\n","Iteration 16928 - loss value [[362.97782432]] accuracy 0.7451790633608816\n","Iteration 16929 - loss value [[326.86069924]] accuracy 0.7823691460055097\n","Iteration 16930 - loss value [[325.90031545]] accuracy 0.7851239669421488\n","Iteration 16931 - loss value [[325.50438037]] accuracy 0.7865013774104683\n","Iteration 16932 - loss value [[327.26859559]] accuracy 0.7837465564738292\n","Iteration 16933 - loss value [[325.84132926]] accuracy 0.7823691460055097\n","Iteration 16934 - loss value [[325.88811355]] accuracy 0.7865013774104683\n","Iteration 16935 - loss value [[325.32594456]] accuracy 0.7851239669421488\n","Iteration 16936 - loss value [[326.39259053]] accuracy 0.7851239669421488\n","Iteration 16937 - loss value [[327.55773275]] accuracy 0.78099173553719\n","Iteration 16938 - loss value [[332.55067882]] accuracy 0.7796143250688705\n","Iteration 16939 - loss value [[329.31225752]] accuracy 0.7823691460055097\n","Iteration 16940 - loss value [[328.38595186]] accuracy 0.7837465564738292\n","Iteration 16941 - loss value [[326.65078828]] accuracy 0.7823691460055097\n","Iteration 16942 - loss value [[328.06661048]] accuracy 0.7837465564738292\n","Iteration 16943 - loss value [[328.40130431]] accuracy 0.778236914600551\n","Iteration 16944 - loss value [[332.01722091]] accuracy 0.78099173553719\n","Iteration 16945 - loss value [[338.03734107]] accuracy 0.7727272727272727\n","Iteration 16946 - loss value [[334.87496794]] accuracy 0.7823691460055097\n","Iteration 16947 - loss value [[340.86298843]] accuracy 0.7699724517906336\n","Iteration 16948 - loss value [[339.85843077]] accuracy 0.7754820936639119\n","Iteration 16949 - loss value [[358.21441378]] accuracy 0.7479338842975206\n","Iteration 16950 - loss value [[326.4685722]] accuracy 0.7796143250688705\n","Iteration 16951 - loss value [[328.2225529]] accuracy 0.7796143250688705\n","Iteration 16952 - loss value [[328.39350129]] accuracy 0.7823691460055097\n","Iteration 16953 - loss value [[326.86208168]] accuracy 0.7823691460055097\n","Iteration 16954 - loss value [[330.09643312]] accuracy 0.7823691460055097\n","Iteration 16955 - loss value [[330.44297534]] accuracy 0.7754820936639119\n","Iteration 16956 - loss value [[332.28974529]] accuracy 0.7796143250688705\n","Iteration 16957 - loss value [[334.25128643]] accuracy 0.7713498622589532\n","Iteration 16958 - loss value [[347.61179629]] accuracy 0.768595041322314\n","Iteration 16959 - loss value [[392.44386331]] accuracy 0.7079889807162535\n","Iteration 16960 - loss value [[330.36921599]] accuracy 0.7823691460055097\n","Iteration 16961 - loss value [[327.64293915]] accuracy 0.778236914600551\n","Iteration 16962 - loss value [[326.93548335]] accuracy 0.7865013774104683\n","Iteration 16963 - loss value [[326.16150551]] accuracy 0.78099173553719\n","Iteration 16964 - loss value [[326.64925631]] accuracy 0.7837465564738292\n","Iteration 16965 - loss value [[325.74782013]] accuracy 0.7837465564738292\n","Iteration 16966 - loss value [[325.9508916]] accuracy 0.7837465564738292\n","Iteration 16967 - loss value [[329.62802939]] accuracy 0.778236914600551\n","Iteration 16968 - loss value [[328.00205621]] accuracy 0.7851239669421488\n","Iteration 16969 - loss value [[326.54831227]] accuracy 0.7823691460055097\n","Iteration 16970 - loss value [[327.92462732]] accuracy 0.7823691460055097\n","Iteration 16971 - loss value [[326.84127004]] accuracy 0.7823691460055097\n","Iteration 16972 - loss value [[327.93795856]] accuracy 0.7837465564738292\n","Iteration 16973 - loss value [[326.96357995]] accuracy 0.7823691460055097\n","Iteration 16974 - loss value [[328.0962136]] accuracy 0.7837465564738292\n","Iteration 16975 - loss value [[327.41849434]] accuracy 0.7823691460055097\n","Iteration 16976 - loss value [[328.88671335]] accuracy 0.7837465564738292\n","Iteration 16977 - loss value [[329.12434969]] accuracy 0.778236914600551\n","Iteration 16978 - loss value [[329.78651289]] accuracy 0.7823691460055097\n","Iteration 16979 - loss value [[326.67550318]] accuracy 0.778236914600551\n","Iteration 16980 - loss value [[325.00694082]] accuracy 0.7851239669421488\n","Iteration 16981 - loss value [[324.88624519]] accuracy 0.7865013774104683\n","Iteration 16982 - loss value [[325.08166025]] accuracy 0.7865013774104683\n","Iteration 16983 - loss value [[325.61957271]] accuracy 0.7837465564738292\n","Iteration 16984 - loss value [[327.68643144]] accuracy 0.7837465564738292\n","Iteration 16985 - loss value [[327.13296903]] accuracy 0.78099173553719\n","Iteration 16986 - loss value [[329.13185302]] accuracy 0.7851239669421488\n","Iteration 16987 - loss value [[328.89833365]] accuracy 0.7741046831955923\n","Iteration 16988 - loss value [[334.2092831]] accuracy 0.78099173553719\n","Iteration 16989 - loss value [[344.35425492]] accuracy 0.7658402203856749\n","Iteration 16990 - loss value [[341.85662336]] accuracy 0.7768595041322314\n","Iteration 16991 - loss value [[358.06118505]] accuracy 0.7506887052341598\n","Iteration 16992 - loss value [[326.4706771]] accuracy 0.78099173553719\n","Iteration 16993 - loss value [[325.42043751]] accuracy 0.7865013774104683\n","Iteration 16994 - loss value [[327.34957574]] accuracy 0.778236914600551\n","Iteration 16995 - loss value [[331.07030018]] accuracy 0.7837465564738292\n","Iteration 16996 - loss value [[327.7594302]] accuracy 0.7796143250688705\n","Iteration 16997 - loss value [[326.67529102]] accuracy 0.7837465564738292\n","Iteration 16998 - loss value [[326.08314634]] accuracy 0.7823691460055097\n","Iteration 16999 - loss value [[326.03640264]] accuracy 0.7837465564738292\n","Iteration 17000 - loss value [[328.99480299]] accuracy 0.78099173553719\n","Iteration 17001 - loss value [[332.80972155]] accuracy 0.7713498622589532\n","Iteration 17002 - loss value [[332.4245018]] accuracy 0.7796143250688705\n","Iteration 17003 - loss value [[337.96012775]] accuracy 0.7713498622589532\n","Iteration 17004 - loss value [[337.67044638]] accuracy 0.7754820936639119\n","Iteration 17005 - loss value [[342.49546286]] accuracy 0.7672176308539945\n","Iteration 17006 - loss value [[333.22654342]] accuracy 0.7768595041322314\n","Iteration 17007 - loss value [[334.72066097]] accuracy 0.7754820936639119\n","Iteration 17008 - loss value [[342.65059039]] accuracy 0.7713498622589532\n","Iteration 17009 - loss value [[373.63318141]] accuracy 0.7327823691460055\n","Iteration 17010 - loss value [[335.89197528]] accuracy 0.7727272727272727\n","Iteration 17011 - loss value [[346.42438238]] accuracy 0.7575757575757576\n","Iteration 17012 - loss value [[328.2782294]] accuracy 0.7754820936639119\n","Iteration 17013 - loss value [[326.05658887]] accuracy 0.7851239669421488\n","Iteration 17014 - loss value [[329.65557392]] accuracy 0.7768595041322314\n","Iteration 17015 - loss value [[328.04527719]] accuracy 0.7865013774104683\n","Iteration 17016 - loss value [[326.5376593]] accuracy 0.7823691460055097\n","Iteration 17017 - loss value [[327.90534692]] accuracy 0.7837465564738292\n","Iteration 17018 - loss value [[326.85573257]] accuracy 0.7823691460055097\n","Iteration 17019 - loss value [[327.84172853]] accuracy 0.7837465564738292\n","Iteration 17020 - loss value [[326.9498048]] accuracy 0.78099173553719\n","Iteration 17021 - loss value [[327.83850643]] accuracy 0.78099173553719\n","Iteration 17022 - loss value [[327.45192143]] accuracy 0.7823691460055097\n","Iteration 17023 - loss value [[325.95608223]] accuracy 0.7837465564738292\n","Iteration 17024 - loss value [[329.4476268]] accuracy 0.7823691460055097\n","Iteration 17025 - loss value [[332.97400569]] accuracy 0.7727272727272727\n","Iteration 17026 - loss value [[327.57532304]] accuracy 0.7837465564738292\n","Iteration 17027 - loss value [[327.52354811]] accuracy 0.7796143250688705\n","Iteration 17028 - loss value [[335.00666174]] accuracy 0.7823691460055097\n","Iteration 17029 - loss value [[341.78429929]] accuracy 0.7713498622589532\n","Iteration 17030 - loss value [[341.06462112]] accuracy 0.7754820936639119\n","Iteration 17031 - loss value [[363.08128864]] accuracy 0.7424242424242424\n","Iteration 17032 - loss value [[327.6613545]] accuracy 0.7796143250688705\n","Iteration 17033 - loss value [[326.71210381]] accuracy 0.7851239669421488\n","Iteration 17034 - loss value [[325.51380343]] accuracy 0.7837465564738292\n","Iteration 17035 - loss value [[325.67931154]] accuracy 0.7851239669421488\n","Iteration 17036 - loss value [[327.99198875]] accuracy 0.778236914600551\n","Iteration 17037 - loss value [[336.04309595]] accuracy 0.778236914600551\n","Iteration 17038 - loss value [[339.82177506]] accuracy 0.768595041322314\n","Iteration 17039 - loss value [[331.40541148]] accuracy 0.7837465564738292\n","Iteration 17040 - loss value [[329.25553435]] accuracy 0.778236914600551\n","Iteration 17041 - loss value [[329.67921603]] accuracy 0.7851239669421488\n","Iteration 17042 - loss value [[327.7668673]] accuracy 0.7768595041322314\n","Iteration 17043 - loss value [[326.89845646]] accuracy 0.7823691460055097\n","Iteration 17044 - loss value [[326.44223749]] accuracy 0.7823691460055097\n","Iteration 17045 - loss value [[327.61305615]] accuracy 0.7837465564738292\n","Iteration 17046 - loss value [[327.63417077]] accuracy 0.7796143250688705\n","Iteration 17047 - loss value [[329.28084741]] accuracy 0.7823691460055097\n","Iteration 17048 - loss value [[327.01883458]] accuracy 0.7768595041322314\n","Iteration 17049 - loss value [[325.75150433]] accuracy 0.7823691460055097\n","Iteration 17050 - loss value [[325.44746582]] accuracy 0.7851239669421488\n","Iteration 17051 - loss value [[326.67581186]] accuracy 0.778236914600551\n","Iteration 17052 - loss value [[331.39569147]] accuracy 0.7865013774104683\n","Iteration 17053 - loss value [[329.53504228]] accuracy 0.7796143250688705\n","Iteration 17054 - loss value [[330.06735261]] accuracy 0.7851239669421488\n","Iteration 17055 - loss value [[329.14091893]] accuracy 0.778236914600551\n","Iteration 17056 - loss value [[329.02480048]] accuracy 0.7823691460055097\n","Iteration 17057 - loss value [[327.08649212]] accuracy 0.7796143250688705\n","Iteration 17058 - loss value [[325.72838014]] accuracy 0.7837465564738292\n","Iteration 17059 - loss value [[325.41774644]] accuracy 0.7851239669421488\n","Iteration 17060 - loss value [[326.71339396]] accuracy 0.7823691460055097\n","Iteration 17061 - loss value [[328.91317847]] accuracy 0.7741046831955923\n","Iteration 17062 - loss value [[334.48521419]] accuracy 0.7823691460055097\n","Iteration 17063 - loss value [[343.44101905]] accuracy 0.7658402203856749\n","Iteration 17064 - loss value [[339.74254069]] accuracy 0.78099173553719\n","Iteration 17065 - loss value [[354.74509061]] accuracy 0.7534435261707989\n","Iteration 17066 - loss value [[327.42002132]] accuracy 0.7796143250688705\n","Iteration 17067 - loss value [[325.58775874]] accuracy 0.7837465564738292\n","Iteration 17068 - loss value [[327.63265604]] accuracy 0.7823691460055097\n","Iteration 17069 - loss value [[327.24833891]] accuracy 0.778236914600551\n","Iteration 17070 - loss value [[331.48611179]] accuracy 0.7823691460055097\n","Iteration 17071 - loss value [[328.43265055]] accuracy 0.78099173553719\n","Iteration 17072 - loss value [[327.82770254]] accuracy 0.7865013774104683\n","Iteration 17073 - loss value [[326.00963569]] accuracy 0.78099173553719\n","Iteration 17074 - loss value [[325.20547523]] accuracy 0.7823691460055097\n","Iteration 17075 - loss value [[325.51239726]] accuracy 0.7837465564738292\n","Iteration 17076 - loss value [[326.33162225]] accuracy 0.7851239669421488\n","Iteration 17077 - loss value [[327.05001385]] accuracy 0.778236914600551\n","Iteration 17078 - loss value [[333.38669013]] accuracy 0.78099173553719\n","Iteration 17079 - loss value [[335.13547051]] accuracy 0.7741046831955923\n","Iteration 17080 - loss value [[329.4852187]] accuracy 0.78099173553719\n","Iteration 17081 - loss value [[326.88788799]] accuracy 0.7768595041322314\n","Iteration 17082 - loss value [[325.61081238]] accuracy 0.7837465564738292\n","Iteration 17083 - loss value [[324.89111162]] accuracy 0.7865013774104683\n","Iteration 17084 - loss value [[324.84280745]] accuracy 0.7865013774104683\n","Iteration 17085 - loss value [[324.9557776]] accuracy 0.7865013774104683\n","Iteration 17086 - loss value [[325.369354]] accuracy 0.7837465564738292\n","Iteration 17087 - loss value [[325.77546514]] accuracy 0.7851239669421488\n","Iteration 17088 - loss value [[328.98907021]] accuracy 0.7768595041322314\n","Iteration 17089 - loss value [[328.33461097]] accuracy 0.7796143250688705\n","Iteration 17090 - loss value [[329.60701161]] accuracy 0.7754820936639119\n","Iteration 17091 - loss value [[334.91317145]] accuracy 0.78099173553719\n","Iteration 17092 - loss value [[340.48448693]] accuracy 0.7713498622589532\n","Iteration 17093 - loss value [[334.58423777]] accuracy 0.7754820936639119\n","Iteration 17094 - loss value [[338.85655701]] accuracy 0.7727272727272727\n","Iteration 17095 - loss value [[332.12122333]] accuracy 0.7796143250688705\n","Iteration 17096 - loss value [[329.90303796]] accuracy 0.7796143250688705\n","Iteration 17097 - loss value [[330.74023789]] accuracy 0.7851239669421488\n","Iteration 17098 - loss value [[333.13194831]] accuracy 0.7741046831955923\n","Iteration 17099 - loss value [[343.4526608]] accuracy 0.7713498622589532\n","Iteration 17100 - loss value [[379.39772749]] accuracy 0.7245179063360881\n","Iteration 17101 - loss value [[332.98022834]] accuracy 0.7837465564738292\n","Iteration 17102 - loss value [[331.79074063]] accuracy 0.7741046831955923\n","Iteration 17103 - loss value [[343.58672188]] accuracy 0.7741046831955923\n","Iteration 17104 - loss value [[361.07483832]] accuracy 0.7465564738292011\n","Iteration 17105 - loss value [[326.66968326]] accuracy 0.78099173553719\n","Iteration 17106 - loss value [[325.58352273]] accuracy 0.7823691460055097\n","Iteration 17107 - loss value [[327.51980458]] accuracy 0.7823691460055097\n","Iteration 17108 - loss value [[326.5515654]] accuracy 0.7796143250688705\n","Iteration 17109 - loss value [[327.03765941]] accuracy 0.7837465564738292\n","Iteration 17110 - loss value [[325.95174254]] accuracy 0.78099173553719\n","Iteration 17111 - loss value [[325.94816943]] accuracy 0.7837465564738292\n","Iteration 17112 - loss value [[325.13715661]] accuracy 0.7837465564738292\n","Iteration 17113 - loss value [[325.04224668]] accuracy 0.7851239669421488\n","Iteration 17114 - loss value [[325.77400843]] accuracy 0.78099173553719\n","Iteration 17115 - loss value [[327.6787485]] accuracy 0.7865013774104683\n","Iteration 17116 - loss value [[326.07519524]] accuracy 0.7837465564738292\n","Iteration 17117 - loss value [[326.95540597]] accuracy 0.7878787878787878\n","Iteration 17118 - loss value [[326.28983841]] accuracy 0.7823691460055097\n","Iteration 17119 - loss value [[326.96597898]] accuracy 0.7823691460055097\n","Iteration 17120 - loss value [[326.21152927]] accuracy 0.7851239669421488\n","Iteration 17121 - loss value [[328.7557039]] accuracy 0.7823691460055097\n","Iteration 17122 - loss value [[328.57744277]] accuracy 0.778236914600551\n","Iteration 17123 - loss value [[329.06562425]] accuracy 0.7823691460055097\n","Iteration 17124 - loss value [[326.95261101]] accuracy 0.778236914600551\n","Iteration 17125 - loss value [[326.37158879]] accuracy 0.7865013774104683\n","Iteration 17126 - loss value [[326.17790509]] accuracy 0.7837465564738292\n","Iteration 17127 - loss value [[326.1696204]] accuracy 0.7851239669421488\n","Iteration 17128 - loss value [[325.34833258]] accuracy 0.7823691460055097\n","Iteration 17129 - loss value [[325.16040595]] accuracy 0.7851239669421488\n","Iteration 17130 - loss value [[325.60464676]] accuracy 0.7823691460055097\n","Iteration 17131 - loss value [[327.68975695]] accuracy 0.7837465564738292\n","Iteration 17132 - loss value [[326.8913524]] accuracy 0.7823691460055097\n","Iteration 17133 - loss value [[325.31978729]] accuracy 0.7851239669421488\n","Iteration 17134 - loss value [[325.64720338]] accuracy 0.7823691460055097\n","Iteration 17135 - loss value [[325.66445008]] accuracy 0.7823691460055097\n","Iteration 17136 - loss value [[328.51931188]] accuracy 0.7796143250688705\n","Iteration 17137 - loss value [[331.32541929]] accuracy 0.78099173553719\n","Iteration 17138 - loss value [[329.13482813]] accuracy 0.778236914600551\n","Iteration 17139 - loss value [[329.64831237]] accuracy 0.7851239669421488\n","Iteration 17140 - loss value [[327.93784154]] accuracy 0.778236914600551\n","Iteration 17141 - loss value [[327.59008955]] accuracy 0.7851239669421488\n","Iteration 17142 - loss value [[329.30213428]] accuracy 0.7754820936639119\n","Iteration 17143 - loss value [[331.71054228]] accuracy 0.7823691460055097\n","Iteration 17144 - loss value [[337.52863599]] accuracy 0.7727272727272727\n","Iteration 17145 - loss value [[335.804735]] accuracy 0.778236914600551\n","Iteration 17146 - loss value [[335.24091393]] accuracy 0.7754820936639119\n","Iteration 17147 - loss value [[335.61943544]] accuracy 0.7837465564738292\n","Iteration 17148 - loss value [[353.41503596]] accuracy 0.7548209366391184\n","Iteration 17149 - loss value [[324.95890211]] accuracy 0.7851239669421488\n","Iteration 17150 - loss value [[324.73648397]] accuracy 0.7878787878787878\n","Iteration 17151 - loss value [[324.73821523]] accuracy 0.7878787878787878\n","Iteration 17152 - loss value [[324.84793019]] accuracy 0.7865013774104683\n","Iteration 17153 - loss value [[325.3253573]] accuracy 0.7865013774104683\n","Iteration 17154 - loss value [[327.38435199]] accuracy 0.7837465564738292\n","Iteration 17155 - loss value [[326.33330642]] accuracy 0.7837465564738292\n","Iteration 17156 - loss value [[328.61248027]] accuracy 0.7837465564738292\n","Iteration 17157 - loss value [[328.06622476]] accuracy 0.7796143250688705\n","Iteration 17158 - loss value [[330.51060618]] accuracy 0.7796143250688705\n","Iteration 17159 - loss value [[327.96661875]] accuracy 0.7823691460055097\n","Iteration 17160 - loss value [[326.43358865]] accuracy 0.7837465564738292\n","Iteration 17161 - loss value [[325.40021265]] accuracy 0.7837465564738292\n","Iteration 17162 - loss value [[325.20612158]] accuracy 0.7851239669421488\n","Iteration 17163 - loss value [[325.82827611]] accuracy 0.7823691460055097\n","Iteration 17164 - loss value [[326.6240478]] accuracy 0.7837465564738292\n","Iteration 17165 - loss value [[327.78188623]] accuracy 0.778236914600551\n","Iteration 17166 - loss value [[332.42669612]] accuracy 0.78099173553719\n","Iteration 17167 - loss value [[330.01990175]] accuracy 0.7796143250688705\n","Iteration 17168 - loss value [[329.45279497]] accuracy 0.7823691460055097\n","Iteration 17169 - loss value [[328.23028227]] accuracy 0.7796143250688705\n","Iteration 17170 - loss value [[328.30874791]] accuracy 0.7837465564738292\n","Iteration 17171 - loss value [[326.48833257]] accuracy 0.78099173553719\n","Iteration 17172 - loss value [[325.55447498]] accuracy 0.7837465564738292\n","Iteration 17173 - loss value [[325.35466821]] accuracy 0.7837465564738292\n","Iteration 17174 - loss value [[325.24121097]] accuracy 0.7851239669421488\n","Iteration 17175 - loss value [[325.82124244]] accuracy 0.7837465564738292\n","Iteration 17176 - loss value [[325.87398921]] accuracy 0.7837465564738292\n","Iteration 17177 - loss value [[327.65812305]] accuracy 0.7837465564738292\n","Iteration 17178 - loss value [[327.14557767]] accuracy 0.7823691460055097\n","Iteration 17179 - loss value [[328.01521076]] accuracy 0.7851239669421488\n","Iteration 17180 - loss value [[328.24816972]] accuracy 0.7796143250688705\n","Iteration 17181 - loss value [[332.15447794]] accuracy 0.78099173553719\n","Iteration 17182 - loss value [[332.27275929]] accuracy 0.7768595041322314\n","Iteration 17183 - loss value [[342.45463209]] accuracy 0.7741046831955923\n","Iteration 17184 - loss value [[361.90024875]] accuracy 0.7451790633608816\n","Iteration 17185 - loss value [[326.34146042]] accuracy 0.7823691460055097\n","Iteration 17186 - loss value [[326.76188575]] accuracy 0.778236914600551\n","Iteration 17187 - loss value [[332.02447162]] accuracy 0.78099173553719\n","Iteration 17188 - loss value [[330.82980098]] accuracy 0.7796143250688705\n","Iteration 17189 - loss value [[331.97425674]] accuracy 0.7823691460055097\n","Iteration 17190 - loss value [[336.64525756]] accuracy 0.7727272727272727\n","Iteration 17191 - loss value [[328.91590994]] accuracy 0.7837465564738292\n","Iteration 17192 - loss value [[332.08537406]] accuracy 0.7727272727272727\n","Iteration 17193 - loss value [[329.2159664]] accuracy 0.7851239669421488\n","Iteration 17194 - loss value [[331.90758297]] accuracy 0.7741046831955923\n","Iteration 17195 - loss value [[334.44369669]] accuracy 0.778236914600551\n","Iteration 17196 - loss value [[342.94309931]] accuracy 0.768595041322314\n","Iteration 17197 - loss value [[347.70657862]] accuracy 0.768595041322314\n","Iteration 17198 - loss value [[390.7882015]] accuracy 0.7148760330578512\n","Iteration 17199 - loss value [[327.69119029]] accuracy 0.7823691460055097\n","Iteration 17200 - loss value [[328.49237347]] accuracy 0.778236914600551\n","Iteration 17201 - loss value [[329.82371468]] accuracy 0.7837465564738292\n","Iteration 17202 - loss value [[330.1278215]] accuracy 0.7754820936639119\n","Iteration 17203 - loss value [[331.42780553]] accuracy 0.7796143250688705\n","Iteration 17204 - loss value [[334.87570358]] accuracy 0.7727272727272727\n","Iteration 17205 - loss value [[330.49206101]] accuracy 0.78099173553719\n","Iteration 17206 - loss value [[331.37681793]] accuracy 0.7754820936639119\n","Iteration 17207 - loss value [[328.33862587]] accuracy 0.7837465564738292\n","Iteration 17208 - loss value [[328.21578065]] accuracy 0.778236914600551\n","Iteration 17209 - loss value [[330.7818766]] accuracy 0.78099173553719\n","Iteration 17210 - loss value [[332.77517147]] accuracy 0.7727272727272727\n","Iteration 17211 - loss value [[346.4500383]] accuracy 0.7713498622589532\n","Iteration 17212 - loss value [[390.6167212]] accuracy 0.7148760330578512\n","Iteration 17213 - loss value [[333.00015519]] accuracy 0.7796143250688705\n","Iteration 17214 - loss value [[327.46856034]] accuracy 0.778236914600551\n","Iteration 17215 - loss value [[326.62639281]] accuracy 0.7823691460055097\n","Iteration 17216 - loss value [[325.85545448]] accuracy 0.7823691460055097\n","Iteration 17217 - loss value [[325.65890703]] accuracy 0.7865013774104683\n","Iteration 17218 - loss value [[325.48825645]] accuracy 0.7837465564738292\n","Iteration 17219 - loss value [[326.85015982]] accuracy 0.7823691460055097\n","Iteration 17220 - loss value [[326.81458096]] accuracy 0.7837465564738292\n","Iteration 17221 - loss value [[329.36086256]] accuracy 0.7837465564738292\n","Iteration 17222 - loss value [[331.27066967]] accuracy 0.7741046831955923\n","Iteration 17223 - loss value [[329.53159471]] accuracy 0.78099173553719\n","Iteration 17224 - loss value [[327.06885905]] accuracy 0.7796143250688705\n","Iteration 17225 - loss value [[325.71404798]] accuracy 0.7837465564738292\n","Iteration 17226 - loss value [[325.32057105]] accuracy 0.7837465564738292\n","Iteration 17227 - loss value [[326.3605667]] accuracy 0.7865013774104683\n","Iteration 17228 - loss value [[325.99810202]] accuracy 0.7823691460055097\n","Iteration 17229 - loss value [[327.20579362]] accuracy 0.7837465564738292\n","Iteration 17230 - loss value [[327.51834942]] accuracy 0.778236914600551\n","Iteration 17231 - loss value [[330.44519777]] accuracy 0.7823691460055097\n","Iteration 17232 - loss value [[333.99245213]] accuracy 0.7727272727272727\n","Iteration 17233 - loss value [[328.12830151]] accuracy 0.7851239669421488\n","Iteration 17234 - loss value [[325.96946111]] accuracy 0.78099173553719\n","Iteration 17235 - loss value [[326.6683417]] accuracy 0.7892561983471075\n","Iteration 17236 - loss value [[325.15765472]] accuracy 0.7823691460055097\n","Iteration 17237 - loss value [[324.7160774]] accuracy 0.7865013774104683\n","Iteration 17238 - loss value [[324.74744515]] accuracy 0.7865013774104683\n","Iteration 17239 - loss value [[324.98258561]] accuracy 0.7851239669421488\n","Iteration 17240 - loss value [[326.4692335]] accuracy 0.78099173553719\n","Iteration 17241 - loss value [[331.20240607]] accuracy 0.7823691460055097\n","Iteration 17242 - loss value [[329.19421676]] accuracy 0.7796143250688705\n","Iteration 17243 - loss value [[331.12823103]] accuracy 0.7851239669421488\n","Iteration 17244 - loss value [[332.41848893]] accuracy 0.7768595041322314\n","Iteration 17245 - loss value [[333.62022623]] accuracy 0.7837465564738292\n","Iteration 17246 - loss value [[343.44697255]] accuracy 0.7672176308539945\n","Iteration 17247 - loss value [[336.51390467]] accuracy 0.7741046831955923\n","Iteration 17248 - loss value [[343.31143105]] accuracy 0.7644628099173554\n","Iteration 17249 - loss value [[331.32471294]] accuracy 0.7796143250688705\n","Iteration 17250 - loss value [[329.8032758]] accuracy 0.7768595041322314\n","Iteration 17251 - loss value [[328.29995794]] accuracy 0.7823691460055097\n","Iteration 17252 - loss value [[328.46460897]] accuracy 0.7768595041322314\n","Iteration 17253 - loss value [[330.63022041]] accuracy 0.78099173553719\n","Iteration 17254 - loss value [[330.21084177]] accuracy 0.778236914600551\n","Iteration 17255 - loss value [[329.51082266]] accuracy 0.7837465564738292\n","Iteration 17256 - loss value [[327.86356429]] accuracy 0.78099173553719\n","Iteration 17257 - loss value [[325.98450325]] accuracy 0.7823691460055097\n","Iteration 17258 - loss value [[325.51129717]] accuracy 0.7851239669421488\n","Iteration 17259 - loss value [[326.25464318]] accuracy 0.78099173553719\n","Iteration 17260 - loss value [[327.5040863]] accuracy 0.78099173553719\n","Iteration 17261 - loss value [[336.40927776]] accuracy 0.7754820936639119\n","Iteration 17262 - loss value [[352.89334514]] accuracy 0.7534435261707989\n","Iteration 17263 - loss value [[325.92278033]] accuracy 0.7837465564738292\n","Iteration 17264 - loss value [[329.10829739]] accuracy 0.7768595041322314\n","Iteration 17265 - loss value [[330.28428619]] accuracy 0.7823691460055097\n","Iteration 17266 - loss value [[332.55834774]] accuracy 0.7741046831955923\n","Iteration 17267 - loss value [[330.30672983]] accuracy 0.7851239669421488\n","Iteration 17268 - loss value [[331.88028566]] accuracy 0.7741046831955923\n","Iteration 17269 - loss value [[327.47625493]] accuracy 0.7837465564738292\n","Iteration 17270 - loss value [[326.18917099]] accuracy 0.7823691460055097\n","Iteration 17271 - loss value [[326.76831062]] accuracy 0.7851239669421488\n","Iteration 17272 - loss value [[326.59258902]] accuracy 0.7823691460055097\n","Iteration 17273 - loss value [[327.39867492]] accuracy 0.7851239669421488\n","Iteration 17274 - loss value [[329.42616139]] accuracy 0.7754820936639119\n","Iteration 17275 - loss value [[338.04263098]] accuracy 0.7768595041322314\n","Iteration 17276 - loss value [[352.72264113]] accuracy 0.7575757575757576\n","Iteration 17277 - loss value [[325.83878057]] accuracy 0.7851239669421488\n","Iteration 17278 - loss value [[327.19018339]] accuracy 0.778236914600551\n","Iteration 17279 - loss value [[331.89138319]] accuracy 0.7796143250688705\n","Iteration 17280 - loss value [[328.54493533]] accuracy 0.7837465564738292\n","Iteration 17281 - loss value [[327.97408456]] accuracy 0.7878787878787878\n","Iteration 17282 - loss value [[328.32281553]] accuracy 0.778236914600551\n","Iteration 17283 - loss value [[334.3926759]] accuracy 0.7823691460055097\n","Iteration 17284 - loss value [[340.81145865]] accuracy 0.7727272727272727\n","Iteration 17285 - loss value [[340.42030327]] accuracy 0.7754820936639119\n","Iteration 17286 - loss value [[359.9016455]] accuracy 0.7479338842975206\n","Iteration 17287 - loss value [[326.86723755]] accuracy 0.7823691460055097\n","Iteration 17288 - loss value [[326.42639386]] accuracy 0.7851239669421488\n","Iteration 17289 - loss value [[328.17551345]] accuracy 0.7768595041322314\n","Iteration 17290 - loss value [[329.04609627]] accuracy 0.7837465564738292\n","Iteration 17291 - loss value [[331.19842849]] accuracy 0.7713498622589532\n","Iteration 17292 - loss value [[327.89723433]] accuracy 0.7823691460055097\n","Iteration 17293 - loss value [[329.82381295]] accuracy 0.7768595041322314\n","Iteration 17294 - loss value [[333.06120805]] accuracy 0.7823691460055097\n","Iteration 17295 - loss value [[335.64210894]] accuracy 0.7768595041322314\n","Iteration 17296 - loss value [[334.68788947]] accuracy 0.7823691460055097\n","Iteration 17297 - loss value [[351.45887717]] accuracy 0.7548209366391184\n","Iteration 17298 - loss value [[328.56064867]] accuracy 0.7837465564738292\n","Iteration 17299 - loss value [[330.57856646]] accuracy 0.7741046831955923\n","Iteration 17300 - loss value [[331.4370261]] accuracy 0.7837465564738292\n","Iteration 17301 - loss value [[328.82034132]] accuracy 0.778236914600551\n","Iteration 17302 - loss value [[328.7940945]] accuracy 0.78099173553719\n","Iteration 17303 - loss value [[327.03380566]] accuracy 0.7768595041322314\n","Iteration 17304 - loss value [[325.41130535]] accuracy 0.7837465564738292\n","Iteration 17305 - loss value [[324.88831393]] accuracy 0.7851239669421488\n","Iteration 17306 - loss value [[325.22629555]] accuracy 0.7865013774104683\n","Iteration 17307 - loss value [[326.98128831]] accuracy 0.7837465564738292\n","Iteration 17308 - loss value [[327.01309508]] accuracy 0.7823691460055097\n","Iteration 17309 - loss value [[326.694734]] accuracy 0.7837465564738292\n","Iteration 17310 - loss value [[328.35653474]] accuracy 0.778236914600551\n","Iteration 17311 - loss value [[335.8484631]] accuracy 0.7796143250688705\n","Iteration 17312 - loss value [[343.10723293]] accuracy 0.7699724517906336\n","Iteration 17313 - loss value [[332.72890556]] accuracy 0.7851239669421488\n","Iteration 17314 - loss value [[337.74972986]] accuracy 0.7713498622589532\n","Iteration 17315 - loss value [[333.52600759]] accuracy 0.7865013774104683\n","Iteration 17316 - loss value [[337.24488618]] accuracy 0.7713498622589532\n","Iteration 17317 - loss value [[331.74687146]] accuracy 0.7865013774104683\n","Iteration 17318 - loss value [[332.24922126]] accuracy 0.778236914600551\n","Iteration 17319 - loss value [[330.28133597]] accuracy 0.7851239669421488\n","Iteration 17320 - loss value [[330.3015726]] accuracy 0.7754820936639119\n","Iteration 17321 - loss value [[333.68004348]] accuracy 0.7851239669421488\n","Iteration 17322 - loss value [[348.6512402]] accuracy 0.7603305785123967\n","Iteration 17323 - loss value [[341.3225246]] accuracy 0.7741046831955923\n","Iteration 17324 - loss value [[365.51592164]] accuracy 0.7424242424242424\n","Iteration 17325 - loss value [[327.67943324]] accuracy 0.7796143250688705\n","Iteration 17326 - loss value [[326.37048729]] accuracy 0.78099173553719\n","Iteration 17327 - loss value [[331.23441952]] accuracy 0.78099173553719\n","Iteration 17328 - loss value [[327.75223195]] accuracy 0.7768595041322314\n","Iteration 17329 - loss value [[325.9278268]] accuracy 0.7865013774104683\n","Iteration 17330 - loss value [[325.18491145]] accuracy 0.7837465564738292\n","Iteration 17331 - loss value [[325.48226006]] accuracy 0.7865013774104683\n","Iteration 17332 - loss value [[326.86919033]] accuracy 0.78099173553719\n","Iteration 17333 - loss value [[332.59886625]] accuracy 0.7851239669421488\n","Iteration 17334 - loss value [[336.77880101]] accuracy 0.7741046831955923\n","Iteration 17335 - loss value [[335.61556101]] accuracy 0.7823691460055097\n","Iteration 17336 - loss value [[353.0256882]] accuracy 0.7548209366391184\n","Iteration 17337 - loss value [[325.24772789]] accuracy 0.7837465564738292\n","Iteration 17338 - loss value [[327.29804185]] accuracy 0.778236914600551\n","Iteration 17339 - loss value [[333.6780603]] accuracy 0.78099173553719\n","Iteration 17340 - loss value [[336.57732704]] accuracy 0.7699724517906336\n","Iteration 17341 - loss value [[330.98395829]] accuracy 0.78099173553719\n","Iteration 17342 - loss value [[329.37207897]] accuracy 0.778236914600551\n","Iteration 17343 - loss value [[330.40558075]] accuracy 0.7851239669421488\n","Iteration 17344 - loss value [[330.33756496]] accuracy 0.7754820936639119\n","Iteration 17345 - loss value [[336.23520496]] accuracy 0.7837465564738292\n","Iteration 17346 - loss value [[357.56037028]] accuracy 0.7479338842975206\n","Iteration 17347 - loss value [[326.47480508]] accuracy 0.7837465564738292\n","Iteration 17348 - loss value [[330.46140175]] accuracy 0.7754820936639119\n","Iteration 17349 - loss value [[328.30803869]] accuracy 0.7837465564738292\n","Iteration 17350 - loss value [[326.80177069]] accuracy 0.78099173553719\n","Iteration 17351 - loss value [[328.57500244]] accuracy 0.78099173553719\n","Iteration 17352 - loss value [[328.18740831]] accuracy 0.7796143250688705\n","Iteration 17353 - loss value [[326.5946141]] accuracy 0.7851239669421488\n","Iteration 17354 - loss value [[325.26950853]] accuracy 0.7823691460055097\n","Iteration 17355 - loss value [[324.82700212]] accuracy 0.7865013774104683\n","Iteration 17356 - loss value [[325.21743792]] accuracy 0.7865013774104683\n","Iteration 17357 - loss value [[327.00092795]] accuracy 0.7837465564738292\n","Iteration 17358 - loss value [[325.54070192]] accuracy 0.7837465564738292\n","Iteration 17359 - loss value [[326.24581983]] accuracy 0.7823691460055097\n","Iteration 17360 - loss value [[327.63044514]] accuracy 0.778236914600551\n","Iteration 17361 - loss value [[332.68938958]] accuracy 0.778236914600551\n","Iteration 17362 - loss value [[329.80965062]] accuracy 0.7837465564738292\n","Iteration 17363 - loss value [[329.96876797]] accuracy 0.78099173553719\n","Iteration 17364 - loss value [[328.60553482]] accuracy 0.78099173553719\n","Iteration 17365 - loss value [[329.20921602]] accuracy 0.7837465564738292\n","Iteration 17366 - loss value [[326.71405591]] accuracy 0.778236914600551\n","Iteration 17367 - loss value [[326.12202616]] accuracy 0.7823691460055097\n","Iteration 17368 - loss value [[326.26471148]] accuracy 0.7823691460055097\n","Iteration 17369 - loss value [[326.07660958]] accuracy 0.7823691460055097\n","Iteration 17370 - loss value [[325.46663392]] accuracy 0.7823691460055097\n","Iteration 17371 - loss value [[325.83577246]] accuracy 0.7851239669421488\n","Iteration 17372 - loss value [[325.55601414]] accuracy 0.7823691460055097\n","Iteration 17373 - loss value [[324.91157487]] accuracy 0.7878787878787878\n","Iteration 17374 - loss value [[325.17650204]] accuracy 0.7837465564738292\n","Iteration 17375 - loss value [[326.60134206]] accuracy 0.7823691460055097\n","Iteration 17376 - loss value [[328.61748613]] accuracy 0.7754820936639119\n","Iteration 17377 - loss value [[333.7394378]] accuracy 0.7837465564738292\n","Iteration 17378 - loss value [[340.46874565]] accuracy 0.7713498622589532\n","Iteration 17379 - loss value [[336.08473659]] accuracy 0.78099173553719\n","Iteration 17380 - loss value [[350.12652226]] accuracy 0.7589531680440771\n","Iteration 17381 - loss value [[326.46181346]] accuracy 0.78099173553719\n","Iteration 17382 - loss value [[328.11756791]] accuracy 0.778236914600551\n","Iteration 17383 - loss value [[328.60634528]] accuracy 0.7837465564738292\n","Iteration 17384 - loss value [[329.28363462]] accuracy 0.7768595041322314\n","Iteration 17385 - loss value [[337.4464573]] accuracy 0.7837465564738292\n","Iteration 17386 - loss value [[357.27532795]] accuracy 0.7479338842975206\n","Iteration 17387 - loss value [[326.23981219]] accuracy 0.7837465564738292\n","Iteration 17388 - loss value [[326.55832613]] accuracy 0.7796143250688705\n","Iteration 17389 - loss value [[331.23943519]] accuracy 0.7837465564738292\n","Iteration 17390 - loss value [[330.03746674]] accuracy 0.7768595041322314\n","Iteration 17391 - loss value [[333.88399772]] accuracy 0.7823691460055097\n","Iteration 17392 - loss value [[346.79083575]] accuracy 0.7589531680440771\n","Iteration 17393 - loss value [[345.65449795]] accuracy 0.7741046831955923\n","Iteration 17394 - loss value [[369.65426378]] accuracy 0.7382920110192838\n","Iteration 17395 - loss value [[331.00294072]] accuracy 0.7837465564738292\n","Iteration 17396 - loss value [[326.88993577]] accuracy 0.78099173553719\n","Iteration 17397 - loss value [[328.67442752]] accuracy 0.7823691460055097\n","Iteration 17398 - loss value [[326.46615068]] accuracy 0.78099173553719\n","Iteration 17399 - loss value [[326.95076183]] accuracy 0.7851239669421488\n","Iteration 17400 - loss value [[325.86533927]] accuracy 0.78099173553719\n","Iteration 17401 - loss value [[325.03970812]] accuracy 0.7837465564738292\n","Iteration 17402 - loss value [[324.89144242]] accuracy 0.7851239669421488\n","Iteration 17403 - loss value [[324.91372388]] accuracy 0.7865013774104683\n","Iteration 17404 - loss value [[325.18432246]] accuracy 0.7823691460055097\n","Iteration 17405 - loss value [[325.20993142]] accuracy 0.7851239669421488\n","Iteration 17406 - loss value [[327.39152374]] accuracy 0.778236914600551\n","Iteration 17407 - loss value [[328.42424186]] accuracy 0.7865013774104683\n","Iteration 17408 - loss value [[331.09381897]] accuracy 0.7754820936639119\n","Iteration 17409 - loss value [[327.62833823]] accuracy 0.7865013774104683\n","Iteration 17410 - loss value [[327.674796]] accuracy 0.778236914600551\n","Iteration 17411 - loss value [[331.67113788]] accuracy 0.7823691460055097\n","Iteration 17412 - loss value [[330.16610451]] accuracy 0.78099173553719\n","Iteration 17413 - loss value [[330.44218535]] accuracy 0.7851239669421488\n","Iteration 17414 - loss value [[329.53429428]] accuracy 0.778236914600551\n","Iteration 17415 - loss value [[330.63809209]] accuracy 0.7851239669421488\n","Iteration 17416 - loss value [[331.82621604]] accuracy 0.7768595041322314\n","Iteration 17417 - loss value [[342.91819243]] accuracy 0.7754820936639119\n","Iteration 17418 - loss value [[365.83909639]] accuracy 0.7424242424242424\n","Iteration 17419 - loss value [[325.15367626]] accuracy 0.7851239669421488\n","Iteration 17420 - loss value [[325.38885215]] accuracy 0.78099173553719\n","Iteration 17421 - loss value [[325.03760028]] accuracy 0.7878787878787878\n","Iteration 17422 - loss value [[325.82632715]] accuracy 0.7851239669421488\n","Iteration 17423 - loss value [[325.51609941]] accuracy 0.7823691460055097\n","Iteration 17424 - loss value [[326.91035904]] accuracy 0.7837465564738292\n","Iteration 17425 - loss value [[326.69130173]] accuracy 0.78099173553719\n","Iteration 17426 - loss value [[330.30204931]] accuracy 0.7823691460055097\n","Iteration 17427 - loss value [[333.95233169]] accuracy 0.7741046831955923\n","Iteration 17428 - loss value [[331.0330831]] accuracy 0.7823691460055097\n","Iteration 17429 - loss value [[336.84134086]] accuracy 0.7727272727272727\n","Iteration 17430 - loss value [[332.00627983]] accuracy 0.78099173553719\n","Iteration 17431 - loss value [[337.99947458]] accuracy 0.7741046831955923\n","Iteration 17432 - loss value [[331.9083782]] accuracy 0.7823691460055097\n","Iteration 17433 - loss value [[337.61439178]] accuracy 0.7727272727272727\n","Iteration 17434 - loss value [[334.48481437]] accuracy 0.7796143250688705\n","Iteration 17435 - loss value [[340.50773134]] accuracy 0.7699724517906336\n","Iteration 17436 - loss value [[345.7061725]] accuracy 0.7754820936639119\n","Iteration 17437 - loss value [[374.42181978]] accuracy 0.7369146005509641\n","Iteration 17438 - loss value [[332.76663906]] accuracy 0.7768595041322314\n","Iteration 17439 - loss value [[336.93395699]] accuracy 0.7672176308539945\n","Iteration 17440 - loss value [[338.54018801]] accuracy 0.7741046831955923\n","Iteration 17441 - loss value [[352.60461567]] accuracy 0.7506887052341598\n","Iteration 17442 - loss value [[329.2764932]] accuracy 0.7741046831955923\n","Iteration 17443 - loss value [[326.6093127]] accuracy 0.7851239669421488\n","Iteration 17444 - loss value [[327.42269164]] accuracy 0.778236914600551\n","Iteration 17445 - loss value [[332.30547052]] accuracy 0.778236914600551\n","Iteration 17446 - loss value [[329.95309705]] accuracy 0.7768595041322314\n","Iteration 17447 - loss value [[330.78620738]] accuracy 0.7837465564738292\n","Iteration 17448 - loss value [[328.07722243]] accuracy 0.7768595041322314\n","Iteration 17449 - loss value [[326.31023323]] accuracy 0.7837465564738292\n","Iteration 17450 - loss value [[325.91674312]] accuracy 0.78099173553719\n","Iteration 17451 - loss value [[324.93064695]] accuracy 0.7837465564738292\n","Iteration 17452 - loss value [[324.81809946]] accuracy 0.7851239669421488\n","Iteration 17453 - loss value [[324.9427949]] accuracy 0.7851239669421488\n","Iteration 17454 - loss value [[325.78580404]] accuracy 0.7823691460055097\n","Iteration 17455 - loss value [[328.60697417]] accuracy 0.7851239669421488\n","Iteration 17456 - loss value [[329.24127523]] accuracy 0.778236914600551\n","Iteration 17457 - loss value [[337.57282356]] accuracy 0.7796143250688705\n","Iteration 17458 - loss value [[354.55295548]] accuracy 0.756198347107438\n","Iteration 17459 - loss value [[325.96987001]] accuracy 0.7823691460055097\n","Iteration 17460 - loss value [[327.74445986]] accuracy 0.7796143250688705\n","Iteration 17461 - loss value [[328.82791714]] accuracy 0.7837465564738292\n","Iteration 17462 - loss value [[331.24321717]] accuracy 0.7768595041322314\n","Iteration 17463 - loss value [[330.58129452]] accuracy 0.7823691460055097\n","Iteration 17464 - loss value [[330.76141319]] accuracy 0.7768595041322314\n","Iteration 17465 - loss value [[328.79612176]] accuracy 0.7837465564738292\n","Iteration 17466 - loss value [[330.50905352]] accuracy 0.7754820936639119\n","Iteration 17467 - loss value [[334.50635358]] accuracy 0.7851239669421488\n","Iteration 17468 - loss value [[342.26783184]] accuracy 0.7672176308539945\n","Iteration 17469 - loss value [[343.71025324]] accuracy 0.7768595041322314\n","Iteration 17470 - loss value [[369.47555163]] accuracy 0.7424242424242424\n","Iteration 17471 - loss value [[329.09332926]] accuracy 0.778236914600551\n","Iteration 17472 - loss value [[331.17127256]] accuracy 0.7768595041322314\n","Iteration 17473 - loss value [[330.56728178]] accuracy 0.7754820936639119\n","Iteration 17474 - loss value [[331.83531334]] accuracy 0.7713498622589532\n","Iteration 17475 - loss value [[329.04008135]] accuracy 0.78099173553719\n","Iteration 17476 - loss value [[330.29203739]] accuracy 0.7741046831955923\n","Iteration 17477 - loss value [[329.80438028]] accuracy 0.78099173553719\n","Iteration 17478 - loss value [[330.88877628]] accuracy 0.7727272727272727\n","Iteration 17479 - loss value [[327.47066262]] accuracy 0.7837465564738292\n","Iteration 17480 - loss value [[326.73852042]] accuracy 0.7837465564738292\n","Iteration 17481 - loss value [[326.43425843]] accuracy 0.7851239669421488\n","Iteration 17482 - loss value [[325.65836171]] accuracy 0.7823691460055097\n","Iteration 17483 - loss value [[325.08119949]] accuracy 0.7837465564738292\n","Iteration 17484 - loss value [[325.63257401]] accuracy 0.7865013774104683\n","Iteration 17485 - loss value [[327.93291]] accuracy 0.778236914600551\n","Iteration 17486 - loss value [[335.95969059]] accuracy 0.778236914600551\n","Iteration 17487 - loss value [[340.8222029]] accuracy 0.768595041322314\n","Iteration 17488 - loss value [[334.76670216]] accuracy 0.78099173553719\n","Iteration 17489 - loss value [[339.0059057]] accuracy 0.7713498622589532\n","Iteration 17490 - loss value [[335.36722435]] accuracy 0.7823691460055097\n","Iteration 17491 - loss value [[347.94770697]] accuracy 0.7589531680440771\n","Iteration 17492 - loss value [[337.34871623]] accuracy 0.7823691460055097\n","Iteration 17493 - loss value [[351.35112446]] accuracy 0.7548209366391184\n","Iteration 17494 - loss value [[326.11383249]] accuracy 0.78099173553719\n","Iteration 17495 - loss value [[324.68858182]] accuracy 0.7878787878787878\n","Iteration 17496 - loss value [[324.84835316]] accuracy 0.7865013774104683\n","Iteration 17497 - loss value [[325.64743135]] accuracy 0.7851239669421488\n","Iteration 17498 - loss value [[324.88070344]] accuracy 0.7865013774104683\n","Iteration 17499 - loss value [[325.62452761]] accuracy 0.7851239669421488\n","Iteration 17500 - loss value [[325.2358234]] accuracy 0.7851239669421488\n","Iteration 17501 - loss value [[326.19925213]] accuracy 0.7837465564738292\n","Iteration 17502 - loss value [[325.75121105]] accuracy 0.7823691460055097\n","Iteration 17503 - loss value [[327.03174256]] accuracy 0.78099173553719\n","Iteration 17504 - loss value [[326.93847637]] accuracy 0.7837465564738292\n","Iteration 17505 - loss value [[330.31081963]] accuracy 0.7823691460055097\n","Iteration 17506 - loss value [[333.13829962]] accuracy 0.7741046831955923\n","Iteration 17507 - loss value [[327.93331238]] accuracy 0.7851239669421488\n","Iteration 17508 - loss value [[328.65616252]] accuracy 0.778236914600551\n","Iteration 17509 - loss value [[329.90776516]] accuracy 0.7796143250688705\n","Iteration 17510 - loss value [[327.20494703]] accuracy 0.78099173553719\n","Iteration 17511 - loss value [[326.12163346]] accuracy 0.7837465564738292\n","Iteration 17512 - loss value [[325.67592465]] accuracy 0.7823691460055097\n","Iteration 17513 - loss value [[325.83236244]] accuracy 0.7851239669421488\n","Iteration 17514 - loss value [[325.54183594]] accuracy 0.7796143250688705\n","Iteration 17515 - loss value [[326.24333852]] accuracy 0.7851239669421488\n","Iteration 17516 - loss value [[329.42508636]] accuracy 0.78099173553719\n","Iteration 17517 - loss value [[332.24571778]] accuracy 0.7713498622589532\n","Iteration 17518 - loss value [[331.91516057]] accuracy 0.778236914600551\n","Iteration 17519 - loss value [[334.76729727]] accuracy 0.7741046831955923\n","Iteration 17520 - loss value [[330.11137722]] accuracy 0.78099173553719\n","Iteration 17521 - loss value [[330.63975379]] accuracy 0.7754820936639119\n","Iteration 17522 - loss value [[330.31721863]] accuracy 0.7837465564738292\n","Iteration 17523 - loss value [[329.29084313]] accuracy 0.7754820936639119\n","Iteration 17524 - loss value [[332.67692551]] accuracy 0.7823691460055097\n","Iteration 17525 - loss value [[334.52307383]] accuracy 0.7754820936639119\n","Iteration 17526 - loss value [[330.94567924]] accuracy 0.7865013774104683\n","Iteration 17527 - loss value [[330.29270056]] accuracy 0.7768595041322314\n","Iteration 17528 - loss value [[334.294206]] accuracy 0.7851239669421488\n","Iteration 17529 - loss value [[349.18949724]] accuracy 0.7575757575757576\n","Iteration 17530 - loss value [[337.26377986]] accuracy 0.7796143250688705\n","Iteration 17531 - loss value [[353.37903939]] accuracy 0.756198347107438\n","Iteration 17532 - loss value [[325.03031205]] accuracy 0.7851239669421488\n","Iteration 17533 - loss value [[325.61739874]] accuracy 0.7823691460055097\n","Iteration 17534 - loss value [[328.50088234]] accuracy 0.7768595041322314\n","Iteration 17535 - loss value [[328.47582141]] accuracy 0.7851239669421488\n","Iteration 17536 - loss value [[329.70769352]] accuracy 0.7754820936639119\n","Iteration 17537 - loss value [[331.42283144]] accuracy 0.7823691460055097\n","Iteration 17538 - loss value [[328.03766988]] accuracy 0.7823691460055097\n","Iteration 17539 - loss value [[329.06697375]] accuracy 0.7823691460055097\n","Iteration 17540 - loss value [[326.4198227]] accuracy 0.78099173553719\n","Iteration 17541 - loss value [[325.39014478]] accuracy 0.7837465564738292\n","Iteration 17542 - loss value [[325.59171068]] accuracy 0.7837465564738292\n","Iteration 17543 - loss value [[327.79676073]] accuracy 0.7865013774104683\n","Iteration 17544 - loss value [[328.33935155]] accuracy 0.7768595041322314\n","Iteration 17545 - loss value [[334.93974027]] accuracy 0.7768595041322314\n","Iteration 17546 - loss value [[339.81527046]] accuracy 0.768595041322314\n","Iteration 17547 - loss value [[328.9925588]] accuracy 0.7837465564738292\n","Iteration 17548 - loss value [[331.99973755]] accuracy 0.7754820936639119\n","Iteration 17549 - loss value [[326.85200932]] accuracy 0.7851239669421488\n","Iteration 17550 - loss value [[325.55748154]] accuracy 0.7837465564738292\n","Iteration 17551 - loss value [[326.19312938]] accuracy 0.7851239669421488\n","Iteration 17552 - loss value [[328.84703392]] accuracy 0.778236914600551\n","Iteration 17553 - loss value [[335.4873202]] accuracy 0.78099173553719\n","Iteration 17554 - loss value [[339.13140036]] accuracy 0.7713498622589532\n","Iteration 17555 - loss value [[333.68914229]] accuracy 0.7837465564738292\n","Iteration 17556 - loss value [[338.81877182]] accuracy 0.7699724517906336\n","Iteration 17557 - loss value [[331.28850924]] accuracy 0.778236914600551\n","Iteration 17558 - loss value [[329.75738552]] accuracy 0.778236914600551\n","Iteration 17559 - loss value [[328.11765265]] accuracy 0.7823691460055097\n","Iteration 17560 - loss value [[327.88730898]] accuracy 0.7796143250688705\n","Iteration 17561 - loss value [[327.3796413]] accuracy 0.7851239669421488\n","Iteration 17562 - loss value [[327.23725309]] accuracy 0.7796143250688705\n","Iteration 17563 - loss value [[330.88886166]] accuracy 0.78099173553719\n","Iteration 17564 - loss value [[328.13429646]] accuracy 0.7796143250688705\n","Iteration 17565 - loss value [[326.01816613]] accuracy 0.7851239669421488\n","Iteration 17566 - loss value [[325.61508274]] accuracy 0.7837465564738292\n","Iteration 17567 - loss value [[325.2357456]] accuracy 0.7823691460055097\n","Iteration 17568 - loss value [[325.41513748]] accuracy 0.7837465564738292\n","Iteration 17569 - loss value [[327.0887375]] accuracy 0.7768595041322314\n","Iteration 17570 - loss value [[331.82000809]] accuracy 0.78099173553719\n","Iteration 17571 - loss value [[328.52057706]] accuracy 0.78099173553719\n","Iteration 17572 - loss value [[328.70019915]] accuracy 0.7823691460055097\n","Iteration 17573 - loss value [[326.28994143]] accuracy 0.78099173553719\n","Iteration 17574 - loss value [[325.77276018]] accuracy 0.7823691460055097\n","Iteration 17575 - loss value [[325.1392113]] accuracy 0.7837465564738292\n","Iteration 17576 - loss value [[325.05215324]] accuracy 0.7865013774104683\n","Iteration 17577 - loss value [[326.5603526]] accuracy 0.7851239669421488\n","Iteration 17578 - loss value [[326.77922925]] accuracy 0.78099173553719\n","Iteration 17579 - loss value [[331.53528311]] accuracy 0.7796143250688705\n","Iteration 17580 - loss value [[328.06644536]] accuracy 0.78099173553719\n","Iteration 17581 - loss value [[326.74405128]] accuracy 0.7837465564738292\n","Iteration 17582 - loss value [[326.8201959]] accuracy 0.7837465564738292\n","Iteration 17583 - loss value [[327.07322443]] accuracy 0.7823691460055097\n","Iteration 17584 - loss value [[326.48050183]] accuracy 0.7823691460055097\n","Iteration 17585 - loss value [[325.29303102]] accuracy 0.7878787878787878\n","Iteration 17586 - loss value [[326.05611954]] accuracy 0.7837465564738292\n","Iteration 17587 - loss value [[327.94234053]] accuracy 0.7796143250688705\n","Iteration 17588 - loss value [[326.15398193]] accuracy 0.7837465564738292\n","Iteration 17589 - loss value [[329.44342665]] accuracy 0.778236914600551\n","Iteration 17590 - loss value [[326.5725913]] accuracy 0.7851239669421488\n","Iteration 17591 - loss value [[327.6065065]] accuracy 0.78099173553719\n","Iteration 17592 - loss value [[328.10898711]] accuracy 0.7837465564738292\n","Iteration 17593 - loss value [[329.10400068]] accuracy 0.7768595041322314\n","Iteration 17594 - loss value [[331.95164999]] accuracy 0.78099173553719\n","Iteration 17595 - loss value [[328.80222828]] accuracy 0.78099173553719\n","Iteration 17596 - loss value [[328.39927781]] accuracy 0.78099173553719\n","Iteration 17597 - loss value [[326.4123644]] accuracy 0.78099173553719\n","Iteration 17598 - loss value [[325.76157832]] accuracy 0.7823691460055097\n","Iteration 17599 - loss value [[325.15569251]] accuracy 0.7837465564738292\n","Iteration 17600 - loss value [[325.54537218]] accuracy 0.7837465564738292\n","Iteration 17601 - loss value [[328.26719636]] accuracy 0.7865013774104683\n","Iteration 17602 - loss value [[330.32536688]] accuracy 0.7741046831955923\n","Iteration 17603 - loss value [[328.21648472]] accuracy 0.7837465564738292\n","Iteration 17604 - loss value [[329.39781057]] accuracy 0.7768595041322314\n","Iteration 17605 - loss value [[337.51209632]] accuracy 0.7754820936639119\n","Iteration 17606 - loss value [[351.03047565]] accuracy 0.7589531680440771\n","Iteration 17607 - loss value [[329.68969102]] accuracy 0.7823691460055097\n","Iteration 17608 - loss value [[329.39197515]] accuracy 0.7768595041322314\n","Iteration 17609 - loss value [[332.70001258]] accuracy 0.78099173553719\n","Iteration 17610 - loss value [[332.37636473]] accuracy 0.7727272727272727\n","Iteration 17611 - loss value [[338.27843703]] accuracy 0.7754820936639119\n","Iteration 17612 - loss value [[364.70122048]] accuracy 0.7424242424242424\n","Iteration 17613 - loss value [[325.75937611]] accuracy 0.7837465564738292\n","Iteration 17614 - loss value [[328.35415116]] accuracy 0.7796143250688705\n","Iteration 17615 - loss value [[330.58660371]] accuracy 0.78099173553719\n","Iteration 17616 - loss value [[326.88125382]] accuracy 0.7754820936639119\n","Iteration 17617 - loss value [[325.81685023]] accuracy 0.7865013774104683\n","Iteration 17618 - loss value [[325.02418964]] accuracy 0.7837465564738292\n","Iteration 17619 - loss value [[325.49684621]] accuracy 0.7865013774104683\n","Iteration 17620 - loss value [[325.11620597]] accuracy 0.7837465564738292\n","Iteration 17621 - loss value [[325.97610951]] accuracy 0.7837465564738292\n","Iteration 17622 - loss value [[326.34652518]] accuracy 0.7823691460055097\n","Iteration 17623 - loss value [[327.4092218]] accuracy 0.7851239669421488\n","Iteration 17624 - loss value [[329.10535599]] accuracy 0.7754820936639119\n","Iteration 17625 - loss value [[335.07803157]] accuracy 0.778236914600551\n","Iteration 17626 - loss value [[340.59556808]] accuracy 0.7713498622589532\n","Iteration 17627 - loss value [[335.71502342]] accuracy 0.78099173553719\n","Iteration 17628 - loss value [[351.92840396]] accuracy 0.756198347107438\n","Iteration 17629 - loss value [[327.84927188]] accuracy 0.7851239669421488\n","Iteration 17630 - loss value [[328.90359063]] accuracy 0.7754820936639119\n","Iteration 17631 - loss value [[334.56960981]] accuracy 0.78099173553719\n","Iteration 17632 - loss value [[343.55877415]] accuracy 0.7699724517906336\n","Iteration 17633 - loss value [[335.33257517]] accuracy 0.7837465564738292\n","Iteration 17634 - loss value [[345.3763626]] accuracy 0.7644628099173554\n","Iteration 17635 - loss value [[347.11767335]] accuracy 0.7713498622589532\n","Iteration 17636 - loss value [[379.65130621]] accuracy 0.7272727272727273\n","Iteration 17637 - loss value [[329.46176284]] accuracy 0.778236914600551\n","Iteration 17638 - loss value [[327.27544619]] accuracy 0.7796143250688705\n","Iteration 17639 - loss value [[334.60773371]] accuracy 0.7837465564738292\n","Iteration 17640 - loss value [[341.77596031]] accuracy 0.768595041322314\n","Iteration 17641 - loss value [[336.84977432]] accuracy 0.7796143250688705\n","Iteration 17642 - loss value [[349.69289978]] accuracy 0.7575757575757576\n","Iteration 17643 - loss value [[326.76985034]] accuracy 0.7837465564738292\n","Iteration 17644 - loss value [[327.64133103]] accuracy 0.7768595041322314\n","Iteration 17645 - loss value [[334.80895087]] accuracy 0.7823691460055097\n","Iteration 17646 - loss value [[350.08024976]] accuracy 0.7575757575757576\n","Iteration 17647 - loss value [[326.12683951]] accuracy 0.7837465564738292\n","Iteration 17648 - loss value [[326.34003888]] accuracy 0.78099173553719\n","Iteration 17649 - loss value [[329.58041096]] accuracy 0.7823691460055097\n","Iteration 17650 - loss value [[326.40526687]] accuracy 0.778236914600551\n","Iteration 17651 - loss value [[325.67106207]] accuracy 0.7837465564738292\n","Iteration 17652 - loss value [[325.18476032]] accuracy 0.7837465564738292\n","Iteration 17653 - loss value [[326.14669232]] accuracy 0.7837465564738292\n","Iteration 17654 - loss value [[326.75385557]] accuracy 0.7796143250688705\n","Iteration 17655 - loss value [[329.35572918]] accuracy 0.7851239669421488\n","Iteration 17656 - loss value [[331.34279329]] accuracy 0.7754820936639119\n","Iteration 17657 - loss value [[326.73951489]] accuracy 0.7865013774104683\n","Iteration 17658 - loss value [[326.35268524]] accuracy 0.7823691460055097\n","Iteration 17659 - loss value [[329.45840247]] accuracy 0.7823691460055097\n","Iteration 17660 - loss value [[326.50354258]] accuracy 0.7768595041322314\n","Iteration 17661 - loss value [[326.09894981]] accuracy 0.7823691460055097\n","Iteration 17662 - loss value [[325.61125308]] accuracy 0.7837465564738292\n","Iteration 17663 - loss value [[326.38352744]] accuracy 0.7837465564738292\n","Iteration 17664 - loss value [[326.69462202]] accuracy 0.7823691460055097\n","Iteration 17665 - loss value [[330.70213789]] accuracy 0.7796143250688705\n","Iteration 17666 - loss value [[327.21357142]] accuracy 0.78099173553719\n","Iteration 17667 - loss value [[325.95229582]] accuracy 0.7851239669421488\n","Iteration 17668 - loss value [[328.16641089]] accuracy 0.778236914600551\n","Iteration 17669 - loss value [[332.3586382]] accuracy 0.7796143250688705\n","Iteration 17670 - loss value [[330.0022423]] accuracy 0.7796143250688705\n","Iteration 17671 - loss value [[328.46037749]] accuracy 0.7851239669421488\n","Iteration 17672 - loss value [[326.54150609]] accuracy 0.7796143250688705\n","Iteration 17673 - loss value [[325.22676369]] accuracy 0.7851239669421488\n","Iteration 17674 - loss value [[325.98019998]] accuracy 0.7823691460055097\n","Iteration 17675 - loss value [[329.17765497]] accuracy 0.7865013774104683\n","Iteration 17676 - loss value [[330.65547122]] accuracy 0.7768595041322314\n","Iteration 17677 - loss value [[333.35912968]] accuracy 0.7823691460055097\n","Iteration 17678 - loss value [[330.3796548]] accuracy 0.78099173553719\n","Iteration 17679 - loss value [[331.13818083]] accuracy 0.7851239669421488\n","Iteration 17680 - loss value [[329.07616637]] accuracy 0.7796143250688705\n","Iteration 17681 - loss value [[327.4054556]] accuracy 0.7865013774104683\n","Iteration 17682 - loss value [[326.02293222]] accuracy 0.78099173553719\n","Iteration 17683 - loss value [[325.63451713]] accuracy 0.7823691460055097\n","Iteration 17684 - loss value [[325.31804206]] accuracy 0.78099173553719\n","Iteration 17685 - loss value [[324.97210175]] accuracy 0.7865013774104683\n","Iteration 17686 - loss value [[326.17507098]] accuracy 0.7837465564738292\n","Iteration 17687 - loss value [[326.9465678]] accuracy 0.78099173553719\n","Iteration 17688 - loss value [[330.99292949]] accuracy 0.78099173553719\n","Iteration 17689 - loss value [[326.6791786]] accuracy 0.78099173553719\n","Iteration 17690 - loss value [[325.6097306]] accuracy 0.7837465564738292\n","Iteration 17691 - loss value [[327.67897536]] accuracy 0.7768595041322314\n","Iteration 17692 - loss value [[333.77879913]] accuracy 0.78099173553719\n","Iteration 17693 - loss value [[332.87742122]] accuracy 0.7768595041322314\n","Iteration 17694 - loss value [[331.77309606]] accuracy 0.7823691460055097\n","Iteration 17695 - loss value [[328.78504454]] accuracy 0.78099173553719\n","Iteration 17696 - loss value [[328.63697038]] accuracy 0.78099173553719\n","Iteration 17697 - loss value [[326.43788951]] accuracy 0.78099173553719\n","Iteration 17698 - loss value [[326.03046216]] accuracy 0.7823691460055097\n","Iteration 17699 - loss value [[325.26239069]] accuracy 0.7823691460055097\n","Iteration 17700 - loss value [[324.64040527]] accuracy 0.7865013774104683\n","Iteration 17701 - loss value [[324.67137618]] accuracy 0.7865013774104683\n","Iteration 17702 - loss value [[324.89716821]] accuracy 0.7865013774104683\n","Iteration 17703 - loss value [[326.02237022]] accuracy 0.7837465564738292\n","Iteration 17704 - loss value [[326.89606466]] accuracy 0.778236914600551\n","Iteration 17705 - loss value [[332.42561394]] accuracy 0.78099173553719\n","Iteration 17706 - loss value [[332.06413032]] accuracy 0.778236914600551\n","Iteration 17707 - loss value [[340.95652404]] accuracy 0.7754820936639119\n","Iteration 17708 - loss value [[360.98029339]] accuracy 0.7465564738292011\n","Iteration 17709 - loss value [[326.54006902]] accuracy 0.7837465564738292\n","Iteration 17710 - loss value [[326.85840887]] accuracy 0.778236914600551\n","Iteration 17711 - loss value [[331.63836341]] accuracy 0.7823691460055097\n","Iteration 17712 - loss value [[330.36218627]] accuracy 0.7796143250688705\n","Iteration 17713 - loss value [[333.5983081]] accuracy 0.7837465564738292\n","Iteration 17714 - loss value [[343.27372155]] accuracy 0.7658402203856749\n","Iteration 17715 - loss value [[338.74718762]] accuracy 0.78099173553719\n","Iteration 17716 - loss value [[352.52935095]] accuracy 0.756198347107438\n","Iteration 17717 - loss value [[326.56217615]] accuracy 0.778236914600551\n","Iteration 17718 - loss value [[326.29052334]] accuracy 0.78099173553719\n","Iteration 17719 - loss value [[331.20834091]] accuracy 0.7823691460055097\n","Iteration 17720 - loss value [[328.58738201]] accuracy 0.78099173553719\n","Iteration 17721 - loss value [[328.97565805]] accuracy 0.78099173553719\n","Iteration 17722 - loss value [[327.27264497]] accuracy 0.78099173553719\n","Iteration 17723 - loss value [[326.01140852]] accuracy 0.7823691460055097\n","Iteration 17724 - loss value [[325.51495992]] accuracy 0.7837465564738292\n","Iteration 17725 - loss value [[325.94062459]] accuracy 0.7851239669421488\n","Iteration 17726 - loss value [[326.77400267]] accuracy 0.78099173553719\n","Iteration 17727 - loss value [[326.56765504]] accuracy 0.78099173553719\n","Iteration 17728 - loss value [[333.20044705]] accuracy 0.778236914600551\n","Iteration 17729 - loss value [[335.55693818]] accuracy 0.7713498622589532\n","Iteration 17730 - loss value [[340.56049983]] accuracy 0.7727272727272727\n","Iteration 17731 - loss value [[370.66108635]] accuracy 0.7382920110192838\n","Iteration 17732 - loss value [[330.42672035]] accuracy 0.7768595041322314\n","Iteration 17733 - loss value [[332.08923351]] accuracy 0.7768595041322314\n","Iteration 17734 - loss value [[325.76158087]] accuracy 0.7851239669421488\n","Iteration 17735 - loss value [[327.41765634]] accuracy 0.78099173553719\n","Iteration 17736 - loss value [[327.27220376]] accuracy 0.7865013774104683\n","Iteration 17737 - loss value [[326.22780471]] accuracy 0.78099173553719\n","Iteration 17738 - loss value [[325.62919018]] accuracy 0.7837465564738292\n","Iteration 17739 - loss value [[325.68986224]] accuracy 0.7837465564738292\n","Iteration 17740 - loss value [[327.37101322]] accuracy 0.7823691460055097\n","Iteration 17741 - loss value [[325.96447829]] accuracy 0.78099173553719\n","Iteration 17742 - loss value [[328.01199575]] accuracy 0.7823691460055097\n","Iteration 17743 - loss value [[326.58429604]] accuracy 0.78099173553719\n","Iteration 17744 - loss value [[328.00540752]] accuracy 0.7823691460055097\n","Iteration 17745 - loss value [[327.68817476]] accuracy 0.7796143250688705\n","Iteration 17746 - loss value [[328.34229134]] accuracy 0.7796143250688705\n","Iteration 17747 - loss value [[328.2535238]] accuracy 0.78099173553719\n","Iteration 17748 - loss value [[326.91640492]] accuracy 0.7865013774104683\n","Iteration 17749 - loss value [[325.80155419]] accuracy 0.78099173553719\n","Iteration 17750 - loss value [[326.02908281]] accuracy 0.78099173553719\n","Iteration 17751 - loss value [[328.06577193]] accuracy 0.778236914600551\n","Iteration 17752 - loss value [[338.75319787]] accuracy 0.7741046831955923\n","Iteration 17753 - loss value [[356.70125355]] accuracy 0.7465564738292011\n","Iteration 17754 - loss value [[325.5629327]] accuracy 0.7823691460055097\n","Iteration 17755 - loss value [[324.98102074]] accuracy 0.7878787878787878\n","Iteration 17756 - loss value [[326.74669122]] accuracy 0.778236914600551\n","Iteration 17757 - loss value [[331.63442556]] accuracy 0.7823691460055097\n","Iteration 17758 - loss value [[329.87884579]] accuracy 0.7796143250688705\n","Iteration 17759 - loss value [[333.77164186]] accuracy 0.7837465564738292\n","Iteration 17760 - loss value [[343.65221683]] accuracy 0.768595041322314\n","Iteration 17761 - loss value [[337.30825488]] accuracy 0.7796143250688705\n","Iteration 17762 - loss value [[350.55320102]] accuracy 0.756198347107438\n","Iteration 17763 - loss value [[326.05980699]] accuracy 0.7796143250688705\n","Iteration 17764 - loss value [[327.77304044]] accuracy 0.778236914600551\n","Iteration 17765 - loss value [[329.63493986]] accuracy 0.7823691460055097\n","Iteration 17766 - loss value [[332.26142488]] accuracy 0.7754820936639119\n","Iteration 17767 - loss value [[329.90842209]] accuracy 0.7837465564738292\n","Iteration 17768 - loss value [[334.71717627]] accuracy 0.7727272727272727\n","Iteration 17769 - loss value [[330.58950188]] accuracy 0.7823691460055097\n","Iteration 17770 - loss value [[335.7387175]] accuracy 0.7727272727272727\n","Iteration 17771 - loss value [[335.74201157]] accuracy 0.7754820936639119\n","Iteration 17772 - loss value [[339.20342035]] accuracy 0.7699724517906336\n","Iteration 17773 - loss value [[331.99749575]] accuracy 0.78099173553719\n","Iteration 17774 - loss value [[332.99049705]] accuracy 0.7727272727272727\n","Iteration 17775 - loss value [[338.37057454]] accuracy 0.7754820936639119\n","Iteration 17776 - loss value [[361.07298528]] accuracy 0.7493112947658402\n","Iteration 17777 - loss value [[326.12559884]] accuracy 0.7796143250688705\n","Iteration 17778 - loss value [[329.91694998]] accuracy 0.7823691460055097\n","Iteration 17779 - loss value [[334.55335422]] accuracy 0.7768595041322314\n","Iteration 17780 - loss value [[330.35592746]] accuracy 0.7851239669421488\n","Iteration 17781 - loss value [[333.18902535]] accuracy 0.7754820936639119\n","Iteration 17782 - loss value [[329.22769335]] accuracy 0.7837465564738292\n","Iteration 17783 - loss value [[329.03115317]] accuracy 0.7754820936639119\n","Iteration 17784 - loss value [[327.12059594]] accuracy 0.7878787878787878\n","Iteration 17785 - loss value [[327.82461826]] accuracy 0.7768595041322314\n","Iteration 17786 - loss value [[333.18031036]] accuracy 0.7796143250688705\n","Iteration 17787 - loss value [[330.64228101]] accuracy 0.7796143250688705\n","Iteration 17788 - loss value [[330.08302296]] accuracy 0.7851239669421488\n","Iteration 17789 - loss value [[329.37399021]] accuracy 0.7796143250688705\n","Iteration 17790 - loss value [[329.96369016]] accuracy 0.7851239669421488\n","Iteration 17791 - loss value [[329.14306744]] accuracy 0.7796143250688705\n","Iteration 17792 - loss value [[328.2222868]] accuracy 0.78099173553719\n","Iteration 17793 - loss value [[326.60146923]] accuracy 0.78099173553719\n","Iteration 17794 - loss value [[325.96310142]] accuracy 0.7823691460055097\n","Iteration 17795 - loss value [[325.46530609]] accuracy 0.7796143250688705\n","Iteration 17796 - loss value [[325.37763341]] accuracy 0.7837465564738292\n","Iteration 17797 - loss value [[325.34106265]] accuracy 0.7823691460055097\n","Iteration 17798 - loss value [[325.81845557]] accuracy 0.7837465564738292\n","Iteration 17799 - loss value [[329.13639274]] accuracy 0.7768595041322314\n","Iteration 17800 - loss value [[329.88445566]] accuracy 0.7851239669421488\n","Iteration 17801 - loss value [[332.18035874]] accuracy 0.7741046831955923\n","Iteration 17802 - loss value [[326.48586259]] accuracy 0.7851239669421488\n","Iteration 17803 - loss value [[327.64939197]] accuracy 0.7796143250688705\n","Iteration 17804 - loss value [[328.10829686]] accuracy 0.7851239669421488\n","Iteration 17805 - loss value [[329.52277914]] accuracy 0.7741046831955923\n","Iteration 17806 - loss value [[333.75032896]] accuracy 0.7851239669421488\n","Iteration 17807 - loss value [[337.55931818]] accuracy 0.7741046831955923\n","Iteration 17808 - loss value [[330.19845751]] accuracy 0.7823691460055097\n","Iteration 17809 - loss value [[327.8233749]] accuracy 0.7796143250688705\n","Iteration 17810 - loss value [[325.91863066]] accuracy 0.7823691460055097\n","Iteration 17811 - loss value [[325.63840875]] accuracy 0.7837465564738292\n","Iteration 17812 - loss value [[325.19414765]] accuracy 0.7837465564738292\n","Iteration 17813 - loss value [[324.79835007]] accuracy 0.7865013774104683\n","Iteration 17814 - loss value [[325.41418727]] accuracy 0.7851239669421488\n","Iteration 17815 - loss value [[327.08144707]] accuracy 0.7823691460055097\n","Iteration 17816 - loss value [[330.92433946]] accuracy 0.78099173553719\n","Iteration 17817 - loss value [[327.96391716]] accuracy 0.778236914600551\n","Iteration 17818 - loss value [[327.25470876]] accuracy 0.7878787878787878\n","Iteration 17819 - loss value [[326.32101945]] accuracy 0.7823691460055097\n","Iteration 17820 - loss value [[327.58534812]] accuracy 0.7837465564738292\n","Iteration 17821 - loss value [[327.07818863]] accuracy 0.7796143250688705\n","Iteration 17822 - loss value [[331.64604465]] accuracy 0.78099173553719\n","Iteration 17823 - loss value [[332.5983316]] accuracy 0.7754820936639119\n","Iteration 17824 - loss value [[331.80837715]] accuracy 0.7851239669421488\n","Iteration 17825 - loss value [[337.56731622]] accuracy 0.7713498622589532\n","Iteration 17826 - loss value [[332.43912912]] accuracy 0.7796143250688705\n","Iteration 17827 - loss value [[334.92909537]] accuracy 0.7727272727272727\n","Iteration 17828 - loss value [[332.19108309]] accuracy 0.7837465564738292\n","Iteration 17829 - loss value [[337.51529832]] accuracy 0.7699724517906336\n","Iteration 17830 - loss value [[331.89860929]] accuracy 0.78099173553719\n","Iteration 17831 - loss value [[332.30088388]] accuracy 0.7754820936639119\n","Iteration 17832 - loss value [[338.68886754]] accuracy 0.7768595041322314\n","Iteration 17833 - loss value [[364.53586806]] accuracy 0.7479338842975206\n","Iteration 17834 - loss value [[325.23348365]] accuracy 0.7851239669421488\n","Iteration 17835 - loss value [[324.85294986]] accuracy 0.7851239669421488\n","Iteration 17836 - loss value [[325.784693]] accuracy 0.78099173553719\n","Iteration 17837 - loss value [[328.17496844]] accuracy 0.7837465564738292\n","Iteration 17838 - loss value [[330.10394519]] accuracy 0.7727272727272727\n","Iteration 17839 - loss value [[330.7279301]] accuracy 0.7837465564738292\n","Iteration 17840 - loss value [[329.71725559]] accuracy 0.7768595041322314\n","Iteration 17841 - loss value [[334.95855277]] accuracy 0.7851239669421488\n","Iteration 17842 - loss value [[352.20006077]] accuracy 0.756198347107438\n","Iteration 17843 - loss value [[325.41985385]] accuracy 0.7851239669421488\n","Iteration 17844 - loss value [[327.89534868]] accuracy 0.778236914600551\n","Iteration 17845 - loss value [[329.5381714]] accuracy 0.7837465564738292\n","Iteration 17846 - loss value [[325.98644428]] accuracy 0.7796143250688705\n","Iteration 17847 - loss value [[325.86982511]] accuracy 0.7837465564738292\n","Iteration 17848 - loss value [[325.16722434]] accuracy 0.7837465564738292\n","Iteration 17849 - loss value [[325.19722609]] accuracy 0.7851239669421488\n","Iteration 17850 - loss value [[325.70986476]] accuracy 0.7796143250688705\n","Iteration 17851 - loss value [[325.53848401]] accuracy 0.7837465564738292\n","Iteration 17852 - loss value [[327.63439961]] accuracy 0.7837465564738292\n","Iteration 17853 - loss value [[327.13350643]] accuracy 0.7823691460055097\n","Iteration 17854 - loss value [[327.65011366]] accuracy 0.7851239669421488\n","Iteration 17855 - loss value [[327.92047364]] accuracy 0.778236914600551\n","Iteration 17856 - loss value [[334.71931938]] accuracy 0.78099173553719\n","Iteration 17857 - loss value [[345.06439152]] accuracy 0.7617079889807162\n","Iteration 17858 - loss value [[332.91632531]] accuracy 0.7823691460055097\n","Iteration 17859 - loss value [[338.87512143]] accuracy 0.7713498622589532\n","Iteration 17860 - loss value [[339.60518533]] accuracy 0.7754820936639119\n","Iteration 17861 - loss value [[357.75428947]] accuracy 0.7479338842975206\n","Iteration 17862 - loss value [[326.13963761]] accuracy 0.78099173553719\n","Iteration 17863 - loss value [[329.45323695]] accuracy 0.7754820936639119\n","Iteration 17864 - loss value [[326.62293451]] accuracy 0.7823691460055097\n","Iteration 17865 - loss value [[327.37264076]] accuracy 0.7796143250688705\n","Iteration 17866 - loss value [[334.38774021]] accuracy 0.7851239669421488\n","Iteration 17867 - loss value [[341.97679189]] accuracy 0.7699724517906336\n","Iteration 17868 - loss value [[335.21148474]] accuracy 0.78099173553719\n","Iteration 17869 - loss value [[338.426788]] accuracy 0.7727272727272727\n","Iteration 17870 - loss value [[333.14220622]] accuracy 0.7837465564738292\n","Iteration 17871 - loss value [[339.75323924]] accuracy 0.7727272727272727\n","Iteration 17872 - loss value [[332.29179845]] accuracy 0.778236914600551\n","Iteration 17873 - loss value [[330.90897526]] accuracy 0.7754820936639119\n","Iteration 17874 - loss value [[331.71176133]] accuracy 0.7851239669421488\n","Iteration 17875 - loss value [[333.54332898]] accuracy 0.7713498622589532\n","Iteration 17876 - loss value [[343.50934563]] accuracy 0.7727272727272727\n","Iteration 17877 - loss value [[377.49352527]] accuracy 0.7258953168044077\n","Iteration 17878 - loss value [[340.57581123]] accuracy 0.7768595041322314\n","Iteration 17879 - loss value [[348.34566182]] accuracy 0.7548209366391184\n","Iteration 17880 - loss value [[328.40282478]] accuracy 0.7837465564738292\n","Iteration 17881 - loss value [[327.69993639]] accuracy 0.778236914600551\n","Iteration 17882 - loss value [[327.28511464]] accuracy 0.7851239669421488\n","Iteration 17883 - loss value [[327.53109449]] accuracy 0.7768595041322314\n","Iteration 17884 - loss value [[331.20437895]] accuracy 0.7823691460055097\n","Iteration 17885 - loss value [[328.87086736]] accuracy 0.7796143250688705\n","Iteration 17886 - loss value [[327.88373157]] accuracy 0.7851239669421488\n","Iteration 17887 - loss value [[326.04613807]] accuracy 0.78099173553719\n","Iteration 17888 - loss value [[325.56957886]] accuracy 0.7823691460055097\n","Iteration 17889 - loss value [[325.34605077]] accuracy 0.7823691460055097\n","Iteration 17890 - loss value [[325.25344803]] accuracy 0.7837465564738292\n","Iteration 17891 - loss value [[325.42271789]] accuracy 0.7851239669421488\n","Iteration 17892 - loss value [[326.41986797]] accuracy 0.7851239669421488\n","Iteration 17893 - loss value [[326.8238819]] accuracy 0.78099173553719\n","Iteration 17894 - loss value [[331.36245358]] accuracy 0.7796143250688705\n","Iteration 17895 - loss value [[328.02055821]] accuracy 0.78099173553719\n","Iteration 17896 - loss value [[326.53882514]] accuracy 0.7851239669421488\n","Iteration 17897 - loss value [[325.54493428]] accuracy 0.7837465564738292\n","Iteration 17898 - loss value [[326.28721805]] accuracy 0.7837465564738292\n","Iteration 17899 - loss value [[325.91781994]] accuracy 0.7837465564738292\n","Iteration 17900 - loss value [[327.61015834]] accuracy 0.78099173553719\n","Iteration 17901 - loss value [[326.11394639]] accuracy 0.7823691460055097\n","Iteration 17902 - loss value [[325.71544563]] accuracy 0.7851239669421488\n","Iteration 17903 - loss value [[326.7432926]] accuracy 0.7823691460055097\n","Iteration 17904 - loss value [[325.55517239]] accuracy 0.7837465564738292\n","Iteration 17905 - loss value [[328.8826343]] accuracy 0.7768595041322314\n","Iteration 17906 - loss value [[327.37530501]] accuracy 0.7865013774104683\n","Iteration 17907 - loss value [[329.61397924]] accuracy 0.7741046831955923\n","Iteration 17908 - loss value [[330.67061566]] accuracy 0.7837465564738292\n","Iteration 17909 - loss value [[335.42702922]] accuracy 0.7727272727272727\n","Iteration 17910 - loss value [[331.56619501]] accuracy 0.7823691460055097\n","Iteration 17911 - loss value [[330.64046927]] accuracy 0.7796143250688705\n","Iteration 17912 - loss value [[337.53906009]] accuracy 0.7796143250688705\n","Iteration 17913 - loss value [[352.95248657]] accuracy 0.756198347107438\n","Iteration 17914 - loss value [[325.17422073]] accuracy 0.7865013774104683\n","Iteration 17915 - loss value [[325.56602888]] accuracy 0.7837465564738292\n","Iteration 17916 - loss value [[326.6028018]] accuracy 0.7851239669421488\n","Iteration 17917 - loss value [[326.397981]] accuracy 0.78099173553719\n","Iteration 17918 - loss value [[330.35078685]] accuracy 0.7796143250688705\n","Iteration 17919 - loss value [[326.79147672]] accuracy 0.7796143250688705\n","Iteration 17920 - loss value [[325.33412265]] accuracy 0.7837465564738292\n","Iteration 17921 - loss value [[326.14719267]] accuracy 0.78099173553719\n","Iteration 17922 - loss value [[328.34839736]] accuracy 0.7851239669421488\n","Iteration 17923 - loss value [[329.24489752]] accuracy 0.7768595041322314\n","Iteration 17924 - loss value [[336.29653859]] accuracy 0.7768595041322314\n","Iteration 17925 - loss value [[344.77465948]] accuracy 0.768595041322314\n","Iteration 17926 - loss value [[337.94894647]] accuracy 0.7823691460055097\n","Iteration 17927 - loss value [[350.05333339]] accuracy 0.756198347107438\n","Iteration 17928 - loss value [[325.78920694]] accuracy 0.7837465564738292\n","Iteration 17929 - loss value [[325.23895623]] accuracy 0.7851239669421488\n","Iteration 17930 - loss value [[326.60579216]] accuracy 0.7823691460055097\n","Iteration 17931 - loss value [[328.78509885]] accuracy 0.7754820936639119\n","Iteration 17932 - loss value [[334.82752726]] accuracy 0.7796143250688705\n","Iteration 17933 - loss value [[339.25126906]] accuracy 0.7699724517906336\n","Iteration 17934 - loss value [[332.05257885]] accuracy 0.7823691460055097\n","Iteration 17935 - loss value [[336.11674195]] accuracy 0.7727272727272727\n","Iteration 17936 - loss value [[329.2324069]] accuracy 0.7837465564738292\n","Iteration 17937 - loss value [[326.08263374]] accuracy 0.7823691460055097\n","Iteration 17938 - loss value [[325.98451294]] accuracy 0.7837465564738292\n","Iteration 17939 - loss value [[325.32878245]] accuracy 0.7837465564738292\n","Iteration 17940 - loss value [[326.06411952]] accuracy 0.7823691460055097\n","Iteration 17941 - loss value [[326.99839634]] accuracy 0.78099173553719\n","Iteration 17942 - loss value [[331.12139632]] accuracy 0.78099173553719\n","Iteration 17943 - loss value [[326.74177669]] accuracy 0.7823691460055097\n","Iteration 17944 - loss value [[325.89689318]] accuracy 0.7851239669421488\n","Iteration 17945 - loss value [[326.07761916]] accuracy 0.7823691460055097\n","Iteration 17946 - loss value [[325.75281193]] accuracy 0.7865013774104683\n","Iteration 17947 - loss value [[327.91317454]] accuracy 0.778236914600551\n","Iteration 17948 - loss value [[331.80372193]] accuracy 0.7823691460055097\n","Iteration 17949 - loss value [[329.30548101]] accuracy 0.7768595041322314\n","Iteration 17950 - loss value [[329.44599068]] accuracy 0.7837465564738292\n","Iteration 17951 - loss value [[327.31497246]] accuracy 0.78099173553719\n","Iteration 17952 - loss value [[326.26005459]] accuracy 0.7837465564738292\n","Iteration 17953 - loss value [[325.91312194]] accuracy 0.7837465564738292\n","Iteration 17954 - loss value [[325.51634199]] accuracy 0.7837465564738292\n","Iteration 17955 - loss value [[325.25428835]] accuracy 0.7837465564738292\n","Iteration 17956 - loss value [[326.18769668]] accuracy 0.7851239669421488\n","Iteration 17957 - loss value [[325.4541444]] accuracy 0.7823691460055097\n","Iteration 17958 - loss value [[326.10203052]] accuracy 0.7851239669421488\n","Iteration 17959 - loss value [[327.27875835]] accuracy 0.778236914600551\n","Iteration 17960 - loss value [[331.11428889]] accuracy 0.7823691460055097\n","Iteration 17961 - loss value [[327.72688401]] accuracy 0.7796143250688705\n","Iteration 17962 - loss value [[326.83166106]] accuracy 0.7837465564738292\n","Iteration 17963 - loss value [[326.02329055]] accuracy 0.7823691460055097\n","Iteration 17964 - loss value [[327.20420343]] accuracy 0.7865013774104683\n","Iteration 17965 - loss value [[327.08811789]] accuracy 0.7768595041322314\n","Iteration 17966 - loss value [[332.18362306]] accuracy 0.78099173553719\n","Iteration 17967 - loss value [[333.31879355]] accuracy 0.7741046831955923\n","Iteration 17968 - loss value [[334.43849437]] accuracy 0.7823691460055097\n","Iteration 17969 - loss value [[349.77517491]] accuracy 0.756198347107438\n","Iteration 17970 - loss value [[330.54996926]] accuracy 0.7851239669421488\n","Iteration 17971 - loss value [[328.69619499]] accuracy 0.78099173553719\n","Iteration 17972 - loss value [[328.30528578]] accuracy 0.7837465564738292\n","Iteration 17973 - loss value [[326.27708403]] accuracy 0.78099173553719\n","Iteration 17974 - loss value [[326.04165092]] accuracy 0.7837465564738292\n","Iteration 17975 - loss value [[325.24087307]] accuracy 0.78099173553719\n","Iteration 17976 - loss value [[324.60496261]] accuracy 0.7865013774104683\n","Iteration 17977 - loss value [[324.72946279]] accuracy 0.7865013774104683\n","Iteration 17978 - loss value [[325.48213715]] accuracy 0.7837465564738292\n","Iteration 17979 - loss value [[324.7018735]] accuracy 0.7865013774104683\n","Iteration 17980 - loss value [[325.13748133]] accuracy 0.7837465564738292\n","Iteration 17981 - loss value [[326.89281681]] accuracy 0.7796143250688705\n","Iteration 17982 - loss value [[333.47666906]] accuracy 0.7796143250688705\n","Iteration 17983 - loss value [[334.36497314]] accuracy 0.7741046831955923\n","Iteration 17984 - loss value [[335.73521419]] accuracy 0.7837465564738292\n","Iteration 17985 - loss value [[354.26099473]] accuracy 0.7548209366391184\n","Iteration 17986 - loss value [[325.58234427]] accuracy 0.7823691460055097\n","Iteration 17987 - loss value [[327.22667533]] accuracy 0.778236914600551\n","Iteration 17988 - loss value [[328.25134467]] accuracy 0.7865013774104683\n","Iteration 17989 - loss value [[329.47324248]] accuracy 0.7768595041322314\n","Iteration 17990 - loss value [[330.54931727]] accuracy 0.778236914600551\n","Iteration 17991 - loss value [[327.36131117]] accuracy 0.78099173553719\n","Iteration 17992 - loss value [[325.80514673]] accuracy 0.7837465564738292\n","Iteration 17993 - loss value [[325.53839749]] accuracy 0.7823691460055097\n","Iteration 17994 - loss value [[326.57757562]] accuracy 0.7837465564738292\n","Iteration 17995 - loss value [[326.77242575]] accuracy 0.7796143250688705\n","Iteration 17996 - loss value [[331.02901318]] accuracy 0.78099173553719\n","Iteration 17997 - loss value [[326.78513863]] accuracy 0.7823691460055097\n","Iteration 17998 - loss value [[325.98255654]] accuracy 0.7851239669421488\n","Iteration 17999 - loss value [[328.53747867]] accuracy 0.778236914600551\n","Iteration 18000 - loss value [[335.00080468]] accuracy 0.78099173553719\n","Iteration 18001 - loss value [[338.93907476]] accuracy 0.7699724517906336\n","Iteration 18002 - loss value [[333.60712684]] accuracy 0.7837465564738292\n","Iteration 18003 - loss value [[341.35920302]] accuracy 0.7727272727272727\n","Iteration 18004 - loss value [[335.92016547]] accuracy 0.7796143250688705\n","Iteration 18005 - loss value [[345.1086641]] accuracy 0.7589531680440771\n","Iteration 18006 - loss value [[339.41594776]] accuracy 0.78099173553719\n","Iteration 18007 - loss value [[358.30321618]] accuracy 0.7493112947658402\n","Iteration 18008 - loss value [[326.43371729]] accuracy 0.7796143250688705\n","Iteration 18009 - loss value [[327.06308082]] accuracy 0.7796143250688705\n","Iteration 18010 - loss value [[326.77566711]] accuracy 0.7796143250688705\n","Iteration 18011 - loss value [[331.06663146]] accuracy 0.78099173553719\n","Iteration 18012 - loss value [[329.49692248]] accuracy 0.778236914600551\n","Iteration 18013 - loss value [[327.65755075]] accuracy 0.7851239669421488\n","Iteration 18014 - loss value [[327.94517821]] accuracy 0.7754820936639119\n","Iteration 18015 - loss value [[335.55806652]] accuracy 0.7796143250688705\n","Iteration 18016 - loss value [[351.80838637]] accuracy 0.7548209366391184\n","Iteration 18017 - loss value [[326.2528709]] accuracy 0.7823691460055097\n","Iteration 18018 - loss value [[327.05023825]] accuracy 0.778236914600551\n","Iteration 18019 - loss value [[332.54661029]] accuracy 0.7796143250688705\n","Iteration 18020 - loss value [[332.17842933]] accuracy 0.7727272727272727\n","Iteration 18021 - loss value [[337.99413946]] accuracy 0.7823691460055097\n","Iteration 18022 - loss value [[356.75280209]] accuracy 0.7493112947658402\n","Iteration 18023 - loss value [[325.7437697]] accuracy 0.7837465564738292\n","Iteration 18024 - loss value [[328.90581441]] accuracy 0.7768595041322314\n","Iteration 18025 - loss value [[328.06312508]] accuracy 0.7851239669421488\n","Iteration 18026 - loss value [[327.77133516]] accuracy 0.7796143250688705\n","Iteration 18027 - loss value [[328.48144795]] accuracy 0.7851239669421488\n","Iteration 18028 - loss value [[330.93671795]] accuracy 0.7741046831955923\n","Iteration 18029 - loss value [[328.82326511]] accuracy 0.7851239669421488\n","Iteration 18030 - loss value [[331.33671382]] accuracy 0.7754820936639119\n","Iteration 18031 - loss value [[327.18672303]] accuracy 0.7851239669421488\n","Iteration 18032 - loss value [[327.15951403]] accuracy 0.778236914600551\n","Iteration 18033 - loss value [[330.19483209]] accuracy 0.778236914600551\n","Iteration 18034 - loss value [[327.3813252]] accuracy 0.7796143250688705\n","Iteration 18035 - loss value [[326.00241736]] accuracy 0.7823691460055097\n","Iteration 18036 - loss value [[325.32221488]] accuracy 0.7823691460055097\n","Iteration 18037 - loss value [[324.75210655]] accuracy 0.7851239669421488\n","Iteration 18038 - loss value [[324.9615812]] accuracy 0.7851239669421488\n","Iteration 18039 - loss value [[326.30742575]] accuracy 0.778236914600551\n","Iteration 18040 - loss value [[330.43351412]] accuracy 0.7823691460055097\n","Iteration 18041 - loss value [[327.24126641]] accuracy 0.778236914600551\n","Iteration 18042 - loss value [[325.50508369]] accuracy 0.7823691460055097\n","Iteration 18043 - loss value [[325.02666866]] accuracy 0.7837465564738292\n","Iteration 18044 - loss value [[325.74078115]] accuracy 0.7851239669421488\n","Iteration 18045 - loss value [[328.38061507]] accuracy 0.778236914600551\n","Iteration 18046 - loss value [[331.12791821]] accuracy 0.7823691460055097\n","Iteration 18047 - loss value [[328.87736918]] accuracy 0.7796143250688705\n","Iteration 18048 - loss value [[331.31046499]] accuracy 0.7865013774104683\n","Iteration 18049 - loss value [[330.63796172]] accuracy 0.78099173553719\n","Iteration 18050 - loss value [[331.81079723]] accuracy 0.78099173553719\n","Iteration 18051 - loss value [[339.43335178]] accuracy 0.768595041322314\n","Iteration 18052 - loss value [[337.29842825]] accuracy 0.7796143250688705\n","Iteration 18053 - loss value [[352.33040301]] accuracy 0.7534435261707989\n","Iteration 18054 - loss value [[325.17897242]] accuracy 0.7837465564738292\n","Iteration 18055 - loss value [[325.81558038]] accuracy 0.7837465564738292\n","Iteration 18056 - loss value [[326.25978355]] accuracy 0.7823691460055097\n","Iteration 18057 - loss value [[329.63011021]] accuracy 0.7837465564738292\n","Iteration 18058 - loss value [[331.20914832]] accuracy 0.7741046831955923\n","Iteration 18059 - loss value [[329.69841546]] accuracy 0.7837465564738292\n","Iteration 18060 - loss value [[327.76121634]] accuracy 0.78099173553719\n","Iteration 18061 - loss value [[325.84794731]] accuracy 0.7837465564738292\n","Iteration 18062 - loss value [[325.75090184]] accuracy 0.78099173553719\n","Iteration 18063 - loss value [[325.64788604]] accuracy 0.7837465564738292\n","Iteration 18064 - loss value [[325.49017839]] accuracy 0.7837465564738292\n","Iteration 18065 - loss value [[325.45145018]] accuracy 0.7865013774104683\n","Iteration 18066 - loss value [[326.32972176]] accuracy 0.7837465564738292\n","Iteration 18067 - loss value [[325.86189132]] accuracy 0.7837465564738292\n","Iteration 18068 - loss value [[325.80829641]] accuracy 0.78099173553719\n","Iteration 18069 - loss value [[328.53577701]] accuracy 0.7851239669421488\n","Iteration 18070 - loss value [[329.37964798]] accuracy 0.778236914600551\n","Iteration 18071 - loss value [[338.44053813]] accuracy 0.7796143250688705\n","Iteration 18072 - loss value [[354.30242815]] accuracy 0.7534435261707989\n","Iteration 18073 - loss value [[325.45393475]] accuracy 0.7823691460055097\n","Iteration 18074 - loss value [[327.07869324]] accuracy 0.7796143250688705\n","Iteration 18075 - loss value [[334.16749315]] accuracy 0.7837465564738292\n","Iteration 18076 - loss value [[338.81232545]] accuracy 0.7713498622589532\n","Iteration 18077 - loss value [[334.79635637]] accuracy 0.7837465564738292\n","Iteration 18078 - loss value [[347.75452166]] accuracy 0.7589531680440771\n","Iteration 18079 - loss value [[334.44253565]] accuracy 0.7851239669421488\n","Iteration 18080 - loss value [[342.48095041]] accuracy 0.7699724517906336\n","Iteration 18081 - loss value [[335.49154706]] accuracy 0.78099173553719\n","Iteration 18082 - loss value [[349.93369883]] accuracy 0.7603305785123967\n","Iteration 18083 - loss value [[325.3406304]] accuracy 0.7837465564738292\n","Iteration 18084 - loss value [[326.24391673]] accuracy 0.7865013774104683\n","Iteration 18085 - loss value [[325.30939862]] accuracy 0.7837465564738292\n","Iteration 18086 - loss value [[326.47855705]] accuracy 0.7851239669421488\n","Iteration 18087 - loss value [[325.13770943]] accuracy 0.7837465564738292\n","Iteration 18088 - loss value [[325.22296931]] accuracy 0.7865013774104683\n","Iteration 18089 - loss value [[326.84801201]] accuracy 0.7768595041322314\n","Iteration 18090 - loss value [[331.66284216]] accuracy 0.7796143250688705\n","Iteration 18091 - loss value [[328.10027298]] accuracy 0.78099173553719\n","Iteration 18092 - loss value [[328.35873706]] accuracy 0.7837465564738292\n","Iteration 18093 - loss value [[325.6469639]] accuracy 0.7823691460055097\n","Iteration 18094 - loss value [[325.20582933]] accuracy 0.7837465564738292\n","Iteration 18095 - loss value [[326.08237244]] accuracy 0.7796143250688705\n","Iteration 18096 - loss value [[328.66815824]] accuracy 0.7851239669421488\n","Iteration 18097 - loss value [[330.79123073]] accuracy 0.7741046831955923\n","Iteration 18098 - loss value [[329.29328018]] accuracy 0.78099173553719\n","Iteration 18099 - loss value [[325.88725959]] accuracy 0.7796143250688705\n","Iteration 18100 - loss value [[325.15456187]] accuracy 0.7837465564738292\n","Iteration 18101 - loss value [[325.86039407]] accuracy 0.78099173553719\n","Iteration 18102 - loss value [[327.89041204]] accuracy 0.7837465564738292\n","Iteration 18103 - loss value [[328.01894765]] accuracy 0.7754820936639119\n","Iteration 18104 - loss value [[332.07819432]] accuracy 0.7796143250688705\n","Iteration 18105 - loss value [[328.36150342]] accuracy 0.7796143250688705\n","Iteration 18106 - loss value [[328.1565406]] accuracy 0.7823691460055097\n","Iteration 18107 - loss value [[327.38031406]] accuracy 0.7823691460055097\n","Iteration 18108 - loss value [[327.97902038]] accuracy 0.7851239669421488\n","Iteration 18109 - loss value [[327.80235031]] accuracy 0.778236914600551\n","Iteration 18110 - loss value [[331.61672344]] accuracy 0.7837465564738292\n","Iteration 18111 - loss value [[330.22473045]] accuracy 0.778236914600551\n","Iteration 18112 - loss value [[337.1466301]] accuracy 0.7851239669421488\n","Iteration 18113 - loss value [[358.34649555]] accuracy 0.7493112947658402\n","Iteration 18114 - loss value [[325.76756685]] accuracy 0.7837465564738292\n","Iteration 18115 - loss value [[327.22479096]] accuracy 0.778236914600551\n","Iteration 18116 - loss value [[327.53717906]] accuracy 0.7851239669421488\n","Iteration 18117 - loss value [[326.52824068]] accuracy 0.778236914600551\n","Iteration 18118 - loss value [[329.51240157]] accuracy 0.7837465564738292\n","Iteration 18119 - loss value [[327.63905347]] accuracy 0.7823691460055097\n","Iteration 18120 - loss value [[326.57339289]] accuracy 0.7851239669421488\n","Iteration 18121 - loss value [[325.84879691]] accuracy 0.7823691460055097\n","Iteration 18122 - loss value [[325.59710555]] accuracy 0.7851239669421488\n","Iteration 18123 - loss value [[326.56419496]] accuracy 0.7796143250688705\n","Iteration 18124 - loss value [[328.84413239]] accuracy 0.7768595041322314\n","Iteration 18125 - loss value [[326.57882971]] accuracy 0.7837465564738292\n","Iteration 18126 - loss value [[326.62023312]] accuracy 0.78099173553719\n","Iteration 18127 - loss value [[330.9539399]] accuracy 0.7823691460055097\n","Iteration 18128 - loss value [[331.80066873]] accuracy 0.7768595041322314\n","Iteration 18129 - loss value [[332.70640741]] accuracy 0.7823691460055097\n","Iteration 18130 - loss value [[339.73123476]] accuracy 0.7713498622589532\n","Iteration 18131 - loss value [[339.35388888]] accuracy 0.7768595041322314\n","Iteration 18132 - loss value [[355.97102255]] accuracy 0.7506887052341598\n","Iteration 18133 - loss value [[327.08402792]] accuracy 0.778236914600551\n","Iteration 18134 - loss value [[326.3206557]] accuracy 0.7865013774104683\n","Iteration 18135 - loss value [[327.13384831]] accuracy 0.778236914600551\n","Iteration 18136 - loss value [[334.27118699]] accuracy 0.7796143250688705\n","Iteration 18137 - loss value [[335.76840976]] accuracy 0.7727272727272727\n","Iteration 18138 - loss value [[330.24836926]] accuracy 0.7851239669421488\n","Iteration 18139 - loss value [[327.8634382]] accuracy 0.7796143250688705\n","Iteration 18140 - loss value [[327.07029153]] accuracy 0.7865013774104683\n","Iteration 18141 - loss value [[326.94021246]] accuracy 0.778236914600551\n","Iteration 18142 - loss value [[330.67745254]] accuracy 0.78099173553719\n","Iteration 18143 - loss value [[327.70894599]] accuracy 0.778236914600551\n","Iteration 18144 - loss value [[327.19736774]] accuracy 0.7892561983471075\n","Iteration 18145 - loss value [[327.3455821]] accuracy 0.7768595041322314\n","Iteration 18146 - loss value [[332.41151948]] accuracy 0.7837465564738292\n","Iteration 18147 - loss value [[330.35002546]] accuracy 0.7796143250688705\n","Iteration 18148 - loss value [[332.6682446]] accuracy 0.78099173553719\n","Iteration 18149 - loss value [[340.65441993]] accuracy 0.7699724517906336\n","Iteration 18150 - loss value [[330.74304212]] accuracy 0.778236914600551\n","Iteration 18151 - loss value [[329.24128891]] accuracy 0.7768595041322314\n","Iteration 18152 - loss value [[328.83001126]] accuracy 0.7837465564738292\n","Iteration 18153 - loss value [[327.30885372]] accuracy 0.7796143250688705\n","Iteration 18154 - loss value [[326.82334792]] accuracy 0.7865013774104683\n","Iteration 18155 - loss value [[325.9228317]] accuracy 0.78099173553719\n","Iteration 18156 - loss value [[328.4731932]] accuracy 0.7851239669421488\n","Iteration 18157 - loss value [[330.41387693]] accuracy 0.7768595041322314\n","Iteration 18158 - loss value [[329.60055464]] accuracy 0.7851239669421488\n","Iteration 18159 - loss value [[331.89193788]] accuracy 0.7768595041322314\n","Iteration 18160 - loss value [[326.78306607]] accuracy 0.7837465564738292\n","Iteration 18161 - loss value [[325.67217167]] accuracy 0.7823691460055097\n","Iteration 18162 - loss value [[325.0827148]] accuracy 0.7851239669421488\n","Iteration 18163 - loss value [[326.10956736]] accuracy 0.7796143250688705\n","Iteration 18164 - loss value [[328.39546554]] accuracy 0.7823691460055097\n","Iteration 18165 - loss value [[329.37983109]] accuracy 0.7768595041322314\n","Iteration 18166 - loss value [[330.09371451]] accuracy 0.78099173553719\n","Iteration 18167 - loss value [[326.77303947]] accuracy 0.7796143250688705\n","Iteration 18168 - loss value [[326.09881371]] accuracy 0.7823691460055097\n","Iteration 18169 - loss value [[325.82102375]] accuracy 0.7823691460055097\n","Iteration 18170 - loss value [[326.074991]] accuracy 0.7851239669421488\n","Iteration 18171 - loss value [[325.85927825]] accuracy 0.7823691460055097\n","Iteration 18172 - loss value [[325.91585173]] accuracy 0.7865013774104683\n","Iteration 18173 - loss value [[329.26729238]] accuracy 0.7768595041322314\n","Iteration 18174 - loss value [[330.74123681]] accuracy 0.78099173553719\n","Iteration 18175 - loss value [[334.14222245]] accuracy 0.7727272727272727\n","Iteration 18176 - loss value [[328.47955367]] accuracy 0.78099173553719\n","Iteration 18177 - loss value [[329.00467125]] accuracy 0.7727272727272727\n","Iteration 18178 - loss value [[336.34401036]] accuracy 0.7865013774104683\n","Iteration 18179 - loss value [[354.5781558]] accuracy 0.7534435261707989\n","Iteration 18180 - loss value [[325.52519054]] accuracy 0.7823691460055097\n","Iteration 18181 - loss value [[326.71428148]] accuracy 0.7796143250688705\n","Iteration 18182 - loss value [[331.00489598]] accuracy 0.7823691460055097\n","Iteration 18183 - loss value [[329.22099301]] accuracy 0.778236914600551\n","Iteration 18184 - loss value [[326.95674908]] accuracy 0.7865013774104683\n","Iteration 18185 - loss value [[326.38352149]] accuracy 0.7823691460055097\n","Iteration 18186 - loss value [[328.6768975]] accuracy 0.7837465564738292\n","Iteration 18187 - loss value [[327.4126373]] accuracy 0.7796143250688705\n","Iteration 18188 - loss value [[334.14334357]] accuracy 0.778236914600551\n","Iteration 18189 - loss value [[335.24692096]] accuracy 0.7713498622589532\n","Iteration 18190 - loss value [[342.6085999]] accuracy 0.7699724517906336\n","Iteration 18191 - loss value [[378.87507935]] accuracy 0.7286501377410468\n","Iteration 18192 - loss value [[329.64982278]] accuracy 0.7823691460055097\n","Iteration 18193 - loss value [[327.47460035]] accuracy 0.78099173553719\n","Iteration 18194 - loss value [[325.57567215]] accuracy 0.7837465564738292\n","Iteration 18195 - loss value [[325.18443332]] accuracy 0.7837465564738292\n","Iteration 18196 - loss value [[325.86368806]] accuracy 0.78099173553719\n","Iteration 18197 - loss value [[328.72105603]] accuracy 0.7837465564738292\n","Iteration 18198 - loss value [[331.51676227]] accuracy 0.7727272727272727\n","Iteration 18199 - loss value [[330.54641961]] accuracy 0.78099173553719\n","Iteration 18200 - loss value [[335.94880473]] accuracy 0.7727272727272727\n","Iteration 18201 - loss value [[327.65888803]] accuracy 0.7823691460055097\n","Iteration 18202 - loss value [[327.85045906]] accuracy 0.7768595041322314\n","Iteration 18203 - loss value [[337.14449269]] accuracy 0.7796143250688705\n","Iteration 18204 - loss value [[356.22731275]] accuracy 0.7506887052341598\n","Iteration 18205 - loss value [[324.64500333]] accuracy 0.7851239669421488\n","Iteration 18206 - loss value [[324.90501442]] accuracy 0.7865013774104683\n","Iteration 18207 - loss value [[325.93781705]] accuracy 0.7865013774104683\n","Iteration 18208 - loss value [[325.29229562]] accuracy 0.7837465564738292\n","Iteration 18209 - loss value [[327.07417495]] accuracy 0.7851239669421488\n","Iteration 18210 - loss value [[326.22823721]] accuracy 0.7823691460055097\n","Iteration 18211 - loss value [[327.51051094]] accuracy 0.7823691460055097\n","Iteration 18212 - loss value [[326.69199324]] accuracy 0.7823691460055097\n","Iteration 18213 - loss value [[324.80493664]] accuracy 0.7851239669421488\n","Iteration 18214 - loss value [[325.78087578]] accuracy 0.7823691460055097\n","Iteration 18215 - loss value [[328.59788247]] accuracy 0.7851239669421488\n","Iteration 18216 - loss value [[330.17124113]] accuracy 0.778236914600551\n","Iteration 18217 - loss value [[330.54637004]] accuracy 0.7796143250688705\n","Iteration 18218 - loss value [[327.56230126]] accuracy 0.7796143250688705\n","Iteration 18219 - loss value [[326.32452793]] accuracy 0.7865013774104683\n","Iteration 18220 - loss value [[325.2207298]] accuracy 0.78099173553719\n","Iteration 18221 - loss value [[324.55908002]] accuracy 0.7851239669421488\n","Iteration 18222 - loss value [[324.68302588]] accuracy 0.7878787878787878\n","Iteration 18223 - loss value [[325.17216057]] accuracy 0.7837465564738292\n","Iteration 18224 - loss value [[326.02359187]] accuracy 0.7851239669421488\n","Iteration 18225 - loss value [[327.20290358]] accuracy 0.7796143250688705\n","Iteration 18226 - loss value [[332.77006668]] accuracy 0.7837465564738292\n","Iteration 18227 - loss value [[335.39854705]] accuracy 0.7768595041322314\n","Iteration 18228 - loss value [[332.40882946]] accuracy 0.7865013774104683\n","Iteration 18229 - loss value [[336.64992581]] accuracy 0.7713498622589532\n","Iteration 18230 - loss value [[341.96909938]] accuracy 0.7741046831955923\n","Iteration 18231 - loss value [[367.53680656]] accuracy 0.743801652892562\n","Iteration 18232 - loss value [[325.42302903]] accuracy 0.7823691460055097\n","Iteration 18233 - loss value [[324.50476673]] accuracy 0.7865013774104683\n","Iteration 18234 - loss value [[324.51030955]] accuracy 0.7851239669421488\n","Iteration 18235 - loss value [[324.61321237]] accuracy 0.7865013774104683\n","Iteration 18236 - loss value [[325.27117268]] accuracy 0.7865013774104683\n","Iteration 18237 - loss value [[327.94703843]] accuracy 0.7768595041322314\n","Iteration 18238 - loss value [[330.36414491]] accuracy 0.7851239669421488\n","Iteration 18239 - loss value [[334.06455125]] accuracy 0.7727272727272727\n","Iteration 18240 - loss value [[327.83292095]] accuracy 0.7823691460055097\n","Iteration 18241 - loss value [[327.92502253]] accuracy 0.7768595041322314\n","Iteration 18242 - loss value [[332.97487807]] accuracy 0.7796143250688705\n","Iteration 18243 - loss value [[331.13841045]] accuracy 0.778236914600551\n","Iteration 18244 - loss value [[334.140574]] accuracy 0.7851239669421488\n","Iteration 18245 - loss value [[349.5635051]] accuracy 0.7575757575757576\n","Iteration 18246 - loss value [[335.45482736]] accuracy 0.7823691460055097\n","Iteration 18247 - loss value [[351.53035716]] accuracy 0.756198347107438\n","Iteration 18248 - loss value [[330.70334813]] accuracy 0.7796143250688705\n","Iteration 18249 - loss value [[331.31802054]] accuracy 0.7768595041322314\n","Iteration 18250 - loss value [[333.05592743]] accuracy 0.7837465564738292\n","Iteration 18251 - loss value [[335.15756187]] accuracy 0.7754820936639119\n","Iteration 18252 - loss value [[332.73210478]] accuracy 0.7837465564738292\n","Iteration 18253 - loss value [[338.61673219]] accuracy 0.7713498622589532\n","Iteration 18254 - loss value [[330.0926917]] accuracy 0.7823691460055097\n","Iteration 18255 - loss value [[328.6902407]] accuracy 0.7796143250688705\n","Iteration 18256 - loss value [[327.23558018]] accuracy 0.7851239669421488\n","Iteration 18257 - loss value [[326.06937441]] accuracy 0.7796143250688705\n","Iteration 18258 - loss value [[325.38847331]] accuracy 0.7823691460055097\n","Iteration 18259 - loss value [[324.84574858]] accuracy 0.7851239669421488\n","Iteration 18260 - loss value [[325.45623474]] accuracy 0.7837465564738292\n","Iteration 18261 - loss value [[325.11095435]] accuracy 0.7837465564738292\n","Iteration 18262 - loss value [[326.31795437]] accuracy 0.7851239669421488\n","Iteration 18263 - loss value [[327.22835975]] accuracy 0.7796143250688705\n","Iteration 18264 - loss value [[329.19985627]] accuracy 0.7823691460055097\n","Iteration 18265 - loss value [[332.31138441]] accuracy 0.7754820936639119\n","Iteration 18266 - loss value [[329.64615342]] accuracy 0.7837465564738292\n","Iteration 18267 - loss value [[332.08852293]] accuracy 0.7741046831955923\n","Iteration 18268 - loss value [[327.5671343]] accuracy 0.7823691460055097\n","Iteration 18269 - loss value [[326.49898951]] accuracy 0.78099173553719\n","Iteration 18270 - loss value [[327.91174958]] accuracy 0.7796143250688705\n","Iteration 18271 - loss value [[327.15661795]] accuracy 0.7823691460055097\n","Iteration 18272 - loss value [[325.00723411]] accuracy 0.7865013774104683\n","Iteration 18273 - loss value [[326.31320533]] accuracy 0.7837465564738292\n","Iteration 18274 - loss value [[325.61129214]] accuracy 0.7823691460055097\n","Iteration 18275 - loss value [[326.20433096]] accuracy 0.7851239669421488\n","Iteration 18276 - loss value [[325.41118623]] accuracy 0.7837465564738292\n","Iteration 18277 - loss value [[325.69574614]] accuracy 0.7823691460055097\n","Iteration 18278 - loss value [[325.30006615]] accuracy 0.7878787878787878\n","Iteration 18279 - loss value [[325.65030817]] accuracy 0.7837465564738292\n","Iteration 18280 - loss value [[325.59288324]] accuracy 0.7878787878787878\n","Iteration 18281 - loss value [[327.49283816]] accuracy 0.778236914600551\n","Iteration 18282 - loss value [[330.57687053]] accuracy 0.7823691460055097\n","Iteration 18283 - loss value [[328.07782579]] accuracy 0.778236914600551\n","Iteration 18284 - loss value [[329.315776]] accuracy 0.7851239669421488\n","Iteration 18285 - loss value [[326.71268401]] accuracy 0.7768595041322314\n","Iteration 18286 - loss value [[325.47809256]] accuracy 0.7837465564738292\n","Iteration 18287 - loss value [[324.90507539]] accuracy 0.7837465564738292\n","Iteration 18288 - loss value [[325.23702968]] accuracy 0.7865013774104683\n","Iteration 18289 - loss value [[326.90840955]] accuracy 0.78099173553719\n","Iteration 18290 - loss value [[332.4290756]] accuracy 0.7837465564738292\n","Iteration 18291 - loss value [[336.64817759]] accuracy 0.7727272727272727\n","Iteration 18292 - loss value [[333.07577829]] accuracy 0.7851239669421488\n","Iteration 18293 - loss value [[338.50169127]] accuracy 0.7741046831955923\n","Iteration 18294 - loss value [[336.88787164]] accuracy 0.78099173553719\n","Iteration 18295 - loss value [[351.92863537]] accuracy 0.756198347107438\n","Iteration 18296 - loss value [[325.00310084]] accuracy 0.7837465564738292\n","Iteration 18297 - loss value [[324.72207499]] accuracy 0.7851239669421488\n","Iteration 18298 - loss value [[324.58579662]] accuracy 0.7851239669421488\n","Iteration 18299 - loss value [[324.65340039]] accuracy 0.7865013774104683\n","Iteration 18300 - loss value [[325.62261938]] accuracy 0.7837465564738292\n","Iteration 18301 - loss value [[328.50858616]] accuracy 0.7837465564738292\n","Iteration 18302 - loss value [[330.80195013]] accuracy 0.7754820936639119\n","Iteration 18303 - loss value [[329.40327348]] accuracy 0.7837465564738292\n","Iteration 18304 - loss value [[330.25752576]] accuracy 0.7768595041322314\n","Iteration 18305 - loss value [[332.18352993]] accuracy 0.7823691460055097\n","Iteration 18306 - loss value [[331.18071068]] accuracy 0.7768595041322314\n","Iteration 18307 - loss value [[336.17877228]] accuracy 0.7851239669421488\n","Iteration 18308 - loss value [[353.34144839]] accuracy 0.7534435261707989\n","Iteration 18309 - loss value [[326.92070108]] accuracy 0.7837465564738292\n","Iteration 18310 - loss value [[325.79499437]] accuracy 0.7837465564738292\n","Iteration 18311 - loss value [[326.89771718]] accuracy 0.7837465564738292\n","Iteration 18312 - loss value [[327.93300951]] accuracy 0.7768595041322314\n","Iteration 18313 - loss value [[331.59561809]] accuracy 0.78099173553719\n","Iteration 18314 - loss value [[331.70266967]] accuracy 0.7768595041322314\n","Iteration 18315 - loss value [[337.16775415]] accuracy 0.7837465564738292\n","Iteration 18316 - loss value [[355.48237708]] accuracy 0.7520661157024794\n","Iteration 18317 - loss value [[325.1429805]] accuracy 0.7823691460055097\n","Iteration 18318 - loss value [[326.17196753]] accuracy 0.7796143250688705\n","Iteration 18319 - loss value [[328.21363]] accuracy 0.7865013774104683\n","Iteration 18320 - loss value [[329.92539913]] accuracy 0.7768595041322314\n","Iteration 18321 - loss value [[328.8634449]] accuracy 0.7837465564738292\n","Iteration 18322 - loss value [[331.25173047]] accuracy 0.7754820936639119\n","Iteration 18323 - loss value [[330.88182251]] accuracy 0.78099173553719\n","Iteration 18324 - loss value [[326.81249903]] accuracy 0.778236914600551\n","Iteration 18325 - loss value [[326.34096466]] accuracy 0.7851239669421488\n","Iteration 18326 - loss value [[326.11223312]] accuracy 0.7837465564738292\n","Iteration 18327 - loss value [[326.38051204]] accuracy 0.7837465564738292\n","Iteration 18328 - loss value [[325.85846448]] accuracy 0.7823691460055097\n","Iteration 18329 - loss value [[325.47846027]] accuracy 0.7823691460055097\n","Iteration 18330 - loss value [[324.92608733]] accuracy 0.7851239669421488\n","Iteration 18331 - loss value [[324.98462913]] accuracy 0.7865013774104683\n","Iteration 18332 - loss value [[326.29104029]] accuracy 0.78099173553719\n","Iteration 18333 - loss value [[330.33965654]] accuracy 0.7851239669421488\n","Iteration 18334 - loss value [[331.46252994]] accuracy 0.7754820936639119\n","Iteration 18335 - loss value [[327.24563961]] accuracy 0.7851239669421488\n","Iteration 18336 - loss value [[326.97394829]] accuracy 0.78099173553719\n","Iteration 18337 - loss value [[328.56246502]] accuracy 0.7823691460055097\n","Iteration 18338 - loss value [[327.21697016]] accuracy 0.78099173553719\n","Iteration 18339 - loss value [[326.43183473]] accuracy 0.7851239669421488\n","Iteration 18340 - loss value [[325.34260315]] accuracy 0.78099173553719\n","Iteration 18341 - loss value [[324.81932469]] accuracy 0.7851239669421488\n","Iteration 18342 - loss value [[326.37302514]] accuracy 0.778236914600551\n","Iteration 18343 - loss value [[330.92024384]] accuracy 0.7851239669421488\n","Iteration 18344 - loss value [[329.57201035]] accuracy 0.778236914600551\n","Iteration 18345 - loss value [[335.88954787]] accuracy 0.7851239669421488\n","Iteration 18346 - loss value [[354.1350806]] accuracy 0.7534435261707989\n","Iteration 18347 - loss value [[324.76097139]] accuracy 0.7865013774104683\n","Iteration 18348 - loss value [[324.70808141]] accuracy 0.7865013774104683\n","Iteration 18349 - loss value [[325.35974379]] accuracy 0.7865013774104683\n","Iteration 18350 - loss value [[327.52008955]] accuracy 0.778236914600551\n","Iteration 18351 - loss value [[333.08813757]] accuracy 0.7837465564738292\n","Iteration 18352 - loss value [[334.24974233]] accuracy 0.7741046831955923\n","Iteration 18353 - loss value [[329.34356935]] accuracy 0.78099173553719\n","Iteration 18354 - loss value [[326.63549546]] accuracy 0.7768595041322314\n","Iteration 18355 - loss value [[325.23529957]] accuracy 0.7837465564738292\n","Iteration 18356 - loss value [[324.85169187]] accuracy 0.7837465564738292\n","Iteration 18357 - loss value [[325.11732947]] accuracy 0.7823691460055097\n","Iteration 18358 - loss value [[324.93754683]] accuracy 0.7851239669421488\n","Iteration 18359 - loss value [[325.87944284]] accuracy 0.7823691460055097\n","Iteration 18360 - loss value [[328.52284312]] accuracy 0.7823691460055097\n","Iteration 18361 - loss value [[330.19902096]] accuracy 0.7754820936639119\n","Iteration 18362 - loss value [[327.70029983]] accuracy 0.7837465564738292\n","Iteration 18363 - loss value [[328.21379829]] accuracy 0.7768595041322314\n","Iteration 18364 - loss value [[332.45502564]] accuracy 0.78099173553719\n","Iteration 18365 - loss value [[329.08427408]] accuracy 0.78099173553719\n","Iteration 18366 - loss value [[328.24510608]] accuracy 0.7851239669421488\n","Iteration 18367 - loss value [[326.76850504]] accuracy 0.7837465564738292\n","Iteration 18368 - loss value [[326.65113282]] accuracy 0.7837465564738292\n","Iteration 18369 - loss value [[325.94699283]] accuracy 0.7837465564738292\n","Iteration 18370 - loss value [[326.28030019]] accuracy 0.7837465564738292\n","Iteration 18371 - loss value [[325.84691691]] accuracy 0.7837465564738292\n","Iteration 18372 - loss value [[325.45922371]] accuracy 0.7823691460055097\n","Iteration 18373 - loss value [[324.88790478]] accuracy 0.7851239669421488\n","Iteration 18374 - loss value [[324.8983211]] accuracy 0.7865013774104683\n","Iteration 18375 - loss value [[326.09043333]] accuracy 0.78099173553719\n","Iteration 18376 - loss value [[329.72121971]] accuracy 0.7851239669421488\n","Iteration 18377 - loss value [[332.7232138]] accuracy 0.7741046831955923\n","Iteration 18378 - loss value [[328.90096782]] accuracy 0.7837465564738292\n","Iteration 18379 - loss value [[327.98341818]] accuracy 0.7768595041322314\n","Iteration 18380 - loss value [[328.73714745]] accuracy 0.7837465564738292\n","Iteration 18381 - loss value [[331.87294204]] accuracy 0.7741046831955923\n","Iteration 18382 - loss value [[327.06981701]] accuracy 0.7851239669421488\n","Iteration 18383 - loss value [[327.46224364]] accuracy 0.7768595041322314\n","Iteration 18384 - loss value [[331.00834128]] accuracy 0.7796143250688705\n","Iteration 18385 - loss value [[327.28773497]] accuracy 0.78099173553719\n","Iteration 18386 - loss value [[325.86025532]] accuracy 0.7837465564738292\n","Iteration 18387 - loss value [[325.45903878]] accuracy 0.7823691460055097\n","Iteration 18388 - loss value [[326.07567395]] accuracy 0.7851239669421488\n","Iteration 18389 - loss value [[325.88649359]] accuracy 0.7823691460055097\n","Iteration 18390 - loss value [[324.66710099]] accuracy 0.7851239669421488\n","Iteration 18391 - loss value [[325.30427991]] accuracy 0.7823691460055097\n","Iteration 18392 - loss value [[327.14028685]] accuracy 0.7851239669421488\n","Iteration 18393 - loss value [[327.60599239]] accuracy 0.778236914600551\n","Iteration 18394 - loss value [[331.20941738]] accuracy 0.78099173553719\n","Iteration 18395 - loss value [[329.88545466]] accuracy 0.778236914600551\n","Iteration 18396 - loss value [[329.57664538]] accuracy 0.7837465564738292\n","Iteration 18397 - loss value [[327.88772229]] accuracy 0.78099173553719\n","Iteration 18398 - loss value [[328.18129074]] accuracy 0.7837465564738292\n","Iteration 18399 - loss value [[326.07194095]] accuracy 0.7796143250688705\n","Iteration 18400 - loss value [[325.30312305]] accuracy 0.7837465564738292\n","Iteration 18401 - loss value [[325.11358367]] accuracy 0.7851239669421488\n","Iteration 18402 - loss value [[326.62889107]] accuracy 0.7837465564738292\n","Iteration 18403 - loss value [[327.98108325]] accuracy 0.7754820936639119\n","Iteration 18404 - loss value [[331.62398929]] accuracy 0.78099173553719\n","Iteration 18405 - loss value [[327.97054023]] accuracy 0.78099173553719\n","Iteration 18406 - loss value [[328.38813332]] accuracy 0.7837465564738292\n","Iteration 18407 - loss value [[325.68786106]] accuracy 0.78099173553719\n","Iteration 18408 - loss value [[325.20742913]] accuracy 0.78099173553719\n","Iteration 18409 - loss value [[324.85647247]] accuracy 0.7851239669421488\n","Iteration 18410 - loss value [[325.88789586]] accuracy 0.7823691460055097\n","Iteration 18411 - loss value [[326.236798]] accuracy 0.78099173553719\n","Iteration 18412 - loss value [[330.05086304]] accuracy 0.7851239669421488\n","Iteration 18413 - loss value [[328.10837314]] accuracy 0.7823691460055097\n","Iteration 18414 - loss value [[328.33347344]] accuracy 0.7851239669421488\n","Iteration 18415 - loss value [[326.1309313]] accuracy 0.7823691460055097\n","Iteration 18416 - loss value [[326.37885847]] accuracy 0.7851239669421488\n","Iteration 18417 - loss value [[325.74385136]] accuracy 0.78099173553719\n","Iteration 18418 - loss value [[325.5895494]] accuracy 0.7865013774104683\n","Iteration 18419 - loss value [[325.46462159]] accuracy 0.7823691460055097\n","Iteration 18420 - loss value [[327.02581016]] accuracy 0.7837465564738292\n","Iteration 18421 - loss value [[325.42580057]] accuracy 0.7823691460055097\n","Iteration 18422 - loss value [[326.24756089]] accuracy 0.7865013774104683\n","Iteration 18423 - loss value [[327.50622394]] accuracy 0.778236914600551\n","Iteration 18424 - loss value [[332.24555529]] accuracy 0.7796143250688705\n","Iteration 18425 - loss value [[330.66507481]] accuracy 0.7768595041322314\n","Iteration 18426 - loss value [[338.83487167]] accuracy 0.78099173553719\n","Iteration 18427 - loss value [[359.77216342]] accuracy 0.7465564738292011\n","Iteration 18428 - loss value [[325.87318225]] accuracy 0.7837465564738292\n","Iteration 18429 - loss value [[328.6668418]] accuracy 0.778236914600551\n","Iteration 18430 - loss value [[326.92315931]] accuracy 0.7865013774104683\n","Iteration 18431 - loss value [[326.93536758]] accuracy 0.778236914600551\n","Iteration 18432 - loss value [[331.24789586]] accuracy 0.7823691460055097\n","Iteration 18433 - loss value [[328.4650734]] accuracy 0.7796143250688705\n","Iteration 18434 - loss value [[328.3766074]] accuracy 0.7851239669421488\n","Iteration 18435 - loss value [[326.83864565]] accuracy 0.7796143250688705\n","Iteration 18436 - loss value [[326.22335769]] accuracy 0.7796143250688705\n","Iteration 18437 - loss value [[325.56922286]] accuracy 0.7823691460055097\n","Iteration 18438 - loss value [[325.26812342]] accuracy 0.7851239669421488\n","Iteration 18439 - loss value [[325.8640323]] accuracy 0.7837465564738292\n","Iteration 18440 - loss value [[324.61105919]] accuracy 0.7851239669421488\n","Iteration 18441 - loss value [[324.76672485]] accuracy 0.7865013774104683\n","Iteration 18442 - loss value [[325.751198]] accuracy 0.7837465564738292\n","Iteration 18443 - loss value [[325.89719158]] accuracy 0.78099173553719\n","Iteration 18444 - loss value [[328.84639513]] accuracy 0.7837465564738292\n","Iteration 18445 - loss value [[329.75349594]] accuracy 0.778236914600551\n","Iteration 18446 - loss value [[332.29465198]] accuracy 0.7823691460055097\n","Iteration 18447 - loss value [[329.38607031]] accuracy 0.78099173553719\n","Iteration 18448 - loss value [[330.13169172]] accuracy 0.7851239669421488\n","Iteration 18449 - loss value [[328.75098532]] accuracy 0.778236914600551\n","Iteration 18450 - loss value [[329.49630395]] accuracy 0.7865013774104683\n","Iteration 18451 - loss value [[327.25313827]] accuracy 0.7768595041322314\n","Iteration 18452 - loss value [[325.93481715]] accuracy 0.7851239669421488\n","Iteration 18453 - loss value [[325.54533308]] accuracy 0.7823691460055097\n","Iteration 18454 - loss value [[325.20964828]] accuracy 0.7851239669421488\n","Iteration 18455 - loss value [[325.58839707]] accuracy 0.7823691460055097\n","Iteration 18456 - loss value [[327.83710794]] accuracy 0.7837465564738292\n","Iteration 18457 - loss value [[329.85550671]] accuracy 0.7741046831955923\n","Iteration 18458 - loss value [[333.40586566]] accuracy 0.778236914600551\n","Iteration 18459 - loss value [[337.0045548]] accuracy 0.7727272727272727\n","Iteration 18460 - loss value [[337.37846533]] accuracy 0.7768595041322314\n","Iteration 18461 - loss value [[357.33122747]] accuracy 0.7506887052341598\n","Iteration 18462 - loss value [[324.81025823]] accuracy 0.7851239669421488\n","Iteration 18463 - loss value [[325.1986543]] accuracy 0.7837465564738292\n","Iteration 18464 - loss value [[325.53720583]] accuracy 0.7865013774104683\n","Iteration 18465 - loss value [[328.35592163]] accuracy 0.778236914600551\n","Iteration 18466 - loss value [[328.87744599]] accuracy 0.7851239669421488\n","Iteration 18467 - loss value [[329.02665655]] accuracy 0.778236914600551\n","Iteration 18468 - loss value [[332.67453735]] accuracy 0.7837465564738292\n","Iteration 18469 - loss value [[336.16316677]] accuracy 0.7754820936639119\n","Iteration 18470 - loss value [[334.00344689]] accuracy 0.7837465564738292\n","Iteration 18471 - loss value [[343.3740238]] accuracy 0.7672176308539945\n","Iteration 18472 - loss value [[339.26763219]] accuracy 0.7823691460055097\n","Iteration 18473 - loss value [[357.12908363]] accuracy 0.7506887052341598\n","Iteration 18474 - loss value [[326.32468202]] accuracy 0.7837465564738292\n","Iteration 18475 - loss value [[325.27227214]] accuracy 0.7837465564738292\n","Iteration 18476 - loss value [[326.28928505]] accuracy 0.7823691460055097\n","Iteration 18477 - loss value [[329.20081115]] accuracy 0.7865013774104683\n","Iteration 18478 - loss value [[330.45340798]] accuracy 0.7741046831955923\n","Iteration 18479 - loss value [[330.76088631]] accuracy 0.7823691460055097\n","Iteration 18480 - loss value [[333.84944598]] accuracy 0.7727272727272727\n","Iteration 18481 - loss value [[328.61318563]] accuracy 0.7837465564738292\n","Iteration 18482 - loss value [[328.73286731]] accuracy 0.7754820936639119\n","Iteration 18483 - loss value [[332.43842792]] accuracy 0.7837465564738292\n","Iteration 18484 - loss value [[339.4889361]] accuracy 0.768595041322314\n","Iteration 18485 - loss value [[343.57230984]] accuracy 0.7754820936639119\n","Iteration 18486 - loss value [[370.89177992]] accuracy 0.7355371900826446\n","Iteration 18487 - loss value [[327.27042403]] accuracy 0.778236914600551\n","Iteration 18488 - loss value [[329.24176457]] accuracy 0.7754820936639119\n","Iteration 18489 - loss value [[327.15302438]] accuracy 0.7851239669421488\n","Iteration 18490 - loss value [[326.60033613]] accuracy 0.778236914600551\n","Iteration 18491 - loss value [[331.54046137]] accuracy 0.7851239669421488\n","Iteration 18492 - loss value [[334.20031222]] accuracy 0.7768595041322314\n","Iteration 18493 - loss value [[333.29608469]] accuracy 0.7837465564738292\n","Iteration 18494 - loss value [[337.06766375]] accuracy 0.7727272727272727\n","Iteration 18495 - loss value [[330.11767895]] accuracy 0.7796143250688705\n","Iteration 18496 - loss value [[328.50107243]] accuracy 0.778236914600551\n","Iteration 18497 - loss value [[328.09970763]] accuracy 0.7837465564738292\n","Iteration 18498 - loss value [[326.22371978]] accuracy 0.78099173553719\n","Iteration 18499 - loss value [[325.75092952]] accuracy 0.78099173553719\n","Iteration 18500 - loss value [[325.29927473]] accuracy 0.7837465564738292\n","Iteration 18501 - loss value [[325.12688983]] accuracy 0.7837465564738292\n","Iteration 18502 - loss value [[325.29180684]] accuracy 0.7823691460055097\n","Iteration 18503 - loss value [[326.48380808]] accuracy 0.7837465564738292\n","Iteration 18504 - loss value [[325.69657773]] accuracy 0.7823691460055097\n","Iteration 18505 - loss value [[325.67087239]] accuracy 0.7837465564738292\n","Iteration 18506 - loss value [[325.58975946]] accuracy 0.7823691460055097\n","Iteration 18507 - loss value [[325.17481623]] accuracy 0.7823691460055097\n","Iteration 18508 - loss value [[325.70246239]] accuracy 0.7851239669421488\n","Iteration 18509 - loss value [[326.67028864]] accuracy 0.7823691460055097\n","Iteration 18510 - loss value [[325.47064589]] accuracy 0.7837465564738292\n","Iteration 18511 - loss value [[328.28231856]] accuracy 0.7768595041322314\n","Iteration 18512 - loss value [[328.79726222]] accuracy 0.7878787878787878\n","Iteration 18513 - loss value [[331.08303164]] accuracy 0.7741046831955923\n","Iteration 18514 - loss value [[329.15118143]] accuracy 0.7837465564738292\n","Iteration 18515 - loss value [[331.21246714]] accuracy 0.7741046831955923\n","Iteration 18516 - loss value [[328.43283827]] accuracy 0.78099173553719\n","Iteration 18517 - loss value [[328.91788921]] accuracy 0.7741046831955923\n","Iteration 18518 - loss value [[330.51110282]] accuracy 0.7823691460055097\n","Iteration 18519 - loss value [[334.34354359]] accuracy 0.7741046831955923\n","Iteration 18520 - loss value [[329.35724183]] accuracy 0.7823691460055097\n","Iteration 18521 - loss value [[331.91499805]] accuracy 0.7768595041322314\n","Iteration 18522 - loss value [[329.10814352]] accuracy 0.7823691460055097\n","Iteration 18523 - loss value [[330.49474354]] accuracy 0.7754820936639119\n","Iteration 18524 - loss value [[329.12028142]] accuracy 0.7796143250688705\n","Iteration 18525 - loss value [[330.14338196]] accuracy 0.7727272727272727\n","Iteration 18526 - loss value [[327.55769396]] accuracy 0.7865013774104683\n","Iteration 18527 - loss value [[327.77000549]] accuracy 0.7768595041322314\n","Iteration 18528 - loss value [[336.09288113]] accuracy 0.7796143250688705\n","Iteration 18529 - loss value [[355.08097658]] accuracy 0.7506887052341598\n","Iteration 18530 - loss value [[324.64357029]] accuracy 0.7865013774104683\n","Iteration 18531 - loss value [[324.82789171]] accuracy 0.7837465564738292\n","Iteration 18532 - loss value [[325.74820334]] accuracy 0.7837465564738292\n","Iteration 18533 - loss value [[326.04657882]] accuracy 0.7823691460055097\n","Iteration 18534 - loss value [[327.34399536]] accuracy 0.7851239669421488\n","Iteration 18535 - loss value [[326.15778996]] accuracy 0.7837465564738292\n","Iteration 18536 - loss value [[327.71146267]] accuracy 0.7823691460055097\n","Iteration 18537 - loss value [[326.37483651]] accuracy 0.7823691460055097\n","Iteration 18538 - loss value [[326.8459997]] accuracy 0.7837465564738292\n","Iteration 18539 - loss value [[325.7687217]] accuracy 0.7837465564738292\n","Iteration 18540 - loss value [[325.83497907]] accuracy 0.7823691460055097\n","Iteration 18541 - loss value [[326.65088266]] accuracy 0.78099173553719\n","Iteration 18542 - loss value [[325.95732039]] accuracy 0.78099173553719\n","Iteration 18543 - loss value [[325.01621174]] accuracy 0.7823691460055097\n","Iteration 18544 - loss value [[325.0412002]] accuracy 0.7837465564738292\n","Iteration 18545 - loss value [[325.46604717]] accuracy 0.7823691460055097\n","Iteration 18546 - loss value [[325.1220838]] accuracy 0.7837465564738292\n","Iteration 18547 - loss value [[326.67013478]] accuracy 0.7851239669421488\n","Iteration 18548 - loss value [[325.25810643]] accuracy 0.7823691460055097\n","Iteration 18549 - loss value [[324.6646528]] accuracy 0.7851239669421488\n","Iteration 18550 - loss value [[324.48066665]] accuracy 0.7851239669421488\n","Iteration 18551 - loss value [[324.47044645]] accuracy 0.7851239669421488\n","Iteration 18552 - loss value [[324.58815857]] accuracy 0.7865013774104683\n","Iteration 18553 - loss value [[325.47047838]] accuracy 0.7837465564738292\n","Iteration 18554 - loss value [[325.12651323]] accuracy 0.7837465564738292\n","Iteration 18555 - loss value [[326.59150703]] accuracy 0.7837465564738292\n","Iteration 18556 - loss value [[326.0722114]] accuracy 0.7823691460055097\n","Iteration 18557 - loss value [[326.85647173]] accuracy 0.7823691460055097\n","Iteration 18558 - loss value [[325.72003619]] accuracy 0.7837465564738292\n","Iteration 18559 - loss value [[324.91109966]] accuracy 0.7865013774104683\n","Iteration 18560 - loss value [[325.2285908]] accuracy 0.7823691460055097\n","Iteration 18561 - loss value [[324.67512444]] accuracy 0.7892561983471075\n","Iteration 18562 - loss value [[324.76956866]] accuracy 0.7851239669421488\n","Iteration 18563 - loss value [[325.33646656]] accuracy 0.7851239669421488\n","Iteration 18564 - loss value [[325.88092082]] accuracy 0.7823691460055097\n","Iteration 18565 - loss value [[325.16706307]] accuracy 0.7878787878787878\n","Iteration 18566 - loss value [[325.69269237]] accuracy 0.78099173553719\n","Iteration 18567 - loss value [[327.00117417]] accuracy 0.78099173553719\n","Iteration 18568 - loss value [[327.97960029]] accuracy 0.7823691460055097\n","Iteration 18569 - loss value [[328.74539471]] accuracy 0.7754820936639119\n","Iteration 18570 - loss value [[333.54959011]] accuracy 0.7823691460055097\n","Iteration 18571 - loss value [[343.80499105]] accuracy 0.7644628099173554\n","Iteration 18572 - loss value [[346.14933133]] accuracy 0.7713498622589532\n","Iteration 18573 - loss value [[383.31938132]] accuracy 0.7203856749311295\n","Iteration 18574 - loss value [[352.00041023]] accuracy 0.7644628099173554\n","Iteration 18575 - loss value [[380.95834821]] accuracy 0.7231404958677686\n","Iteration 18576 - loss value [[348.58985638]] accuracy 0.7644628099173554\n","Iteration 18577 - loss value [[372.67483887]] accuracy 0.7355371900826446\n","Iteration 18578 - loss value [[333.97924052]] accuracy 0.78099173553719\n","Iteration 18579 - loss value [[334.507605]] accuracy 0.7727272727272727\n","Iteration 18580 - loss value [[330.53428854]] accuracy 0.78099173553719\n","Iteration 18581 - loss value [[332.22691387]] accuracy 0.7741046831955923\n","Iteration 18582 - loss value [[329.07577287]] accuracy 0.7837465564738292\n","Iteration 18583 - loss value [[329.85303298]] accuracy 0.7713498622589532\n","Iteration 18584 - loss value [[330.94011816]] accuracy 0.78099173553719\n","Iteration 18585 - loss value [[334.55216914]] accuracy 0.7754820936639119\n","Iteration 18586 - loss value [[328.51072124]] accuracy 0.7823691460055097\n","Iteration 18587 - loss value [[329.03045817]] accuracy 0.7754820936639119\n","Iteration 18588 - loss value [[327.31503343]] accuracy 0.7823691460055097\n","Iteration 18589 - loss value [[325.559526]] accuracy 0.7837465564738292\n","Iteration 18590 - loss value [[327.09144288]] accuracy 0.7892561983471075\n","Iteration 18591 - loss value [[326.6609566]] accuracy 0.778236914600551\n","Iteration 18592 - loss value [[331.40010832]] accuracy 0.7851239669421488\n","Iteration 18593 - loss value [[332.6544597]] accuracy 0.7754820936639119\n","Iteration 18594 - loss value [[332.89378357]] accuracy 0.7837465564738292\n","Iteration 18595 - loss value [[336.57215474]] accuracy 0.7754820936639119\n","Iteration 18596 - loss value [[332.62149488]] accuracy 0.7851239669421488\n","Iteration 18597 - loss value [[338.84890634]] accuracy 0.7713498622589532\n","Iteration 18598 - loss value [[329.76668353]] accuracy 0.7823691460055097\n","Iteration 18599 - loss value [[332.57745599]] accuracy 0.7754820936639119\n","Iteration 18600 - loss value [[331.14536342]] accuracy 0.7823691460055097\n","Iteration 18601 - loss value [[333.07231595]] accuracy 0.7754820936639119\n","Iteration 18602 - loss value [[331.13180483]] accuracy 0.7823691460055097\n","Iteration 18603 - loss value [[336.00547625]] accuracy 0.7727272727272727\n","Iteration 18604 - loss value [[330.55226812]] accuracy 0.7823691460055097\n","Iteration 18605 - loss value [[328.93642907]] accuracy 0.778236914600551\n","Iteration 18606 - loss value [[329.54287861]] accuracy 0.7851239669421488\n","Iteration 18607 - loss value [[328.49211545]] accuracy 0.7796143250688705\n","Iteration 18608 - loss value [[328.4999051]] accuracy 0.7823691460055097\n","Iteration 18609 - loss value [[326.34233083]] accuracy 0.778236914600551\n","Iteration 18610 - loss value [[325.2000086]] accuracy 0.7837465564738292\n","Iteration 18611 - loss value [[325.3465259]] accuracy 0.7851239669421488\n","Iteration 18612 - loss value [[326.94067829]] accuracy 0.7823691460055097\n","Iteration 18613 - loss value [[325.90085541]] accuracy 0.7837465564738292\n","Iteration 18614 - loss value [[326.97480299]] accuracy 0.7837465564738292\n","Iteration 18615 - loss value [[327.79223916]] accuracy 0.778236914600551\n","Iteration 18616 - loss value [[330.0103007]] accuracy 0.7837465564738292\n","Iteration 18617 - loss value [[326.52167938]] accuracy 0.7796143250688705\n","Iteration 18618 - loss value [[325.53730855]] accuracy 0.7837465564738292\n","Iteration 18619 - loss value [[325.68606174]] accuracy 0.7837465564738292\n","Iteration 18620 - loss value [[327.91155379]] accuracy 0.7796143250688705\n","Iteration 18621 - loss value [[327.01836293]] accuracy 0.7823691460055097\n","Iteration 18622 - loss value [[328.26292569]] accuracy 0.7837465564738292\n","Iteration 18623 - loss value [[328.98279129]] accuracy 0.7768595041322314\n","Iteration 18624 - loss value [[332.03730237]] accuracy 0.78099173553719\n","Iteration 18625 - loss value [[329.93111274]] accuracy 0.778236914600551\n","Iteration 18626 - loss value [[330.9821975]] accuracy 0.7837465564738292\n","Iteration 18627 - loss value [[332.16365227]] accuracy 0.7768595041322314\n","Iteration 18628 - loss value [[341.78642462]] accuracy 0.7768595041322314\n","Iteration 18629 - loss value [[361.72663854]] accuracy 0.7451790633608816\n","Iteration 18630 - loss value [[325.74600739]] accuracy 0.7823691460055097\n","Iteration 18631 - loss value [[324.86528157]] accuracy 0.7865013774104683\n","Iteration 18632 - loss value [[326.18987483]] accuracy 0.7837465564738292\n","Iteration 18633 - loss value [[326.00109722]] accuracy 0.78099173553719\n","Iteration 18634 - loss value [[329.84564961]] accuracy 0.7851239669421488\n","Iteration 18635 - loss value [[326.72284564]] accuracy 0.7796143250688705\n","Iteration 18636 - loss value [[325.94858437]] accuracy 0.78099173553719\n","Iteration 18637 - loss value [[325.46144169]] accuracy 0.7837465564738292\n","Iteration 18638 - loss value [[327.30108552]] accuracy 0.7837465564738292\n","Iteration 18639 - loss value [[325.96900656]] accuracy 0.7837465564738292\n","Iteration 18640 - loss value [[326.36806269]] accuracy 0.7823691460055097\n","Iteration 18641 - loss value [[326.81757688]] accuracy 0.778236914600551\n","Iteration 18642 - loss value [[330.57265239]] accuracy 0.7796143250688705\n","Iteration 18643 - loss value [[327.28246016]] accuracy 0.7796143250688705\n","Iteration 18644 - loss value [[325.86484264]] accuracy 0.78099173553719\n","Iteration 18645 - loss value [[325.25440596]] accuracy 0.7823691460055097\n","Iteration 18646 - loss value [[325.88510215]] accuracy 0.7851239669421488\n","Iteration 18647 - loss value [[326.59726448]] accuracy 0.78099173553719\n","Iteration 18648 - loss value [[328.96256153]] accuracy 0.7823691460055097\n","Iteration 18649 - loss value [[332.05307412]] accuracy 0.7768595041322314\n","Iteration 18650 - loss value [[329.00602928]] accuracy 0.7837465564738292\n","Iteration 18651 - loss value [[330.4910954]] accuracy 0.7754820936639119\n","Iteration 18652 - loss value [[327.7280592]] accuracy 0.7851239669421488\n","Iteration 18653 - loss value [[327.60333346]] accuracy 0.7768595041322314\n","Iteration 18654 - loss value [[332.3544082]] accuracy 0.7823691460055097\n","Iteration 18655 - loss value [[328.94305242]] accuracy 0.78099173553719\n","Iteration 18656 - loss value [[327.23923366]] accuracy 0.7851239669421488\n","Iteration 18657 - loss value [[325.58986172]] accuracy 0.7796143250688705\n","Iteration 18658 - loss value [[324.85731446]] accuracy 0.7837465564738292\n","Iteration 18659 - loss value [[324.64931406]] accuracy 0.7837465564738292\n","Iteration 18660 - loss value [[324.45239429]] accuracy 0.7865013774104683\n","Iteration 18661 - loss value [[324.49966545]] accuracy 0.7865013774104683\n","Iteration 18662 - loss value [[324.70578835]] accuracy 0.7851239669421488\n","Iteration 18663 - loss value [[324.98484261]] accuracy 0.7837465564738292\n","Iteration 18664 - loss value [[326.34487529]] accuracy 0.7837465564738292\n","Iteration 18665 - loss value [[325.69114251]] accuracy 0.7823691460055097\n","Iteration 18666 - loss value [[325.51843921]] accuracy 0.7837465564738292\n","Iteration 18667 - loss value [[325.62585858]] accuracy 0.78099173553719\n","Iteration 18668 - loss value [[328.53978133]] accuracy 0.7796143250688705\n","Iteration 18669 - loss value [[330.09837937]] accuracy 0.7754820936639119\n","Iteration 18670 - loss value [[328.78280491]] accuracy 0.7796143250688705\n","Iteration 18671 - loss value [[330.56054786]] accuracy 0.7741046831955923\n","Iteration 18672 - loss value [[330.4028847]] accuracy 0.7837465564738292\n","Iteration 18673 - loss value [[330.4953959]] accuracy 0.7741046831955923\n","Iteration 18674 - loss value [[328.31880068]] accuracy 0.7865013774104683\n","Iteration 18675 - loss value [[329.36279485]] accuracy 0.7741046831955923\n","Iteration 18676 - loss value [[328.35752994]] accuracy 0.7851239669421488\n","Iteration 18677 - loss value [[325.61257909]] accuracy 0.7837465564738292\n","Iteration 18678 - loss value [[325.11022546]] accuracy 0.7823691460055097\n","Iteration 18679 - loss value [[324.5463497]] accuracy 0.7865013774104683\n","Iteration 18680 - loss value [[325.26890601]] accuracy 0.7837465564738292\n","Iteration 18681 - loss value [[328.18479671]] accuracy 0.7768595041322314\n","Iteration 18682 - loss value [[331.53070537]] accuracy 0.7796143250688705\n","Iteration 18683 - loss value [[327.39705478]] accuracy 0.78099173553719\n","Iteration 18684 - loss value [[326.34450889]] accuracy 0.7837465564738292\n","Iteration 18685 - loss value [[326.08750109]] accuracy 0.7837465564738292\n","Iteration 18686 - loss value [[325.99168321]] accuracy 0.7837465564738292\n","Iteration 18687 - loss value [[325.3299941]] accuracy 0.7823691460055097\n","Iteration 18688 - loss value [[326.32343188]] accuracy 0.7837465564738292\n","Iteration 18689 - loss value [[326.42488605]] accuracy 0.7823691460055097\n","Iteration 18690 - loss value [[328.21740258]] accuracy 0.7796143250688705\n","Iteration 18691 - loss value [[327.95453128]] accuracy 0.7823691460055097\n","Iteration 18692 - loss value [[327.91864773]] accuracy 0.7837465564738292\n","Iteration 18693 - loss value [[328.08150515]] accuracy 0.7768595041322314\n","Iteration 18694 - loss value [[331.95418975]] accuracy 0.78099173553719\n","Iteration 18695 - loss value [[328.41844617]] accuracy 0.7796143250688705\n","Iteration 18696 - loss value [[327.56885309]] accuracy 0.7851239669421488\n","Iteration 18697 - loss value [[328.75129612]] accuracy 0.7768595041322314\n","Iteration 18698 - loss value [[330.9784599]] accuracy 0.7837465564738292\n","Iteration 18699 - loss value [[328.42531292]] accuracy 0.78099173553719\n","Iteration 18700 - loss value [[326.92580832]] accuracy 0.7851239669421488\n","Iteration 18701 - loss value [[327.32191506]] accuracy 0.7768595041322314\n","Iteration 18702 - loss value [[329.39575234]] accuracy 0.7823691460055097\n","Iteration 18703 - loss value [[326.24162881]] accuracy 0.7796143250688705\n","Iteration 18704 - loss value [[325.00891034]] accuracy 0.7851239669421488\n","Iteration 18705 - loss value [[324.46884004]] accuracy 0.7865013774104683\n","Iteration 18706 - loss value [[324.38967678]] accuracy 0.7878787878787878\n","Iteration 18707 - loss value [[324.46678547]] accuracy 0.7865013774104683\n","Iteration 18708 - loss value [[324.9981261]] accuracy 0.7865013774104683\n","Iteration 18709 - loss value [[327.47957448]] accuracy 0.778236914600551\n","Iteration 18710 - loss value [[328.45446465]] accuracy 0.7865013774104683\n","Iteration 18711 - loss value [[329.7719526]] accuracy 0.7768595041322314\n","Iteration 18712 - loss value [[331.09897276]] accuracy 0.778236914600551\n","Iteration 18713 - loss value [[328.69384608]] accuracy 0.78099173553719\n","Iteration 18714 - loss value [[328.40884223]] accuracy 0.7823691460055097\n","Iteration 18715 - loss value [[326.79776568]] accuracy 0.7823691460055097\n","Iteration 18716 - loss value [[326.51292892]] accuracy 0.7865013774104683\n","Iteration 18717 - loss value [[325.8968866]] accuracy 0.7837465564738292\n","Iteration 18718 - loss value [[325.67473979]] accuracy 0.78099173553719\n","Iteration 18719 - loss value [[325.44791924]] accuracy 0.7837465564738292\n","Iteration 18720 - loss value [[324.80211487]] accuracy 0.7865013774104683\n","Iteration 18721 - loss value [[325.13445944]] accuracy 0.7823691460055097\n","Iteration 18722 - loss value [[324.80262425]] accuracy 0.7851239669421488\n","Iteration 18723 - loss value [[325.82206566]] accuracy 0.78099173553719\n","Iteration 18724 - loss value [[328.01282415]] accuracy 0.7865013774104683\n","Iteration 18725 - loss value [[329.20505357]] accuracy 0.7768595041322314\n","Iteration 18726 - loss value [[331.97656438]] accuracy 0.7837465564738292\n","Iteration 18727 - loss value [[329.88323494]] accuracy 0.778236914600551\n","Iteration 18728 - loss value [[335.40370173]] accuracy 0.7851239669421488\n","Iteration 18729 - loss value [[352.20930658]] accuracy 0.7575757575757576\n","Iteration 18730 - loss value [[325.6640424]] accuracy 0.7823691460055097\n","Iteration 18731 - loss value [[327.87865927]] accuracy 0.778236914600551\n","Iteration 18732 - loss value [[325.68517616]] accuracy 0.7851239669421488\n","Iteration 18733 - loss value [[328.5627068]] accuracy 0.7768595041322314\n","Iteration 18734 - loss value [[328.68652]] accuracy 0.7823691460055097\n","Iteration 18735 - loss value [[327.47890009]] accuracy 0.778236914600551\n","Iteration 18736 - loss value [[328.06497462]] accuracy 0.7865013774104683\n","Iteration 18737 - loss value [[328.75403399]] accuracy 0.7768595041322314\n","Iteration 18738 - loss value [[332.65140013]] accuracy 0.7796143250688705\n","Iteration 18739 - loss value [[329.00369739]] accuracy 0.7823691460055097\n","Iteration 18740 - loss value [[328.50823292]] accuracy 0.7837465564738292\n","Iteration 18741 - loss value [[326.55060146]] accuracy 0.7823691460055097\n","Iteration 18742 - loss value [[326.33299314]] accuracy 0.7796143250688705\n","Iteration 18743 - loss value [[325.52823239]] accuracy 0.78099173553719\n","Iteration 18744 - loss value [[324.61511632]] accuracy 0.7851239669421488\n","Iteration 18745 - loss value [[324.69988588]] accuracy 0.7837465564738292\n","Iteration 18746 - loss value [[325.49991707]] accuracy 0.7851239669421488\n","Iteration 18747 - loss value [[328.03097527]] accuracy 0.778236914600551\n","Iteration 18748 - loss value [[334.64884656]] accuracy 0.78099173553719\n","Iteration 18749 - loss value [[338.02540686]] accuracy 0.7713498622589532\n","Iteration 18750 - loss value [[331.80550775]] accuracy 0.78099173553719\n","Iteration 18751 - loss value [[333.87528292]] accuracy 0.7741046831955923\n","Iteration 18752 - loss value [[339.0147138]] accuracy 0.7754820936639119\n","Iteration 18753 - loss value [[368.75267947]] accuracy 0.7382920110192838\n","Iteration 18754 - loss value [[327.06087959]] accuracy 0.7823691460055097\n","Iteration 18755 - loss value [[324.68121538]] accuracy 0.7865013774104683\n","Iteration 18756 - loss value [[325.00135091]] accuracy 0.7823691460055097\n","Iteration 18757 - loss value [[324.65348833]] accuracy 0.7892561983471075\n","Iteration 18758 - loss value [[324.93325944]] accuracy 0.7837465564738292\n","Iteration 18759 - loss value [[325.22859693]] accuracy 0.7878787878787878\n","Iteration 18760 - loss value [[327.76847306]] accuracy 0.7768595041322314\n","Iteration 18761 - loss value [[331.15982848]] accuracy 0.78099173553719\n","Iteration 18762 - loss value [[326.96753384]] accuracy 0.7796143250688705\n","Iteration 18763 - loss value [[325.35771054]] accuracy 0.7823691460055097\n","Iteration 18764 - loss value [[325.12275142]] accuracy 0.7823691460055097\n","Iteration 18765 - loss value [[326.29794948]] accuracy 0.7865013774104683\n","Iteration 18766 - loss value [[325.69490845]] accuracy 0.7823691460055097\n","Iteration 18767 - loss value [[326.70807072]] accuracy 0.7837465564738292\n","Iteration 18768 - loss value [[327.51300952]] accuracy 0.7768595041322314\n","Iteration 18769 - loss value [[331.96694429]] accuracy 0.7796143250688705\n","Iteration 18770 - loss value [[330.24495015]] accuracy 0.7754820936639119\n","Iteration 18771 - loss value [[337.45715811]] accuracy 0.7823691460055097\n","Iteration 18772 - loss value [[354.8629894]] accuracy 0.7534435261707989\n","Iteration 18773 - loss value [[325.09905253]] accuracy 0.7851239669421488\n","Iteration 18774 - loss value [[326.37507593]] accuracy 0.778236914600551\n","Iteration 18775 - loss value [[330.20899273]] accuracy 0.78099173553719\n","Iteration 18776 - loss value [[326.70988075]] accuracy 0.7796143250688705\n","Iteration 18777 - loss value [[325.40401635]] accuracy 0.7837465564738292\n","Iteration 18778 - loss value [[325.27965485]] accuracy 0.7837465564738292\n","Iteration 18779 - loss value [[326.65097498]] accuracy 0.7865013774104683\n","Iteration 18780 - loss value [[325.91052582]] accuracy 0.7823691460055097\n","Iteration 18781 - loss value [[325.96292908]] accuracy 0.7837465564738292\n","Iteration 18782 - loss value [[326.46060987]] accuracy 0.7796143250688705\n","Iteration 18783 - loss value [[327.57970378]] accuracy 0.7837465564738292\n","Iteration 18784 - loss value [[327.45625694]] accuracy 0.7754820936639119\n","Iteration 18785 - loss value [[333.719603]] accuracy 0.7837465564738292\n","Iteration 18786 - loss value [[336.52312681]] accuracy 0.7727272727272727\n","Iteration 18787 - loss value [[329.38140448]] accuracy 0.7823691460055097\n","Iteration 18788 - loss value [[325.81851276]] accuracy 0.78099173553719\n","Iteration 18789 - loss value [[325.61135695]] accuracy 0.7837465564738292\n","Iteration 18790 - loss value [[325.03422792]] accuracy 0.7837465564738292\n","Iteration 18791 - loss value [[324.87531681]] accuracy 0.7865013774104683\n","Iteration 18792 - loss value [[326.41385726]] accuracy 0.7837465564738292\n","Iteration 18793 - loss value [[327.02582082]] accuracy 0.78099173553719\n","Iteration 18794 - loss value [[331.3186579]] accuracy 0.7768595041322314\n","Iteration 18795 - loss value [[328.27036861]] accuracy 0.78099173553719\n","Iteration 18796 - loss value [[326.34603458]] accuracy 0.7851239669421488\n","Iteration 18797 - loss value [[325.97319781]] accuracy 0.7837465564738292\n","Iteration 18798 - loss value [[326.02031867]] accuracy 0.78099173553719\n","Iteration 18799 - loss value [[325.38088648]] accuracy 0.7837465564738292\n","Iteration 18800 - loss value [[326.00697232]] accuracy 0.7851239669421488\n","Iteration 18801 - loss value [[327.033519]] accuracy 0.778236914600551\n","Iteration 18802 - loss value [[330.39938194]] accuracy 0.7823691460055097\n","Iteration 18803 - loss value [[328.14741943]] accuracy 0.78099173553719\n","Iteration 18804 - loss value [[328.18694211]] accuracy 0.7823691460055097\n","Iteration 18805 - loss value [[325.90157268]] accuracy 0.7837465564738292\n","Iteration 18806 - loss value [[326.06928545]] accuracy 0.7865013774104683\n","Iteration 18807 - loss value [[325.45867637]] accuracy 0.78099173553719\n","Iteration 18808 - loss value [[325.09131305]] accuracy 0.7837465564738292\n","Iteration 18809 - loss value [[326.45302544]] accuracy 0.7796143250688705\n","Iteration 18810 - loss value [[332.00977687]] accuracy 0.7851239669421488\n","Iteration 18811 - loss value [[333.71796736]] accuracy 0.778236914600551\n","Iteration 18812 - loss value [[337.31765148]] accuracy 0.7837465564738292\n","Iteration 18813 - loss value [[355.1107878]] accuracy 0.7534435261707989\n","Iteration 18814 - loss value [[324.73367456]] accuracy 0.7837465564738292\n","Iteration 18815 - loss value [[324.7197415]] accuracy 0.7865013774104683\n","Iteration 18816 - loss value [[325.32920943]] accuracy 0.7851239669421488\n","Iteration 18817 - loss value [[325.18836923]] accuracy 0.78099173553719\n","Iteration 18818 - loss value [[325.7814638]] accuracy 0.7851239669421488\n","Iteration 18819 - loss value [[324.75802527]] accuracy 0.7823691460055097\n","Iteration 18820 - loss value [[325.07348252]] accuracy 0.7851239669421488\n","Iteration 18821 - loss value [[326.40972039]] accuracy 0.78099173553719\n","Iteration 18822 - loss value [[330.65550535]] accuracy 0.7823691460055097\n","Iteration 18823 - loss value [[329.41571941]] accuracy 0.778236914600551\n","Iteration 18824 - loss value [[329.99084507]] accuracy 0.7851239669421488\n","Iteration 18825 - loss value [[330.313967]] accuracy 0.7713498622589532\n","Iteration 18826 - loss value [[331.90198277]] accuracy 0.78099173553719\n","Iteration 18827 - loss value [[340.63264664]] accuracy 0.7699724517906336\n","Iteration 18828 - loss value [[340.58176511]] accuracy 0.7768595041322314\n","Iteration 18829 - loss value [[360.70547051]] accuracy 0.7465564738292011\n","Iteration 18830 - loss value [[325.88214269]] accuracy 0.7823691460055097\n","Iteration 18831 - loss value [[325.45257705]] accuracy 0.7851239669421488\n","Iteration 18832 - loss value [[325.14233495]] accuracy 0.7865013774104683\n","Iteration 18833 - loss value [[326.29855085]] accuracy 0.7851239669421488\n","Iteration 18834 - loss value [[324.8669637]] accuracy 0.7837465564738292\n","Iteration 18835 - loss value [[324.75898565]] accuracy 0.7837465564738292\n","Iteration 18836 - loss value [[325.40257247]] accuracy 0.7851239669421488\n","Iteration 18837 - loss value [[327.541231]] accuracy 0.778236914600551\n","Iteration 18838 - loss value [[331.60821347]] accuracy 0.7823691460055097\n","Iteration 18839 - loss value [[330.64850096]] accuracy 0.7796143250688705\n","Iteration 18840 - loss value [[338.60650444]] accuracy 0.7837465564738292\n","Iteration 18841 - loss value [[356.85556692]] accuracy 0.7506887052341598\n","Iteration 18842 - loss value [[325.79928615]] accuracy 0.7837465564738292\n","Iteration 18843 - loss value [[328.74532745]] accuracy 0.7768595041322314\n","Iteration 18844 - loss value [[326.9092864]] accuracy 0.7851239669421488\n","Iteration 18845 - loss value [[326.71957432]] accuracy 0.7796143250688705\n","Iteration 18846 - loss value [[331.04801788]] accuracy 0.7837465564738292\n","Iteration 18847 - loss value [[329.3403964]] accuracy 0.78099173553719\n","Iteration 18848 - loss value [[330.63948268]] accuracy 0.7851239669421488\n","Iteration 18849 - loss value [[327.71100818]] accuracy 0.7796143250688705\n","Iteration 18850 - loss value [[326.31708387]] accuracy 0.7865013774104683\n","Iteration 18851 - loss value [[325.72368654]] accuracy 0.7823691460055097\n","Iteration 18852 - loss value [[325.550572]] accuracy 0.7865013774104683\n","Iteration 18853 - loss value [[324.83276803]] accuracy 0.7837465564738292\n","Iteration 18854 - loss value [[325.31362647]] accuracy 0.7851239669421488\n","Iteration 18855 - loss value [[324.81363678]] accuracy 0.7851239669421488\n","Iteration 18856 - loss value [[325.60450737]] accuracy 0.7851239669421488\n","Iteration 18857 - loss value [[326.73190521]] accuracy 0.78099173553719\n","Iteration 18858 - loss value [[332.43633085]] accuracy 0.7851239669421488\n","Iteration 18859 - loss value [[337.05912906]] accuracy 0.7741046831955923\n","Iteration 18860 - loss value [[330.53412924]] accuracy 0.7837465564738292\n","Iteration 18861 - loss value [[328.1258651]] accuracy 0.78099173553719\n","Iteration 18862 - loss value [[327.53321223]] accuracy 0.7851239669421488\n","Iteration 18863 - loss value [[325.88708346]] accuracy 0.78099173553719\n","Iteration 18864 - loss value [[325.9285936]] accuracy 0.7851239669421488\n","Iteration 18865 - loss value [[325.40372894]] accuracy 0.78099173553719\n","Iteration 18866 - loss value [[325.2825826]] accuracy 0.7837465564738292\n","Iteration 18867 - loss value [[324.62677869]] accuracy 0.7837465564738292\n","Iteration 18868 - loss value [[324.5140718]] accuracy 0.7851239669421488\n","Iteration 18869 - loss value [[324.88155554]] accuracy 0.7851239669421488\n","Iteration 18870 - loss value [[325.31105636]] accuracy 0.78099173553719\n","Iteration 18871 - loss value [[325.00308603]] accuracy 0.7837465564738292\n","Iteration 18872 - loss value [[326.37703164]] accuracy 0.7823691460055097\n","Iteration 18873 - loss value [[325.55111497]] accuracy 0.7837465564738292\n","Iteration 18874 - loss value [[326.8315648]] accuracy 0.7837465564738292\n","Iteration 18875 - loss value [[325.22486558]] accuracy 0.7837465564738292\n","Iteration 18876 - loss value [[326.23651568]] accuracy 0.7837465564738292\n","Iteration 18877 - loss value [[325.15651258]] accuracy 0.7837465564738292\n","Iteration 18878 - loss value [[325.91467474]] accuracy 0.7837465564738292\n","Iteration 18879 - loss value [[325.02255583]] accuracy 0.7823691460055097\n","Iteration 18880 - loss value [[325.21648464]] accuracy 0.7837465564738292\n","Iteration 18881 - loss value [[325.25596752]] accuracy 0.7823691460055097\n","Iteration 18882 - loss value [[326.9670469]] accuracy 0.7865013774104683\n","Iteration 18883 - loss value [[325.72020789]] accuracy 0.7823691460055097\n","Iteration 18884 - loss value [[327.22708577]] accuracy 0.7878787878787878\n","Iteration 18885 - loss value [[327.27647386]] accuracy 0.7768595041322314\n","Iteration 18886 - loss value [[331.16392839]] accuracy 0.7796143250688705\n","Iteration 18887 - loss value [[329.85077218]] accuracy 0.7796143250688705\n","Iteration 18888 - loss value [[328.60179895]] accuracy 0.7837465564738292\n","Iteration 18889 - loss value [[326.6732831]] accuracy 0.778236914600551\n","Iteration 18890 - loss value [[325.67250555]] accuracy 0.7851239669421488\n","Iteration 18891 - loss value [[325.45010172]] accuracy 0.7837465564738292\n","Iteration 18892 - loss value [[325.09904036]] accuracy 0.7823691460055097\n","Iteration 18893 - loss value [[324.96976593]] accuracy 0.7837465564738292\n","Iteration 18894 - loss value [[324.89583462]] accuracy 0.7851239669421488\n","Iteration 18895 - loss value [[325.56693798]] accuracy 0.7851239669421488\n","Iteration 18896 - loss value [[326.12107312]] accuracy 0.78099173553719\n","Iteration 18897 - loss value [[330.60327335]] accuracy 0.7823691460055097\n","Iteration 18898 - loss value [[327.36912494]] accuracy 0.7796143250688705\n","Iteration 18899 - loss value [[326.83463569]] accuracy 0.7851239669421488\n","Iteration 18900 - loss value [[325.14149045]] accuracy 0.78099173553719\n","Iteration 18901 - loss value [[324.66295599]] accuracy 0.7851239669421488\n","Iteration 18902 - loss value [[325.02157661]] accuracy 0.7851239669421488\n","Iteration 18903 - loss value [[326.65127159]] accuracy 0.7837465564738292\n","Iteration 18904 - loss value [[326.13950626]] accuracy 0.7823691460055097\n","Iteration 18905 - loss value [[326.85366108]] accuracy 0.7823691460055097\n","Iteration 18906 - loss value [[325.85056544]] accuracy 0.7823691460055097\n","Iteration 18907 - loss value [[324.45180248]] accuracy 0.7865013774104683\n","Iteration 18908 - loss value [[324.56905305]] accuracy 0.7865013774104683\n","Iteration 18909 - loss value [[325.27004216]] accuracy 0.7851239669421488\n","Iteration 18910 - loss value [[324.90437113]] accuracy 0.7837465564738292\n","Iteration 18911 - loss value [[325.86249852]] accuracy 0.7851239669421488\n","Iteration 18912 - loss value [[325.38723471]] accuracy 0.7823691460055097\n","Iteration 18913 - loss value [[326.16133212]] accuracy 0.7865013774104683\n","Iteration 18914 - loss value [[326.71820634]] accuracy 0.7796143250688705\n","Iteration 18915 - loss value [[327.77077376]] accuracy 0.7851239669421488\n","Iteration 18916 - loss value [[328.88324395]] accuracy 0.778236914600551\n","Iteration 18917 - loss value [[329.73625372]] accuracy 0.7823691460055097\n","Iteration 18918 - loss value [[326.82663759]] accuracy 0.7796143250688705\n","Iteration 18919 - loss value [[325.41736373]] accuracy 0.7837465564738292\n","Iteration 18920 - loss value [[325.24272263]] accuracy 0.7823691460055097\n","Iteration 18921 - loss value [[326.80810026]] accuracy 0.7837465564738292\n","Iteration 18922 - loss value [[326.14765013]] accuracy 0.7823691460055097\n","Iteration 18923 - loss value [[327.1232239]] accuracy 0.7823691460055097\n","Iteration 18924 - loss value [[325.7879097]] accuracy 0.7823691460055097\n","Iteration 18925 - loss value [[326.87753531]] accuracy 0.78099173553719\n","Iteration 18926 - loss value [[326.83458651]] accuracy 0.778236914600551\n","Iteration 18927 - loss value [[330.34149497]] accuracy 0.778236914600551\n","Iteration 18928 - loss value [[327.09393085]] accuracy 0.78099173553719\n","Iteration 18929 - loss value [[325.56563387]] accuracy 0.7823691460055097\n","Iteration 18930 - loss value [[324.75312897]] accuracy 0.7851239669421488\n","Iteration 18931 - loss value [[325.14268802]] accuracy 0.7837465564738292\n","Iteration 18932 - loss value [[327.36637935]] accuracy 0.778236914600551\n","Iteration 18933 - loss value [[329.15889744]] accuracy 0.7837465564738292\n","Iteration 18934 - loss value [[330.39436962]] accuracy 0.7768595041322314\n","Iteration 18935 - loss value [[332.36276706]] accuracy 0.78099173553719\n","Iteration 18936 - loss value [[332.53570948]] accuracy 0.7768595041322314\n","Iteration 18937 - loss value [[331.46157686]] accuracy 0.7837465564738292\n","Iteration 18938 - loss value [[333.19488423]] accuracy 0.778236914600551\n","Iteration 18939 - loss value [[334.28664034]] accuracy 0.7837465564738292\n","Iteration 18940 - loss value [[349.51775662]] accuracy 0.7589531680440771\n","Iteration 18941 - loss value [[331.36016975]] accuracy 0.778236914600551\n","Iteration 18942 - loss value [[337.23772995]] accuracy 0.7727272727272727\n","Iteration 18943 - loss value [[330.48075639]] accuracy 0.7823691460055097\n","Iteration 18944 - loss value [[336.06595292]] accuracy 0.7727272727272727\n","Iteration 18945 - loss value [[332.43043756]] accuracy 0.7796143250688705\n","Iteration 18946 - loss value [[340.12940899]] accuracy 0.768595041322314\n","Iteration 18947 - loss value [[347.16247997]] accuracy 0.7727272727272727\n","Iteration 18948 - loss value [[375.44214074]] accuracy 0.7355371900826446\n","Iteration 18949 - loss value [[335.16304221]] accuracy 0.778236914600551\n","Iteration 18950 - loss value [[337.14268361]] accuracy 0.768595041322314\n","Iteration 18951 - loss value [[335.29540013]] accuracy 0.7768595041322314\n","Iteration 18952 - loss value [[343.85658995]] accuracy 0.7603305785123967\n","Iteration 18953 - loss value [[330.37660883]] accuracy 0.7768595041322314\n","Iteration 18954 - loss value [[330.18653282]] accuracy 0.7741046831955923\n","Iteration 18955 - loss value [[331.15225881]] accuracy 0.7823691460055097\n","Iteration 18956 - loss value [[331.05265186]] accuracy 0.7754820936639119\n","Iteration 18957 - loss value [[334.32377136]] accuracy 0.7823691460055097\n","Iteration 18958 - loss value [[349.87810593]] accuracy 0.7548209366391184\n","Iteration 18959 - loss value [[326.54375333]] accuracy 0.7837465564738292\n","Iteration 18960 - loss value [[325.08675116]] accuracy 0.78099173553719\n","Iteration 18961 - loss value [[324.67857419]] accuracy 0.7837465564738292\n","Iteration 18962 - loss value [[325.63779668]] accuracy 0.7823691460055097\n","Iteration 18963 - loss value [[327.89591577]] accuracy 0.7878787878787878\n","Iteration 18964 - loss value [[329.37494355]] accuracy 0.7768595041322314\n","Iteration 18965 - loss value [[331.45483206]] accuracy 0.778236914600551\n","Iteration 18966 - loss value [[328.83949231]] accuracy 0.7796143250688705\n","Iteration 18967 - loss value [[329.78337491]] accuracy 0.78099173553719\n","Iteration 18968 - loss value [[327.32611836]] accuracy 0.78099173553719\n","Iteration 18969 - loss value [[325.80434785]] accuracy 0.78099173553719\n","Iteration 18970 - loss value [[324.92812641]] accuracy 0.7823691460055097\n","Iteration 18971 - loss value [[324.40796279]] accuracy 0.7878787878787878\n","Iteration 18972 - loss value [[324.75479429]] accuracy 0.7865013774104683\n","Iteration 18973 - loss value [[326.47943651]] accuracy 0.7796143250688705\n","Iteration 18974 - loss value [[332.14888588]] accuracy 0.7851239669421488\n","Iteration 18975 - loss value [[337.41550607]] accuracy 0.7713498622589532\n","Iteration 18976 - loss value [[332.3905416]] accuracy 0.7851239669421488\n","Iteration 18977 - loss value [[337.28631037]] accuracy 0.7727272727272727\n","Iteration 18978 - loss value [[330.900766]] accuracy 0.7823691460055097\n","Iteration 18979 - loss value [[327.67618927]] accuracy 0.78099173553719\n","Iteration 18980 - loss value [[325.55789282]] accuracy 0.7823691460055097\n","Iteration 18981 - loss value [[325.57600635]] accuracy 0.7837465564738292\n","Iteration 18982 - loss value [[325.29887861]] accuracy 0.7837465564738292\n","Iteration 18983 - loss value [[325.17485348]] accuracy 0.7823691460055097\n","Iteration 18984 - loss value [[325.3367018]] accuracy 0.78099173553719\n","Iteration 18985 - loss value [[327.34891013]] accuracy 0.7837465564738292\n","Iteration 18986 - loss value [[327.87476131]] accuracy 0.7741046831955923\n","Iteration 18987 - loss value [[330.5426476]] accuracy 0.7837465564738292\n","Iteration 18988 - loss value [[329.22926383]] accuracy 0.7768595041322314\n","Iteration 18989 - loss value [[328.89524578]] accuracy 0.7865013774104683\n","Iteration 18990 - loss value [[327.45412597]] accuracy 0.778236914600551\n","Iteration 18991 - loss value [[325.55788941]] accuracy 0.7865013774104683\n","Iteration 18992 - loss value [[325.26153798]] accuracy 0.7823691460055097\n","Iteration 18993 - loss value [[324.64036567]] accuracy 0.7851239669421488\n","Iteration 18994 - loss value [[324.48379328]] accuracy 0.7865013774104683\n","Iteration 18995 - loss value [[325.02731006]] accuracy 0.7837465564738292\n","Iteration 18996 - loss value [[327.02499607]] accuracy 0.7851239669421488\n","Iteration 18997 - loss value [[326.25886933]] accuracy 0.7837465564738292\n","Iteration 18998 - loss value [[328.33828691]] accuracy 0.7851239669421488\n","Iteration 18999 - loss value [[327.78277238]] accuracy 0.778236914600551\n","Iteration 19000 - loss value [[328.40720084]] accuracy 0.7823691460055097\n","Iteration 19001 - loss value [[326.36364373]] accuracy 0.778236914600551\n","Iteration 19002 - loss value [[325.89277982]] accuracy 0.7837465564738292\n","Iteration 19003 - loss value [[324.96593362]] accuracy 0.7823691460055097\n","Iteration 19004 - loss value [[324.50723054]] accuracy 0.7865013774104683\n","Iteration 19005 - loss value [[325.14479996]] accuracy 0.7837465564738292\n","Iteration 19006 - loss value [[327.25593472]] accuracy 0.7837465564738292\n","Iteration 19007 - loss value [[326.90456964]] accuracy 0.7796143250688705\n","Iteration 19008 - loss value [[327.22634406]] accuracy 0.7837465564738292\n","Iteration 19009 - loss value [[326.91036061]] accuracy 0.7796143250688705\n","Iteration 19010 - loss value [[330.30117549]] accuracy 0.78099173553719\n","Iteration 19011 - loss value [[326.9923254]] accuracy 0.7796143250688705\n","Iteration 19012 - loss value [[325.77514776]] accuracy 0.7851239669421488\n","Iteration 19013 - loss value [[325.25736218]] accuracy 0.7837465564738292\n","Iteration 19014 - loss value [[325.12291145]] accuracy 0.7837465564738292\n","Iteration 19015 - loss value [[324.72810147]] accuracy 0.7837465564738292\n","Iteration 19016 - loss value [[325.09791668]] accuracy 0.7837465564738292\n","Iteration 19017 - loss value [[325.38516449]] accuracy 0.7865013774104683\n","Iteration 19018 - loss value [[328.31021573]] accuracy 0.7768595041322314\n","Iteration 19019 - loss value [[328.95070511]] accuracy 0.7851239669421488\n","Iteration 19020 - loss value [[329.97332365]] accuracy 0.7768595041322314\n","Iteration 19021 - loss value [[331.03416257]] accuracy 0.778236914600551\n","Iteration 19022 - loss value [[327.67077371]] accuracy 0.7823691460055097\n","Iteration 19023 - loss value [[326.60348749]] accuracy 0.7837465564738292\n","Iteration 19024 - loss value [[325.65288113]] accuracy 0.7823691460055097\n","Iteration 19025 - loss value [[327.35416461]] accuracy 0.78099173553719\n","Iteration 19026 - loss value [[326.46036289]] accuracy 0.7796143250688705\n","Iteration 19027 - loss value [[325.93980591]] accuracy 0.7837465564738292\n","Iteration 19028 - loss value [[325.69331967]] accuracy 0.7823691460055097\n","Iteration 19029 - loss value [[325.05847116]] accuracy 0.7865013774104683\n","Iteration 19030 - loss value [[326.89609967]] accuracy 0.7768595041322314\n","Iteration 19031 - loss value [[332.30739398]] accuracy 0.78099173553719\n","Iteration 19032 - loss value [[328.84867251]] accuracy 0.7851239669421488\n","Iteration 19033 - loss value [[328.9649378]] accuracy 0.7837465564738292\n","Iteration 19034 - loss value [[326.74875895]] accuracy 0.7768595041322314\n","Iteration 19035 - loss value [[325.13899386]] accuracy 0.7837465564738292\n","Iteration 19036 - loss value [[324.56104522]] accuracy 0.7823691460055097\n","Iteration 19037 - loss value [[324.76541939]] accuracy 0.7823691460055097\n","Iteration 19038 - loss value [[324.43437552]] accuracy 0.7837465564738292\n","Iteration 19039 - loss value [[324.43205478]] accuracy 0.7865013774104683\n","Iteration 19040 - loss value [[325.20742963]] accuracy 0.7851239669421488\n","Iteration 19041 - loss value [[327.81590966]] accuracy 0.778236914600551\n","Iteration 19042 - loss value [[329.80409936]] accuracy 0.7796143250688705\n","Iteration 19043 - loss value [[326.39249664]] accuracy 0.7796143250688705\n","Iteration 19044 - loss value [[324.9281748]] accuracy 0.7823691460055097\n","Iteration 19045 - loss value [[325.42356768]] accuracy 0.7823691460055097\n","Iteration 19046 - loss value [[326.66477435]] accuracy 0.7837465564738292\n","Iteration 19047 - loss value [[327.56514159]] accuracy 0.7768595041322314\n","Iteration 19048 - loss value [[332.22741559]] accuracy 0.7796143250688705\n","Iteration 19049 - loss value [[328.50795863]] accuracy 0.7796143250688705\n","Iteration 19050 - loss value [[328.84144218]] accuracy 0.7823691460055097\n","Iteration 19051 - loss value [[326.9504275]] accuracy 0.7796143250688705\n","Iteration 19052 - loss value [[326.24780383]] accuracy 0.7837465564738292\n","Iteration 19053 - loss value [[325.30270251]] accuracy 0.7837465564738292\n","Iteration 19054 - loss value [[324.87457446]] accuracy 0.7865013774104683\n","Iteration 19055 - loss value [[325.85365178]] accuracy 0.78099173553719\n","Iteration 19056 - loss value [[327.59978789]] accuracy 0.7837465564738292\n","Iteration 19057 - loss value [[327.34180864]] accuracy 0.7796143250688705\n","Iteration 19058 - loss value [[327.91445635]] accuracy 0.7851239669421488\n","Iteration 19059 - loss value [[329.12702806]] accuracy 0.7768595041322314\n","Iteration 19060 - loss value [[330.18758327]] accuracy 0.7796143250688705\n","Iteration 19061 - loss value [[327.64595089]] accuracy 0.78099173553719\n","Iteration 19062 - loss value [[327.26515845]] accuracy 0.7837465564738292\n","Iteration 19063 - loss value [[325.74566797]] accuracy 0.78099173553719\n","Iteration 19064 - loss value [[325.44687611]] accuracy 0.7851239669421488\n","Iteration 19065 - loss value [[325.3877018]] accuracy 0.7823691460055097\n","Iteration 19066 - loss value [[326.3822027]] accuracy 0.7851239669421488\n","Iteration 19067 - loss value [[326.11110828]] accuracy 0.78099173553719\n","Iteration 19068 - loss value [[329.95672788]] accuracy 0.78099173553719\n","Iteration 19069 - loss value [[326.41990713]] accuracy 0.7796143250688705\n","Iteration 19070 - loss value [[325.94055785]] accuracy 0.7851239669421488\n","Iteration 19071 - loss value [[325.45956452]] accuracy 0.7823691460055097\n","Iteration 19072 - loss value [[325.68461373]] accuracy 0.7823691460055097\n","Iteration 19073 - loss value [[325.88774329]] accuracy 0.7837465564738292\n","Iteration 19074 - loss value [[325.80388305]] accuracy 0.78099173553719\n","Iteration 19075 - loss value [[324.8980975]] accuracy 0.7837465564738292\n","Iteration 19076 - loss value [[325.32642554]] accuracy 0.7851239669421488\n","Iteration 19077 - loss value [[327.12281585]] accuracy 0.778236914600551\n","Iteration 19078 - loss value [[329.65975615]] accuracy 0.7837465564738292\n","Iteration 19079 - loss value [[325.99763217]] accuracy 0.7796143250688705\n","Iteration 19080 - loss value [[324.93382424]] accuracy 0.7837465564738292\n","Iteration 19081 - loss value [[325.74597481]] accuracy 0.7796143250688705\n","Iteration 19082 - loss value [[328.22646026]] accuracy 0.7851239669421488\n","Iteration 19083 - loss value [[330.09919322]] accuracy 0.7754820936639119\n","Iteration 19084 - loss value [[329.71261554]] accuracy 0.78099173553719\n","Iteration 19085 - loss value [[326.51377898]] accuracy 0.7796143250688705\n","Iteration 19086 - loss value [[325.22752539]] accuracy 0.7837465564738292\n","Iteration 19087 - loss value [[324.92912186]] accuracy 0.7837465564738292\n","Iteration 19088 - loss value [[326.29894929]] accuracy 0.7823691460055097\n","Iteration 19089 - loss value [[326.34379303]] accuracy 0.7823691460055097\n","Iteration 19090 - loss value [[325.38941827]] accuracy 0.7851239669421488\n","Iteration 19091 - loss value [[324.73204104]] accuracy 0.7851239669421488\n","Iteration 19092 - loss value [[325.4631346]] accuracy 0.7865013774104683\n","Iteration 19093 - loss value [[325.29985245]] accuracy 0.7837465564738292\n","Iteration 19094 - loss value [[327.47477822]] accuracy 0.7837465564738292\n","Iteration 19095 - loss value [[326.64119041]] accuracy 0.7823691460055097\n","Iteration 19096 - loss value [[327.51139112]] accuracy 0.7865013774104683\n","Iteration 19097 - loss value [[327.86638482]] accuracy 0.7796143250688705\n","Iteration 19098 - loss value [[330.58803897]] accuracy 0.7823691460055097\n","Iteration 19099 - loss value [[329.247678]] accuracy 0.7768595041322314\n","Iteration 19100 - loss value [[331.98460282]] accuracy 0.7823691460055097\n","Iteration 19101 - loss value [[329.38159907]] accuracy 0.778236914600551\n","Iteration 19102 - loss value [[330.56291179]] accuracy 0.7865013774104683\n","Iteration 19103 - loss value [[327.56186578]] accuracy 0.7796143250688705\n","Iteration 19104 - loss value [[325.7175578]] accuracy 0.7851239669421488\n","Iteration 19105 - loss value [[325.25363081]] accuracy 0.7837465564738292\n","Iteration 19106 - loss value [[324.85015453]] accuracy 0.7851239669421488\n","Iteration 19107 - loss value [[325.42730699]] accuracy 0.7865013774104683\n","Iteration 19108 - loss value [[327.72731129]] accuracy 0.778236914600551\n","Iteration 19109 - loss value [[332.91286475]] accuracy 0.7823691460055097\n","Iteration 19110 - loss value [[337.39589051]] accuracy 0.7727272727272727\n","Iteration 19111 - loss value [[332.16246415]] accuracy 0.7865013774104683\n","Iteration 19112 - loss value [[338.08780681]] accuracy 0.7713498622589532\n","Iteration 19113 - loss value [[335.94600311]] accuracy 0.7823691460055097\n","Iteration 19114 - loss value [[350.01639045]] accuracy 0.7575757575757576\n","Iteration 19115 - loss value [[328.37990313]] accuracy 0.78099173553719\n","Iteration 19116 - loss value [[330.18837823]] accuracy 0.7713498622589532\n","Iteration 19117 - loss value [[331.00102732]] accuracy 0.78099173553719\n","Iteration 19118 - loss value [[330.02330687]] accuracy 0.778236914600551\n","Iteration 19119 - loss value [[331.36446406]] accuracy 0.7837465564738292\n","Iteration 19120 - loss value [[334.52932403]] accuracy 0.7768595041322314\n","Iteration 19121 - loss value [[331.76469726]] accuracy 0.7837465564738292\n","Iteration 19122 - loss value [[337.23354037]] accuracy 0.7727272727272727\n","Iteration 19123 - loss value [[332.97806081]] accuracy 0.7865013774104683\n","Iteration 19124 - loss value [[337.88061932]] accuracy 0.7741046831955923\n","Iteration 19125 - loss value [[331.00818618]] accuracy 0.7851239669421488\n","Iteration 19126 - loss value [[333.44433083]] accuracy 0.7754820936639119\n","Iteration 19127 - loss value [[334.43681437]] accuracy 0.7837465564738292\n","Iteration 19128 - loss value [[348.98941403]] accuracy 0.7603305785123967\n","Iteration 19129 - loss value [[330.56680329]] accuracy 0.7823691460055097\n","Iteration 19130 - loss value [[334.83242428]] accuracy 0.7741046831955923\n","Iteration 19131 - loss value [[330.92036812]] accuracy 0.7837465564738292\n","Iteration 19132 - loss value [[327.29162575]] accuracy 0.7796143250688705\n","Iteration 19133 - loss value [[326.48521201]] accuracy 0.7878787878787878\n","Iteration 19134 - loss value [[325.52779646]] accuracy 0.78099173553719\n","Iteration 19135 - loss value [[325.47198501]] accuracy 0.7865013774104683\n","Iteration 19136 - loss value [[324.81342872]] accuracy 0.7837465564738292\n","Iteration 19137 - loss value [[325.51593878]] accuracy 0.7865013774104683\n","Iteration 19138 - loss value [[325.64414152]] accuracy 0.7823691460055097\n","Iteration 19139 - loss value [[326.63951258]] accuracy 0.7851239669421488\n","Iteration 19140 - loss value [[325.28391943]] accuracy 0.7837465564738292\n","Iteration 19141 - loss value [[326.10765166]] accuracy 0.7851239669421488\n","Iteration 19142 - loss value [[325.71915557]] accuracy 0.7823691460055097\n","Iteration 19143 - loss value [[325.5910616]] accuracy 0.7865013774104683\n","Iteration 19144 - loss value [[326.54287989]] accuracy 0.7796143250688705\n","Iteration 19145 - loss value [[331.36186619]] accuracy 0.78099173553719\n","Iteration 19146 - loss value [[327.17335459]] accuracy 0.78099173553719\n","Iteration 19147 - loss value [[326.06134565]] accuracy 0.7837465564738292\n","Iteration 19148 - loss value [[325.59906497]] accuracy 0.7837465564738292\n","Iteration 19149 - loss value [[327.33197744]] accuracy 0.78099173553719\n","Iteration 19150 - loss value [[325.90092677]] accuracy 0.7823691460055097\n","Iteration 19151 - loss value [[326.24134716]] accuracy 0.7837465564738292\n","Iteration 19152 - loss value [[325.98256896]] accuracy 0.7837465564738292\n","Iteration 19153 - loss value [[327.81440863]] accuracy 0.78099173553719\n","Iteration 19154 - loss value [[326.68855677]] accuracy 0.7823691460055097\n","Iteration 19155 - loss value [[327.32224857]] accuracy 0.7823691460055097\n","Iteration 19156 - loss value [[325.54032889]] accuracy 0.7837465564738292\n","Iteration 19157 - loss value [[326.36738624]] accuracy 0.7823691460055097\n","Iteration 19158 - loss value [[326.78239369]] accuracy 0.778236914600551\n","Iteration 19159 - loss value [[330.15969831]] accuracy 0.7796143250688705\n","Iteration 19160 - loss value [[326.66551121]] accuracy 0.7796143250688705\n","Iteration 19161 - loss value [[325.47032089]] accuracy 0.7837465564738292\n","Iteration 19162 - loss value [[324.82314813]] accuracy 0.7823691460055097\n","Iteration 19163 - loss value [[324.98677109]] accuracy 0.7837465564738292\n","Iteration 19164 - loss value [[325.41616695]] accuracy 0.7823691460055097\n","Iteration 19165 - loss value [[324.74243225]] accuracy 0.7878787878787878\n","Iteration 19166 - loss value [[325.47856798]] accuracy 0.7851239669421488\n","Iteration 19167 - loss value [[327.74022764]] accuracy 0.778236914600551\n","Iteration 19168 - loss value [[328.4617925]] accuracy 0.7851239669421488\n","Iteration 19169 - loss value [[329.60335285]] accuracy 0.7727272727272727\n","Iteration 19170 - loss value [[330.95048583]] accuracy 0.78099173553719\n","Iteration 19171 - loss value [[331.66799498]] accuracy 0.7768595041322314\n","Iteration 19172 - loss value [[343.08678442]] accuracy 0.7768595041322314\n","Iteration 19173 - loss value [[371.04070789]] accuracy 0.7382920110192838\n","Iteration 19174 - loss value [[326.25483035]] accuracy 0.78099173553719\n","Iteration 19175 - loss value [[327.28593891]] accuracy 0.7796143250688705\n","Iteration 19176 - loss value [[328.42115359]] accuracy 0.7851239669421488\n","Iteration 19177 - loss value [[329.34597357]] accuracy 0.7754820936639119\n","Iteration 19178 - loss value [[330.26886223]] accuracy 0.78099173553719\n","Iteration 19179 - loss value [[327.36170296]] accuracy 0.7796143250688705\n","Iteration 19180 - loss value [[325.44815481]] accuracy 0.7796143250688705\n","Iteration 19181 - loss value [[325.18265817]] accuracy 0.7823691460055097\n","Iteration 19182 - loss value [[324.6022768]] accuracy 0.7851239669421488\n","Iteration 19183 - loss value [[324.97188618]] accuracy 0.7823691460055097\n","Iteration 19184 - loss value [[325.32779721]] accuracy 0.7851239669421488\n","Iteration 19185 - loss value [[326.58031768]] accuracy 0.7796143250688705\n","Iteration 19186 - loss value [[332.02378418]] accuracy 0.7796143250688705\n","Iteration 19187 - loss value [[329.02459269]] accuracy 0.778236914600551\n","Iteration 19188 - loss value [[327.46173431]] accuracy 0.7837465564738292\n","Iteration 19189 - loss value [[325.91582497]] accuracy 0.7823691460055097\n","Iteration 19190 - loss value [[325.28387184]] accuracy 0.7796143250688705\n","Iteration 19191 - loss value [[325.09585443]] accuracy 0.7851239669421488\n","Iteration 19192 - loss value [[324.86955763]] accuracy 0.7837465564738292\n","Iteration 19193 - loss value [[324.49743546]] accuracy 0.7865013774104683\n","Iteration 19194 - loss value [[324.94166793]] accuracy 0.7851239669421488\n","Iteration 19195 - loss value [[325.94435213]] accuracy 0.78099173553719\n","Iteration 19196 - loss value [[329.80233299]] accuracy 0.7796143250688705\n","Iteration 19197 - loss value [[327.95847345]] accuracy 0.7768595041322314\n","Iteration 19198 - loss value [[328.16918409]] accuracy 0.7851239669421488\n","Iteration 19199 - loss value [[325.54546112]] accuracy 0.78099173553719\n","Iteration 19200 - loss value [[324.66485743]] accuracy 0.7837465564738292\n","Iteration 19201 - loss value [[325.82148158]] accuracy 0.7796143250688705\n","Iteration 19202 - loss value [[329.37624112]] accuracy 0.7837465564738292\n","Iteration 19203 - loss value [[326.19506392]] accuracy 0.7823691460055097\n","Iteration 19204 - loss value [[325.83514623]] accuracy 0.7837465564738292\n","Iteration 19205 - loss value [[325.12266697]] accuracy 0.7823691460055097\n","Iteration 19206 - loss value [[325.93106524]] accuracy 0.7865013774104683\n","Iteration 19207 - loss value [[326.24143007]] accuracy 0.7837465564738292\n","Iteration 19208 - loss value [[325.14548952]] accuracy 0.7837465564738292\n","Iteration 19209 - loss value [[327.38185648]] accuracy 0.778236914600551\n","Iteration 19210 - loss value [[334.66241077]] accuracy 0.7796143250688705\n","Iteration 19211 - loss value [[338.47944503]] accuracy 0.7713498622589532\n","Iteration 19212 - loss value [[330.79716464]] accuracy 0.7823691460055097\n","Iteration 19213 - loss value [[328.94780904]] accuracy 0.7796143250688705\n","Iteration 19214 - loss value [[327.65124257]] accuracy 0.7823691460055097\n","Iteration 19215 - loss value [[326.14459129]] accuracy 0.78099173553719\n","Iteration 19216 - loss value [[325.67227968]] accuracy 0.7823691460055097\n","Iteration 19217 - loss value [[325.42436025]] accuracy 0.7823691460055097\n","Iteration 19218 - loss value [[325.10953811]] accuracy 0.78099173553719\n","Iteration 19219 - loss value [[325.00672803]] accuracy 0.7851239669421488\n","Iteration 19220 - loss value [[324.95090764]] accuracy 0.7823691460055097\n","Iteration 19221 - loss value [[324.69855334]] accuracy 0.7865013774104683\n","Iteration 19222 - loss value [[325.92189443]] accuracy 0.7851239669421488\n","Iteration 19223 - loss value [[325.64383212]] accuracy 0.78099173553719\n","Iteration 19224 - loss value [[328.5682735]] accuracy 0.7865013774104683\n","Iteration 19225 - loss value [[331.95716958]] accuracy 0.7741046831955923\n","Iteration 19226 - loss value [[327.97919109]] accuracy 0.7865013774104683\n","Iteration 19227 - loss value [[328.41182444]] accuracy 0.778236914600551\n","Iteration 19228 - loss value [[329.80344866]] accuracy 0.7837465564738292\n","Iteration 19229 - loss value [[326.90602606]] accuracy 0.7796143250688705\n","Iteration 19230 - loss value [[325.55268969]] accuracy 0.7851239669421488\n","Iteration 19231 - loss value [[325.03707082]] accuracy 0.7837465564738292\n","Iteration 19232 - loss value [[325.14326744]] accuracy 0.7796143250688705\n","Iteration 19233 - loss value [[326.55757761]] accuracy 0.7851239669421488\n","Iteration 19234 - loss value [[325.44188331]] accuracy 0.78099173553719\n","Iteration 19235 - loss value [[324.97304895]] accuracy 0.7823691460055097\n","Iteration 19236 - loss value [[325.71820262]] accuracy 0.7851239669421488\n","Iteration 19237 - loss value [[327.9919365]] accuracy 0.778236914600551\n","Iteration 19238 - loss value [[332.04324966]] accuracy 0.7768595041322314\n","Iteration 19239 - loss value [[328.31448702]] accuracy 0.7823691460055097\n","Iteration 19240 - loss value [[326.95259617]] accuracy 0.7865013774104683\n","Iteration 19241 - loss value [[326.57430576]] accuracy 0.78099173553719\n","Iteration 19242 - loss value [[329.02544131]] accuracy 0.7865013774104683\n","Iteration 19243 - loss value [[331.7786149]] accuracy 0.7741046831955923\n","Iteration 19244 - loss value [[326.58875767]] accuracy 0.7851239669421488\n","Iteration 19245 - loss value [[326.23748835]] accuracy 0.78099173553719\n","Iteration 19246 - loss value [[328.97040679]] accuracy 0.7837465564738292\n","Iteration 19247 - loss value [[326.39142642]] accuracy 0.7796143250688705\n","Iteration 19248 - loss value [[326.28066714]] accuracy 0.7851239669421488\n","Iteration 19249 - loss value [[325.36129961]] accuracy 0.7823691460055097\n","Iteration 19250 - loss value [[326.36106461]] accuracy 0.7851239669421488\n","Iteration 19251 - loss value [[326.51557882]] accuracy 0.7796143250688705\n","Iteration 19252 - loss value [[330.43288352]] accuracy 0.78099173553719\n","Iteration 19253 - loss value [[326.21042865]] accuracy 0.78099173553719\n","Iteration 19254 - loss value [[325.18205516]] accuracy 0.7837465564738292\n","Iteration 19255 - loss value [[326.68879474]] accuracy 0.78099173553719\n","Iteration 19256 - loss value [[329.0517799]] accuracy 0.7837465564738292\n","Iteration 19257 - loss value [[331.92357162]] accuracy 0.7741046831955923\n","Iteration 19258 - loss value [[329.04598726]] accuracy 0.7851239669421488\n","Iteration 19259 - loss value [[331.96430399]] accuracy 0.7741046831955923\n","Iteration 19260 - loss value [[328.72655917]] accuracy 0.7823691460055097\n","Iteration 19261 - loss value [[329.52688583]] accuracy 0.7754820936639119\n","Iteration 19262 - loss value [[333.63035363]] accuracy 0.78099173553719\n","Iteration 19263 - loss value [[337.38738195]] accuracy 0.7713498622589532\n","Iteration 19264 - loss value [[332.82748549]] accuracy 0.7851239669421488\n","Iteration 19265 - loss value [[338.68911321]] accuracy 0.7699724517906336\n","Iteration 19266 - loss value [[335.75948051]] accuracy 0.7768595041322314\n","Iteration 19267 - loss value [[353.00184727]] accuracy 0.7520661157024794\n","Iteration 19268 - loss value [[324.56316796]] accuracy 0.7865013774104683\n","Iteration 19269 - loss value [[324.56811255]] accuracy 0.7851239669421488\n","Iteration 19270 - loss value [[325.96355157]] accuracy 0.778236914600551\n","Iteration 19271 - loss value [[329.90886842]] accuracy 0.7823691460055097\n","Iteration 19272 - loss value [[326.38892284]] accuracy 0.7796143250688705\n","Iteration 19273 - loss value [[325.48886861]] accuracy 0.7851239669421488\n","Iteration 19274 - loss value [[324.9096194]] accuracy 0.7837465564738292\n","Iteration 19275 - loss value [[325.36136683]] accuracy 0.7837465564738292\n","Iteration 19276 - loss value [[327.73311903]] accuracy 0.7837465564738292\n","Iteration 19277 - loss value [[329.16929662]] accuracy 0.7741046831955923\n","Iteration 19278 - loss value [[332.49208344]] accuracy 0.7851239669421488\n","Iteration 19279 - loss value [[337.054817]] accuracy 0.7713498622589532\n","Iteration 19280 - loss value [[333.3334949]] accuracy 0.7851239669421488\n","Iteration 19281 - loss value [[337.40016837]] accuracy 0.7727272727272727\n","Iteration 19282 - loss value [[332.46059233]] accuracy 0.7837465564738292\n","Iteration 19283 - loss value [[338.93445037]] accuracy 0.7713498622589532\n","Iteration 19284 - loss value [[331.59507875]] accuracy 0.7837465564738292\n","Iteration 19285 - loss value [[335.45873604]] accuracy 0.7727272727272727\n","Iteration 19286 - loss value [[332.38966569]] accuracy 0.78099173553719\n","Iteration 19287 - loss value [[332.6269704]] accuracy 0.7741046831955923\n","Iteration 19288 - loss value [[338.78539061]] accuracy 0.7768595041322314\n","Iteration 19289 - loss value [[364.10760386]] accuracy 0.7451790633608816\n","Iteration 19290 - loss value [[324.7238847]] accuracy 0.7851239669421488\n","Iteration 19291 - loss value [[324.98522792]] accuracy 0.7851239669421488\n","Iteration 19292 - loss value [[326.61818016]] accuracy 0.7865013774104683\n","Iteration 19293 - loss value [[325.60510391]] accuracy 0.7837465564738292\n","Iteration 19294 - loss value [[326.91206432]] accuracy 0.7837465564738292\n","Iteration 19295 - loss value [[326.0019858]] accuracy 0.7823691460055097\n","Iteration 19296 - loss value [[325.52502136]] accuracy 0.7851239669421488\n","Iteration 19297 - loss value [[325.15675931]] accuracy 0.7837465564738292\n","Iteration 19298 - loss value [[325.85010418]] accuracy 0.7851239669421488\n","Iteration 19299 - loss value [[325.63148919]] accuracy 0.7823691460055097\n","Iteration 19300 - loss value [[325.63207606]] accuracy 0.7837465564738292\n","Iteration 19301 - loss value [[328.7732149]] accuracy 0.7837465564738292\n","Iteration 19302 - loss value [[331.34418063]] accuracy 0.7727272727272727\n","Iteration 19303 - loss value [[328.24675933]] accuracy 0.7851239669421488\n","Iteration 19304 - loss value [[329.57189905]] accuracy 0.7727272727272727\n","Iteration 19305 - loss value [[331.16173059]] accuracy 0.78099173553719\n","Iteration 19306 - loss value [[335.31757809]] accuracy 0.7727272727272727\n","Iteration 19307 - loss value [[332.52838479]] accuracy 0.7796143250688705\n","Iteration 19308 - loss value [[334.66918969]] accuracy 0.7727272727272727\n","Iteration 19309 - loss value [[336.75304278]] accuracy 0.7796143250688705\n","Iteration 19310 - loss value [[358.09945109]] accuracy 0.7479338842975206\n","Iteration 19311 - loss value [[325.57288415]] accuracy 0.7837465564738292\n","Iteration 19312 - loss value [[328.77249996]] accuracy 0.7768595041322314\n","Iteration 19313 - loss value [[327.17477832]] accuracy 0.7851239669421488\n","Iteration 19314 - loss value [[325.82426393]] accuracy 0.7823691460055097\n","Iteration 19315 - loss value [[328.02491926]] accuracy 0.7837465564738292\n","Iteration 19316 - loss value [[328.04723104]] accuracy 0.7768595041322314\n","Iteration 19317 - loss value [[331.5590695]] accuracy 0.7837465564738292\n","Iteration 19318 - loss value [[336.33303928]] accuracy 0.7713498622589532\n","Iteration 19319 - loss value [[340.08067743]] accuracy 0.7754820936639119\n","Iteration 19320 - loss value [[369.80978824]] accuracy 0.7410468319559229\n","Iteration 19321 - loss value [[326.93481288]] accuracy 0.7837465564738292\n","Iteration 19322 - loss value [[332.09499924]] accuracy 0.7741046831955923\n","Iteration 19323 - loss value [[331.05484163]] accuracy 0.7837465564738292\n","Iteration 19324 - loss value [[335.04270714]] accuracy 0.7768595041322314\n","Iteration 19325 - loss value [[330.12063496]] accuracy 0.7851239669421488\n","Iteration 19326 - loss value [[331.41818685]] accuracy 0.7768595041322314\n","Iteration 19327 - loss value [[328.5119805]] accuracy 0.7837465564738292\n","Iteration 19328 - loss value [[330.20696501]] accuracy 0.7754820936639119\n","Iteration 19329 - loss value [[327.55456052]] accuracy 0.7892561983471075\n","Iteration 19330 - loss value [[327.24915366]] accuracy 0.778236914600551\n","Iteration 19331 - loss value [[333.69773047]] accuracy 0.7823691460055097\n","Iteration 19332 - loss value [[338.05865216]] accuracy 0.7713498622589532\n","Iteration 19333 - loss value [[333.18911712]] accuracy 0.78099173553719\n","Iteration 19334 - loss value [[338.88467631]] accuracy 0.7727272727272727\n","Iteration 19335 - loss value [[333.66607768]] accuracy 0.7837465564738292\n","Iteration 19336 - loss value [[337.83054867]] accuracy 0.7727272727272727\n","Iteration 19337 - loss value [[330.99191173]] accuracy 0.7878787878787878\n","Iteration 19338 - loss value [[330.27452422]] accuracy 0.778236914600551\n","Iteration 19339 - loss value [[337.09997885]] accuracy 0.7851239669421488\n","Iteration 19340 - loss value [[353.65564596]] accuracy 0.7534435261707989\n","Iteration 19341 - loss value [[324.54169532]] accuracy 0.7851239669421488\n","Iteration 19342 - loss value [[324.3415972]] accuracy 0.7865013774104683\n","Iteration 19343 - loss value [[324.79262236]] accuracy 0.7851239669421488\n","Iteration 19344 - loss value [[326.92893985]] accuracy 0.778236914600551\n","Iteration 19345 - loss value [[334.70424769]] accuracy 0.7796143250688705\n","Iteration 19346 - loss value [[341.62370041]] accuracy 0.7727272727272727\n","Iteration 19347 - loss value [[332.5659938]] accuracy 0.7768595041322314\n","Iteration 19348 - loss value [[333.85515402]] accuracy 0.7741046831955923\n","Iteration 19349 - loss value [[338.47598029]] accuracy 0.7741046831955923\n","Iteration 19350 - loss value [[366.47609315]] accuracy 0.7355371900826446\n","Iteration 19351 - loss value [[324.55831424]] accuracy 0.7837465564738292\n","Iteration 19352 - loss value [[324.77270768]] accuracy 0.7851239669421488\n","Iteration 19353 - loss value [[325.25945374]] accuracy 0.78099173553719\n","Iteration 19354 - loss value [[325.44325963]] accuracy 0.7823691460055097\n","Iteration 19355 - loss value [[328.60835391]] accuracy 0.7823691460055097\n","Iteration 19356 - loss value [[331.56694451]] accuracy 0.7741046831955923\n","Iteration 19357 - loss value [[333.37005024]] accuracy 0.778236914600551\n","Iteration 19358 - loss value [[335.42146136]] accuracy 0.7741046831955923\n","Iteration 19359 - loss value [[328.73369129]] accuracy 0.7837465564738292\n","Iteration 19360 - loss value [[331.63857708]] accuracy 0.7727272727272727\n","Iteration 19361 - loss value [[329.45118599]] accuracy 0.7823691460055097\n","Iteration 19362 - loss value [[326.1129983]] accuracy 0.7796143250688705\n","Iteration 19363 - loss value [[325.35248571]] accuracy 0.7837465564738292\n","Iteration 19364 - loss value [[324.83178223]] accuracy 0.7837465564738292\n","Iteration 19365 - loss value [[324.9983471]] accuracy 0.7837465564738292\n","Iteration 19366 - loss value [[326.25608057]] accuracy 0.7851239669421488\n","Iteration 19367 - loss value [[325.36518737]] accuracy 0.7823691460055097\n","Iteration 19368 - loss value [[325.94315257]] accuracy 0.7837465564738292\n","Iteration 19369 - loss value [[325.3480921]] accuracy 0.7837465564738292\n","Iteration 19370 - loss value [[324.96112783]] accuracy 0.7823691460055097\n","Iteration 19371 - loss value [[325.77417279]] accuracy 0.7865013774104683\n","Iteration 19372 - loss value [[328.81736667]] accuracy 0.7768595041322314\n","Iteration 19373 - loss value [[333.00372697]] accuracy 0.778236914600551\n","Iteration 19374 - loss value [[330.76523948]] accuracy 0.778236914600551\n","Iteration 19375 - loss value [[333.89212817]] accuracy 0.7796143250688705\n","Iteration 19376 - loss value [[343.85144491]] accuracy 0.7644628099173554\n","Iteration 19377 - loss value [[347.74617056]] accuracy 0.7713498622589532\n","Iteration 19378 - loss value [[379.21601968]] accuracy 0.7327823691460055\n","Iteration 19379 - loss value [[339.55748584]] accuracy 0.7796143250688705\n","Iteration 19380 - loss value [[348.62645652]] accuracy 0.756198347107438\n","Iteration 19381 - loss value [[327.16461223]] accuracy 0.7768595041322314\n","Iteration 19382 - loss value [[325.49410728]] accuracy 0.7851239669421488\n","Iteration 19383 - loss value [[324.86169154]] accuracy 0.7865013774104683\n","Iteration 19384 - loss value [[326.10098838]] accuracy 0.7851239669421488\n","Iteration 19385 - loss value [[324.55023388]] accuracy 0.7851239669421488\n","Iteration 19386 - loss value [[324.25674673]] accuracy 0.7865013774104683\n","Iteration 19387 - loss value [[324.36991612]] accuracy 0.7878787878787878\n","Iteration 19388 - loss value [[325.24423783]] accuracy 0.7837465564738292\n","Iteration 19389 - loss value [[327.58178097]] accuracy 0.7823691460055097\n","Iteration 19390 - loss value [[328.87156814]] accuracy 0.7754820936639119\n","Iteration 19391 - loss value [[331.43929771]] accuracy 0.7851239669421488\n","Iteration 19392 - loss value [[330.47714793]] accuracy 0.7768595041322314\n","Iteration 19393 - loss value [[341.58116446]] accuracy 0.7768595041322314\n","Iteration 19394 - loss value [[367.21746155]] accuracy 0.743801652892562\n","Iteration 19395 - loss value [[324.89622855]] accuracy 0.7851239669421488\n","Iteration 19396 - loss value [[325.5565907]] accuracy 0.7837465564738292\n","Iteration 19397 - loss value [[326.96928986]] accuracy 0.7796143250688705\n","Iteration 19398 - loss value [[332.84497705]] accuracy 0.7823691460055097\n","Iteration 19399 - loss value [[334.599706]] accuracy 0.7754820936639119\n","Iteration 19400 - loss value [[332.90086401]] accuracy 0.7851239669421488\n","Iteration 19401 - loss value [[336.38012435]] accuracy 0.7741046831955923\n","Iteration 19402 - loss value [[328.90542597]] accuracy 0.78099173553719\n","Iteration 19403 - loss value [[325.70729015]] accuracy 0.78099173553719\n","Iteration 19404 - loss value [[325.17146766]] accuracy 0.7837465564738292\n","Iteration 19405 - loss value [[324.71031183]] accuracy 0.7837465564738292\n","Iteration 19406 - loss value [[325.31897261]] accuracy 0.7837465564738292\n","Iteration 19407 - loss value [[326.6197085]] accuracy 0.7851239669421488\n","Iteration 19408 - loss value [[327.62539414]] accuracy 0.7768595041322314\n","Iteration 19409 - loss value [[332.44450225]] accuracy 0.78099173553719\n","Iteration 19410 - loss value [[329.56362424]] accuracy 0.78099173553719\n","Iteration 19411 - loss value [[327.89787993]] accuracy 0.78099173553719\n","Iteration 19412 - loss value [[326.14109754]] accuracy 0.7796143250688705\n","Iteration 19413 - loss value [[325.47976291]] accuracy 0.7837465564738292\n","Iteration 19414 - loss value [[324.8815455]] accuracy 0.7823691460055097\n","Iteration 19415 - loss value [[324.62790011]] accuracy 0.7865013774104683\n","Iteration 19416 - loss value [[325.85237081]] accuracy 0.7823691460055097\n","Iteration 19417 - loss value [[327.26585629]] accuracy 0.7754820936639119\n","Iteration 19418 - loss value [[332.12646808]] accuracy 0.7796143250688705\n","Iteration 19419 - loss value [[329.7218193]] accuracy 0.7796143250688705\n","Iteration 19420 - loss value [[335.16431209]] accuracy 0.78099173553719\n","Iteration 19421 - loss value [[345.66999092]] accuracy 0.7644628099173554\n","Iteration 19422 - loss value [[354.77679333]] accuracy 0.768595041322314\n","Iteration 19423 - loss value [[399.45248798]] accuracy 0.7038567493112947\n","Iteration 19424 - loss value [[336.72672911]] accuracy 0.768595041322314\n","Iteration 19425 - loss value [[336.6217561]] accuracy 0.7713498622589532\n","Iteration 19426 - loss value [[339.57843252]] accuracy 0.7658402203856749\n","Iteration 19427 - loss value [[333.60026327]] accuracy 0.7768595041322314\n","Iteration 19428 - loss value [[333.41772378]] accuracy 0.7727272727272727\n","Iteration 19429 - loss value [[343.93608004]] accuracy 0.7672176308539945\n","Iteration 19430 - loss value [[380.10350755]] accuracy 0.7300275482093664\n","Iteration 19431 - loss value [[329.97822772]] accuracy 0.7796143250688705\n","Iteration 19432 - loss value [[326.14288924]] accuracy 0.78099173553719\n","Iteration 19433 - loss value [[325.90716035]] accuracy 0.7837465564738292\n","Iteration 19434 - loss value [[325.39036581]] accuracy 0.7837465564738292\n","Iteration 19435 - loss value [[325.54337835]] accuracy 0.7851239669421488\n","Iteration 19436 - loss value [[324.99113958]] accuracy 0.7837465564738292\n","Iteration 19437 - loss value [[324.94909719]] accuracy 0.7851239669421488\n","Iteration 19438 - loss value [[325.16593079]] accuracy 0.7837465564738292\n","Iteration 19439 - loss value [[326.4102788]] accuracy 0.7823691460055097\n","Iteration 19440 - loss value [[326.67492066]] accuracy 0.78099173553719\n","Iteration 19441 - loss value [[330.18469963]] accuracy 0.7796143250688705\n","Iteration 19442 - loss value [[327.27702076]] accuracy 0.7823691460055097\n","Iteration 19443 - loss value [[325.27150983]] accuracy 0.7837465564738292\n","Iteration 19444 - loss value [[324.5909267]] accuracy 0.7837465564738292\n","Iteration 19445 - loss value [[324.369945]] accuracy 0.7865013774104683\n","Iteration 19446 - loss value [[324.72964987]] accuracy 0.7851239669421488\n","Iteration 19447 - loss value [[325.65937752]] accuracy 0.78099173553719\n","Iteration 19448 - loss value [[328.77237367]] accuracy 0.7837465564738292\n","Iteration 19449 - loss value [[331.87566134]] accuracy 0.7754820936639119\n","Iteration 19450 - loss value [[329.80097873]] accuracy 0.7823691460055097\n","Iteration 19451 - loss value [[330.89784018]] accuracy 0.7768595041322314\n","Iteration 19452 - loss value [[329.96622059]] accuracy 0.7837465564738292\n","Iteration 19453 - loss value [[333.77602301]] accuracy 0.7727272727272727\n","Iteration 19454 - loss value [[328.9605848]] accuracy 0.7837465564738292\n","Iteration 19455 - loss value [[331.03747499]] accuracy 0.7727272727272727\n","Iteration 19456 - loss value [[328.9095198]] accuracy 0.7851239669421488\n","Iteration 19457 - loss value [[330.41451766]] accuracy 0.7727272727272727\n","Iteration 19458 - loss value [[330.3247852]] accuracy 0.7823691460055097\n","Iteration 19459 - loss value [[332.99942962]] accuracy 0.7754820936639119\n","Iteration 19460 - loss value [[329.28947492]] accuracy 0.7823691460055097\n","Iteration 19461 - loss value [[329.76804688]] accuracy 0.7741046831955923\n","Iteration 19462 - loss value [[327.92853345]] accuracy 0.78099173553719\n","Iteration 19463 - loss value [[326.27439762]] accuracy 0.7796143250688705\n","Iteration 19464 - loss value [[331.53673955]] accuracy 0.7823691460055097\n","Iteration 19465 - loss value [[332.7797182]] accuracy 0.7741046831955923\n","Iteration 19466 - loss value [[332.0661803]] accuracy 0.7823691460055097\n","Iteration 19467 - loss value [[337.35664457]] accuracy 0.768595041322314\n","Iteration 19468 - loss value [[332.03213939]] accuracy 0.7796143250688705\n","Iteration 19469 - loss value [[340.21477192]] accuracy 0.768595041322314\n","Iteration 19470 - loss value [[340.90256745]] accuracy 0.7768595041322314\n","Iteration 19471 - loss value [[363.93370739]] accuracy 0.7465564738292011\n","Iteration 19472 - loss value [[325.90841535]] accuracy 0.7851239669421488\n","Iteration 19473 - loss value [[324.51203456]] accuracy 0.7851239669421488\n","Iteration 19474 - loss value [[324.90332988]] accuracy 0.7851239669421488\n","Iteration 19475 - loss value [[325.66810354]] accuracy 0.78099173553719\n","Iteration 19476 - loss value [[327.0623825]] accuracy 0.78099173553719\n","Iteration 19477 - loss value [[328.52453754]] accuracy 0.7823691460055097\n","Iteration 19478 - loss value [[329.78461695]] accuracy 0.7754820936639119\n","Iteration 19479 - loss value [[329.92994198]] accuracy 0.78099173553719\n","Iteration 19480 - loss value [[327.29345409]] accuracy 0.7796143250688705\n","Iteration 19481 - loss value [[325.38269424]] accuracy 0.78099173553719\n","Iteration 19482 - loss value [[325.23212461]] accuracy 0.7837465564738292\n","Iteration 19483 - loss value [[324.94408243]] accuracy 0.7851239669421488\n","Iteration 19484 - loss value [[325.60407798]] accuracy 0.78099173553719\n","Iteration 19485 - loss value [[329.49205925]] accuracy 0.7823691460055097\n","Iteration 19486 - loss value [[333.29294752]] accuracy 0.7741046831955923\n","Iteration 19487 - loss value [[327.14619397]] accuracy 0.7837465564738292\n","Iteration 19488 - loss value [[327.06281202]] accuracy 0.778236914600551\n","Iteration 19489 - loss value [[331.24254549]] accuracy 0.7837465564738292\n","Iteration 19490 - loss value [[328.89813119]] accuracy 0.7796143250688705\n","Iteration 19491 - loss value [[331.06058897]] accuracy 0.7851239669421488\n","Iteration 19492 - loss value [[330.64337365]] accuracy 0.7754820936639119\n","Iteration 19493 - loss value [[333.54739733]] accuracy 0.7837465564738292\n","Iteration 19494 - loss value [[346.82674284]] accuracy 0.7603305785123967\n","Iteration 19495 - loss value [[355.61645971]] accuracy 0.768595041322314\n","Iteration 19496 - loss value [[397.53616174]] accuracy 0.7052341597796143\n","Iteration 19497 - loss value [[333.54358485]] accuracy 0.7699724517906336\n","Iteration 19498 - loss value [[326.93943196]] accuracy 0.7796143250688705\n","Iteration 19499 - loss value [[327.10494726]] accuracy 0.78099173553719\n","Iteration 19500 - loss value [[329.30531214]] accuracy 0.7837465564738292\n","Iteration 19501 - loss value [[332.53511353]] accuracy 0.7727272727272727\n","Iteration 19502 - loss value [[331.50699276]] accuracy 0.7796143250688705\n","Iteration 19503 - loss value [[328.68086242]] accuracy 0.7796143250688705\n","Iteration 19504 - loss value [[331.5897848]] accuracy 0.7851239669421488\n","Iteration 19505 - loss value [[331.91800932]] accuracy 0.7768595041322314\n","Iteration 19506 - loss value [[341.63717055]] accuracy 0.7727272727272727\n","Iteration 19507 - loss value [[361.25487246]] accuracy 0.7451790633608816\n","Iteration 19508 - loss value [[326.72904737]] accuracy 0.7837465564738292\n","Iteration 19509 - loss value [[329.87429668]] accuracy 0.7741046831955923\n","Iteration 19510 - loss value [[327.56242664]] accuracy 0.7865013774104683\n","Iteration 19511 - loss value [[326.28617754]] accuracy 0.78099173553719\n","Iteration 19512 - loss value [[327.82042829]] accuracy 0.7796143250688705\n","Iteration 19513 - loss value [[326.79964221]] accuracy 0.7823691460055097\n","Iteration 19514 - loss value [[327.35898735]] accuracy 0.78099173553719\n","Iteration 19515 - loss value [[326.31454775]] accuracy 0.7823691460055097\n","Iteration 19516 - loss value [[325.03479411]] accuracy 0.7851239669421488\n","Iteration 19517 - loss value [[325.55244278]] accuracy 0.7796143250688705\n","Iteration 19518 - loss value [[324.31707796]] accuracy 0.7878787878787878\n","Iteration 19519 - loss value [[324.33078583]] accuracy 0.7865013774104683\n","Iteration 19520 - loss value [[324.75915083]] accuracy 0.7837465564738292\n","Iteration 19521 - loss value [[325.49034894]] accuracy 0.7851239669421488\n","Iteration 19522 - loss value [[327.42326278]] accuracy 0.778236914600551\n","Iteration 19523 - loss value [[329.15951899]] accuracy 0.78099173553719\n","Iteration 19524 - loss value [[329.1285462]] accuracy 0.7754820936639119\n","Iteration 19525 - loss value [[328.0976618]] accuracy 0.7865013774104683\n","Iteration 19526 - loss value [[325.62107472]] accuracy 0.7837465564738292\n","Iteration 19527 - loss value [[325.79259729]] accuracy 0.7823691460055097\n","Iteration 19528 - loss value [[325.76985354]] accuracy 0.78099173553719\n","Iteration 19529 - loss value [[325.21422407]] accuracy 0.78099173553719\n","Iteration 19530 - loss value [[324.51415246]] accuracy 0.7851239669421488\n","Iteration 19531 - loss value [[324.72446237]] accuracy 0.7823691460055097\n","Iteration 19532 - loss value [[325.50755211]] accuracy 0.7796143250688705\n","Iteration 19533 - loss value [[327.35645817]] accuracy 0.7837465564738292\n","Iteration 19534 - loss value [[327.31683158]] accuracy 0.7796143250688705\n","Iteration 19535 - loss value [[330.96548244]] accuracy 0.7796143250688705\n","Iteration 19536 - loss value [[329.21007234]] accuracy 0.7823691460055097\n","Iteration 19537 - loss value [[327.42640378]] accuracy 0.7823691460055097\n","Iteration 19538 - loss value [[325.77452584]] accuracy 0.78099173553719\n","Iteration 19539 - loss value [[325.0235394]] accuracy 0.7823691460055097\n","Iteration 19540 - loss value [[324.50662957]] accuracy 0.7865013774104683\n","Iteration 19541 - loss value [[324.94073462]] accuracy 0.7851239669421488\n","Iteration 19542 - loss value [[326.81646673]] accuracy 0.7837465564738292\n","Iteration 19543 - loss value [[326.18638755]] accuracy 0.7823691460055097\n","Iteration 19544 - loss value [[325.83835382]] accuracy 0.7823691460055097\n","Iteration 19545 - loss value [[325.49883596]] accuracy 0.7837465564738292\n","Iteration 19546 - loss value [[326.95807164]] accuracy 0.7837465564738292\n","Iteration 19547 - loss value [[325.6705572]] accuracy 0.7837465564738292\n","Iteration 19548 - loss value [[326.22764248]] accuracy 0.7837465564738292\n","Iteration 19549 - loss value [[325.96440057]] accuracy 0.7823691460055097\n","Iteration 19550 - loss value [[325.52285685]] accuracy 0.7851239669421488\n","Iteration 19551 - loss value [[325.8066073]] accuracy 0.778236914600551\n","Iteration 19552 - loss value [[328.3238916]] accuracy 0.7851239669421488\n","Iteration 19553 - loss value [[327.16415448]] accuracy 0.7768595041322314\n","Iteration 19554 - loss value [[331.67189167]] accuracy 0.78099173553719\n","Iteration 19555 - loss value [[330.54739272]] accuracy 0.7754820936639119\n","Iteration 19556 - loss value [[332.92846533]] accuracy 0.7823691460055097\n","Iteration 19557 - loss value [[341.89461284]] accuracy 0.7658402203856749\n","Iteration 19558 - loss value [[334.27380113]] accuracy 0.7796143250688705\n","Iteration 19559 - loss value [[337.46002821]] accuracy 0.7699724517906336\n","Iteration 19560 - loss value [[338.89195497]] accuracy 0.7727272727272727\n","Iteration 19561 - loss value [[362.315968]] accuracy 0.7451790633608816\n","Iteration 19562 - loss value [[324.85313599]] accuracy 0.7837465564738292\n","Iteration 19563 - loss value [[325.10819587]] accuracy 0.78099173553719\n","Iteration 19564 - loss value [[324.75931904]] accuracy 0.7837465564738292\n","Iteration 19565 - loss value [[326.08722056]] accuracy 0.7851239669421488\n","Iteration 19566 - loss value [[325.47317245]] accuracy 0.78099173553719\n","Iteration 19567 - loss value [[327.13532947]] accuracy 0.7823691460055097\n","Iteration 19568 - loss value [[327.38960545]] accuracy 0.7796143250688705\n","Iteration 19569 - loss value [[328.74400777]] accuracy 0.78099173553719\n","Iteration 19570 - loss value [[326.11388494]] accuracy 0.7796143250688705\n","Iteration 19571 - loss value [[325.10186749]] accuracy 0.7837465564738292\n","Iteration 19572 - loss value [[325.5390329]] accuracy 0.7796143250688705\n","Iteration 19573 - loss value [[328.66628075]] accuracy 0.78099173553719\n","Iteration 19574 - loss value [[327.85383978]] accuracy 0.7754820936639119\n","Iteration 19575 - loss value [[331.69219124]] accuracy 0.7837465564738292\n","Iteration 19576 - loss value [[337.09740462]] accuracy 0.7727272727272727\n","Iteration 19577 - loss value [[332.42193452]] accuracy 0.7851239669421488\n","Iteration 19578 - loss value [[338.80872899]] accuracy 0.7713498622589532\n","Iteration 19579 - loss value [[329.29695248]] accuracy 0.7837465564738292\n","Iteration 19580 - loss value [[332.03446833]] accuracy 0.7741046831955923\n","Iteration 19581 - loss value [[330.03714827]] accuracy 0.7823691460055097\n","Iteration 19582 - loss value [[327.35941311]] accuracy 0.7796143250688705\n","Iteration 19583 - loss value [[326.08747736]] accuracy 0.7865013774104683\n","Iteration 19584 - loss value [[325.00977582]] accuracy 0.7851239669421488\n","Iteration 19585 - loss value [[325.57109913]] accuracy 0.7851239669421488\n","Iteration 19586 - loss value [[325.49695103]] accuracy 0.7823691460055097\n","Iteration 19587 - loss value [[326.66528987]] accuracy 0.7851239669421488\n","Iteration 19588 - loss value [[327.81243515]] accuracy 0.7768595041322314\n","Iteration 19589 - loss value [[333.25597295]] accuracy 0.778236914600551\n","Iteration 19590 - loss value [[331.65800728]] accuracy 0.7768595041322314\n","Iteration 19591 - loss value [[333.75067186]] accuracy 0.78099173553719\n","Iteration 19592 - loss value [[346.33172793]] accuracy 0.7603305785123967\n","Iteration 19593 - loss value [[334.62022658]] accuracy 0.7837465564738292\n","Iteration 19594 - loss value [[337.41175174]] accuracy 0.768595041322314\n","Iteration 19595 - loss value [[333.88126837]] accuracy 0.7837465564738292\n","Iteration 19596 - loss value [[342.7703312]] accuracy 0.7672176308539945\n","Iteration 19597 - loss value [[336.86340682]] accuracy 0.78099173553719\n","Iteration 19598 - loss value [[352.11278453]] accuracy 0.756198347107438\n","Iteration 19599 - loss value [[324.6216151]] accuracy 0.7851239669421488\n","Iteration 19600 - loss value [[324.16342756]] accuracy 0.7878787878787878\n","Iteration 19601 - loss value [[324.16506123]] accuracy 0.7865013774104683\n","Iteration 19602 - loss value [[324.18206089]] accuracy 0.7878787878787878\n","Iteration 19603 - loss value [[324.26722705]] accuracy 0.7865013774104683\n","Iteration 19604 - loss value [[324.61565685]] accuracy 0.7865013774104683\n","Iteration 19605 - loss value [[326.15678516]] accuracy 0.7851239669421488\n","Iteration 19606 - loss value [[326.15253763]] accuracy 0.78099173553719\n","Iteration 19607 - loss value [[330.6800551]] accuracy 0.7796143250688705\n","Iteration 19608 - loss value [[326.83233138]] accuracy 0.7796143250688705\n","Iteration 19609 - loss value [[324.86875281]] accuracy 0.7823691460055097\n","Iteration 19610 - loss value [[325.46481813]] accuracy 0.78099173553719\n","Iteration 19611 - loss value [[327.01973759]] accuracy 0.7837465564738292\n","Iteration 19612 - loss value [[325.78247756]] accuracy 0.7823691460055097\n","Iteration 19613 - loss value [[327.10212428]] accuracy 0.7837465564738292\n","Iteration 19614 - loss value [[325.19385116]] accuracy 0.7837465564738292\n","Iteration 19615 - loss value [[325.60434683]] accuracy 0.78099173553719\n","Iteration 19616 - loss value [[326.45352747]] accuracy 0.78099173553719\n","Iteration 19617 - loss value [[329.38998169]] accuracy 0.7837465564738292\n","Iteration 19618 - loss value [[332.12465209]] accuracy 0.7727272727272727\n","Iteration 19619 - loss value [[329.49309586]] accuracy 0.7837465564738292\n","Iteration 19620 - loss value [[330.82866851]] accuracy 0.7768595041322314\n","Iteration 19621 - loss value [[332.08478775]] accuracy 0.7796143250688705\n","Iteration 19622 - loss value [[330.83344804]] accuracy 0.78099173553719\n","Iteration 19623 - loss value [[332.6866455]] accuracy 0.7823691460055097\n","Iteration 19624 - loss value [[338.03624542]] accuracy 0.7713498622589532\n","Iteration 19625 - loss value [[330.92498643]] accuracy 0.7823691460055097\n","Iteration 19626 - loss value [[327.97412326]] accuracy 0.7823691460055097\n","Iteration 19627 - loss value [[325.85959386]] accuracy 0.7837465564738292\n","Iteration 19628 - loss value [[325.27702507]] accuracy 0.7837465564738292\n","Iteration 19629 - loss value [[324.96231632]] accuracy 0.7823691460055097\n","Iteration 19630 - loss value [[325.17283479]] accuracy 0.7851239669421488\n","Iteration 19631 - loss value [[326.13673764]] accuracy 0.78099173553719\n","Iteration 19632 - loss value [[329.14971523]] accuracy 0.7768595041322314\n","Iteration 19633 - loss value [[327.56171892]] accuracy 0.7823691460055097\n","Iteration 19634 - loss value [[326.31280306]] accuracy 0.78099173553719\n","Iteration 19635 - loss value [[329.61699657]] accuracy 0.78099173553719\n","Iteration 19636 - loss value [[327.97460335]] accuracy 0.778236914600551\n","Iteration 19637 - loss value [[328.991703]] accuracy 0.7851239669421488\n","Iteration 19638 - loss value [[332.30908705]] accuracy 0.7741046831955923\n","Iteration 19639 - loss value [[327.79989109]] accuracy 0.7851239669421488\n","Iteration 19640 - loss value [[328.14603756]] accuracy 0.7768595041322314\n","Iteration 19641 - loss value [[331.96346879]] accuracy 0.7823691460055097\n","Iteration 19642 - loss value [[337.53694407]] accuracy 0.7699724517906336\n","Iteration 19643 - loss value [[332.45084673]] accuracy 0.778236914600551\n","Iteration 19644 - loss value [[333.57129859]] accuracy 0.7741046831955923\n","Iteration 19645 - loss value [[336.6648738]] accuracy 0.7796143250688705\n","Iteration 19646 - loss value [[356.62429181]] accuracy 0.7479338842975206\n","Iteration 19647 - loss value [[324.60992346]] accuracy 0.7837465564738292\n","Iteration 19648 - loss value [[326.28183788]] accuracy 0.7823691460055097\n","Iteration 19649 - loss value [[331.48238573]] accuracy 0.7851239669421488\n","Iteration 19650 - loss value [[335.20762064]] accuracy 0.7754820936639119\n","Iteration 19651 - loss value [[331.30499086]] accuracy 0.7823691460055097\n","Iteration 19652 - loss value [[330.45229948]] accuracy 0.778236914600551\n","Iteration 19653 - loss value [[338.50521339]] accuracy 0.78099173553719\n","Iteration 19654 - loss value [[358.42661327]] accuracy 0.7506887052341598\n","Iteration 19655 - loss value [[325.27879022]] accuracy 0.7823691460055097\n","Iteration 19656 - loss value [[326.63880596]] accuracy 0.778236914600551\n","Iteration 19657 - loss value [[330.84014193]] accuracy 0.78099173553719\n","Iteration 19658 - loss value [[326.51743807]] accuracy 0.778236914600551\n","Iteration 19659 - loss value [[324.84434846]] accuracy 0.7837465564738292\n","Iteration 19660 - loss value [[324.25985637]] accuracy 0.7851239669421488\n","Iteration 19661 - loss value [[324.25224459]] accuracy 0.7865013774104683\n","Iteration 19662 - loss value [[324.66863805]] accuracy 0.7851239669421488\n","Iteration 19663 - loss value [[326.67153026]] accuracy 0.778236914600551\n","Iteration 19664 - loss value [[333.25706707]] accuracy 0.7837465564738292\n","Iteration 19665 - loss value [[337.58357325]] accuracy 0.7727272727272727\n","Iteration 19666 - loss value [[340.6347151]] accuracy 0.7796143250688705\n","Iteration 19667 - loss value [[360.00799974]] accuracy 0.7479338842975206\n","Iteration 19668 - loss value [[325.46210824]] accuracy 0.7851239669421488\n","Iteration 19669 - loss value [[324.20218651]] accuracy 0.7865013774104683\n","Iteration 19670 - loss value [[324.18835096]] accuracy 0.7878787878787878\n","Iteration 19671 - loss value [[324.30094052]] accuracy 0.7865013774104683\n","Iteration 19672 - loss value [[325.03033896]] accuracy 0.7851239669421488\n","Iteration 19673 - loss value [[327.86202999]] accuracy 0.7768595041322314\n","Iteration 19674 - loss value [[330.82981196]] accuracy 0.7823691460055097\n","Iteration 19675 - loss value [[335.0913567]] accuracy 0.7727272727272727\n","Iteration 19676 - loss value [[330.79429498]] accuracy 0.7823691460055097\n","Iteration 19677 - loss value [[328.7089481]] accuracy 0.7796143250688705\n","Iteration 19678 - loss value [[332.33007177]] accuracy 0.7837465564738292\n","Iteration 19679 - loss value [[336.69916999]] accuracy 0.7713498622589532\n","Iteration 19680 - loss value [[336.26602434]] accuracy 0.78099173553719\n","Iteration 19681 - loss value [[352.63937373]] accuracy 0.756198347107438\n","Iteration 19682 - loss value [[324.45364216]] accuracy 0.7865013774104683\n","Iteration 19683 - loss value [[324.83947346]] accuracy 0.7851239669421488\n","Iteration 19684 - loss value [[326.72092827]] accuracy 0.78099173553719\n","Iteration 19685 - loss value [[329.64244742]] accuracy 0.7837465564738292\n","Iteration 19686 - loss value [[326.42454208]] accuracy 0.7796143250688705\n","Iteration 19687 - loss value [[325.95763861]] accuracy 0.7851239669421488\n","Iteration 19688 - loss value [[325.69436457]] accuracy 0.7837465564738292\n","Iteration 19689 - loss value [[325.34325875]] accuracy 0.7823691460055097\n","Iteration 19690 - loss value [[325.18252127]] accuracy 0.7837465564738292\n","Iteration 19691 - loss value [[326.51832836]] accuracy 0.7851239669421488\n","Iteration 19692 - loss value [[326.85538581]] accuracy 0.7823691460055097\n","Iteration 19693 - loss value [[327.42353074]] accuracy 0.7851239669421488\n","Iteration 19694 - loss value [[327.57298475]] accuracy 0.7768595041322314\n","Iteration 19695 - loss value [[332.79559435]] accuracy 0.78099173553719\n","Iteration 19696 - loss value [[332.63094817]] accuracy 0.7768595041322314\n","Iteration 19697 - loss value [[333.35152647]] accuracy 0.78099173553719\n","Iteration 19698 - loss value [[335.43446994]] accuracy 0.7727272727272727\n","Iteration 19699 - loss value [[332.24050797]] accuracy 0.7837465564738292\n","Iteration 19700 - loss value [[339.16109457]] accuracy 0.7713498622589532\n","Iteration 19701 - loss value [[337.238069]] accuracy 0.7796143250688705\n","Iteration 19702 - loss value [[351.68146417]] accuracy 0.7575757575757576\n","Iteration 19703 - loss value [[325.00020997]] accuracy 0.7837465564738292\n","Iteration 19704 - loss value [[326.82039451]] accuracy 0.778236914600551\n","Iteration 19705 - loss value [[332.21136761]] accuracy 0.7796143250688705\n","Iteration 19706 - loss value [[332.40034407]] accuracy 0.7768595041322314\n","Iteration 19707 - loss value [[330.11869806]] accuracy 0.7837465564738292\n","Iteration 19708 - loss value [[329.56835241]] accuracy 0.7768595041322314\n","Iteration 19709 - loss value [[332.70586189]] accuracy 0.78099173553719\n","Iteration 19710 - loss value [[342.90623428]] accuracy 0.7644628099173554\n","Iteration 19711 - loss value [[333.12533459]] accuracy 0.7796143250688705\n","Iteration 19712 - loss value [[333.13464087]] accuracy 0.7741046831955923\n","Iteration 19713 - loss value [[335.83502045]] accuracy 0.778236914600551\n","Iteration 19714 - loss value [[354.61505476]] accuracy 0.7506887052341598\n","Iteration 19715 - loss value [[324.70609178]] accuracy 0.7851239669421488\n","Iteration 19716 - loss value [[325.34689359]] accuracy 0.7837465564738292\n","Iteration 19717 - loss value [[326.50949138]] accuracy 0.7851239669421488\n","Iteration 19718 - loss value [[326.87247646]] accuracy 0.7796143250688705\n","Iteration 19719 - loss value [[331.74217672]] accuracy 0.778236914600551\n","Iteration 19720 - loss value [[329.53860318]] accuracy 0.778236914600551\n","Iteration 19721 - loss value [[327.81161646]] accuracy 0.78099173553719\n","Iteration 19722 - loss value [[325.91133775]] accuracy 0.7796143250688705\n","Iteration 19723 - loss value [[325.48275536]] accuracy 0.78099173553719\n","Iteration 19724 - loss value [[324.86510487]] accuracy 0.7837465564738292\n","Iteration 19725 - loss value [[324.4187009]] accuracy 0.7865013774104683\n","Iteration 19726 - loss value [[324.29801014]] accuracy 0.7878787878787878\n","Iteration 19727 - loss value [[324.69237677]] accuracy 0.7865013774104683\n","Iteration 19728 - loss value [[326.30841103]] accuracy 0.7851239669421488\n","Iteration 19729 - loss value [[326.93621143]] accuracy 0.78099173553719\n","Iteration 19730 - loss value [[330.72661604]] accuracy 0.78099173553719\n","Iteration 19731 - loss value [[327.77711273]] accuracy 0.7823691460055097\n","Iteration 19732 - loss value [[326.87151474]] accuracy 0.7865013774104683\n","Iteration 19733 - loss value [[326.87162245]] accuracy 0.7796143250688705\n","Iteration 19734 - loss value [[328.68782524]] accuracy 0.7823691460055097\n","Iteration 19735 - loss value [[325.9774891]] accuracy 0.7796143250688705\n","Iteration 19736 - loss value [[325.13434141]] accuracy 0.78099173553719\n","Iteration 19737 - loss value [[324.78247484]] accuracy 0.7837465564738292\n","Iteration 19738 - loss value [[325.03248385]] accuracy 0.7851239669421488\n","Iteration 19739 - loss value [[325.6576406]] accuracy 0.7837465564738292\n","Iteration 19740 - loss value [[324.22900781]] accuracy 0.7878787878787878\n","Iteration 19741 - loss value [[324.14992404]] accuracy 0.7878787878787878\n","Iteration 19742 - loss value [[324.15957476]] accuracy 0.7865013774104683\n","Iteration 19743 - loss value [[324.2155737]] accuracy 0.7865013774104683\n","Iteration 19744 - loss value [[324.51384457]] accuracy 0.7851239669421488\n","Iteration 19745 - loss value [[326.06670634]] accuracy 0.7796143250688705\n","Iteration 19746 - loss value [[330.68934436]] accuracy 0.7851239669421488\n","Iteration 19747 - loss value [[329.10169706]] accuracy 0.778236914600551\n","Iteration 19748 - loss value [[334.50330221]] accuracy 0.7837465564738292\n","Iteration 19749 - loss value [[349.89260053]] accuracy 0.756198347107438\n","Iteration 19750 - loss value [[330.25935931]] accuracy 0.7865013774104683\n","Iteration 19751 - loss value [[334.15429328]] accuracy 0.7768595041322314\n","Iteration 19752 - loss value [[330.20722062]] accuracy 0.7851239669421488\n","Iteration 19753 - loss value [[334.23170879]] accuracy 0.778236914600551\n","Iteration 19754 - loss value [[333.09264637]] accuracy 0.7823691460055097\n","Iteration 19755 - loss value [[332.10626211]] accuracy 0.7768595041322314\n","Iteration 19756 - loss value [[334.1883698]] accuracy 0.7851239669421488\n","Iteration 19757 - loss value [[347.05078794]] accuracy 0.7617079889807162\n","Iteration 19758 - loss value [[356.02365545]] accuracy 0.7699724517906336\n","Iteration 19759 - loss value [[399.363785]] accuracy 0.7052341597796143\n","Iteration 19760 - loss value [[335.32143533]] accuracy 0.7713498622589532\n","Iteration 19761 - loss value [[329.10735651]] accuracy 0.778236914600551\n","Iteration 19762 - loss value [[331.0933729]] accuracy 0.7741046831955923\n","Iteration 19763 - loss value [[329.39545668]] accuracy 0.7823691460055097\n","Iteration 19764 - loss value [[331.08066684]] accuracy 0.7741046831955923\n","Iteration 19765 - loss value [[329.48542392]] accuracy 0.7837465564738292\n","Iteration 19766 - loss value [[332.11398234]] accuracy 0.7727272727272727\n","Iteration 19767 - loss value [[328.297442]] accuracy 0.7837465564738292\n","Iteration 19768 - loss value [[329.15751037]] accuracy 0.7727272727272727\n","Iteration 19769 - loss value [[332.41354058]] accuracy 0.7823691460055097\n","Iteration 19770 - loss value [[333.24811857]] accuracy 0.7741046831955923\n","Iteration 19771 - loss value [[336.48745986]] accuracy 0.7823691460055097\n","Iteration 19772 - loss value [[356.64648788]] accuracy 0.7506887052341598\n","Iteration 19773 - loss value [[325.48030173]] accuracy 0.7823691460055097\n","Iteration 19774 - loss value [[328.36571199]] accuracy 0.778236914600551\n","Iteration 19775 - loss value [[329.33204116]] accuracy 0.7837465564738292\n","Iteration 19776 - loss value [[330.52780201]] accuracy 0.7768595041322314\n","Iteration 19777 - loss value [[331.94415773]] accuracy 0.78099173553719\n","Iteration 19778 - loss value [[331.02331709]] accuracy 0.7768595041322314\n","Iteration 19779 - loss value [[335.67759847]] accuracy 0.7837465564738292\n","Iteration 19780 - loss value [[352.19130385]] accuracy 0.756198347107438\n","Iteration 19781 - loss value [[327.88467211]] accuracy 0.7851239669421488\n","Iteration 19782 - loss value [[329.51827982]] accuracy 0.7768595041322314\n","Iteration 19783 - loss value [[331.53710821]] accuracy 0.7823691460055097\n","Iteration 19784 - loss value [[329.01002798]] accuracy 0.7796143250688705\n","Iteration 19785 - loss value [[331.60812588]] accuracy 0.7851239669421488\n","Iteration 19786 - loss value [[334.92021754]] accuracy 0.7727272727272727\n","Iteration 19787 - loss value [[336.17009286]] accuracy 0.7851239669421488\n","Iteration 19788 - loss value [[355.55732437]] accuracy 0.7520661157024794\n","Iteration 19789 - loss value [[324.802705]] accuracy 0.7837465564738292\n","Iteration 19790 - loss value [[326.20090113]] accuracy 0.778236914600551\n","Iteration 19791 - loss value [[330.55351561]] accuracy 0.7796143250688705\n","Iteration 19792 - loss value [[327.23639942]] accuracy 0.7796143250688705\n","Iteration 19793 - loss value [[326.22733548]] accuracy 0.7865013774104683\n","Iteration 19794 - loss value [[325.26847424]] accuracy 0.7837465564738292\n","Iteration 19795 - loss value [[326.16440211]] accuracy 0.7865013774104683\n","Iteration 19796 - loss value [[324.87294147]] accuracy 0.7823691460055097\n","Iteration 19797 - loss value [[325.11966377]] accuracy 0.7851239669421488\n","Iteration 19798 - loss value [[326.43939194]] accuracy 0.78099173553719\n","Iteration 19799 - loss value [[327.32592276]] accuracy 0.7837465564738292\n","Iteration 19800 - loss value [[326.2200681]] accuracy 0.7796143250688705\n","Iteration 19801 - loss value [[328.0869121]] accuracy 0.7823691460055097\n","Iteration 19802 - loss value [[328.0757052]] accuracy 0.7754820936639119\n","Iteration 19803 - loss value [[334.77161056]] accuracy 0.7823691460055097\n","Iteration 19804 - loss value [[347.12100169]] accuracy 0.7617079889807162\n","Iteration 19805 - loss value [[357.3330334]] accuracy 0.7658402203856749\n","Iteration 19806 - loss value [[397.09696825]] accuracy 0.7066115702479339\n","Iteration 19807 - loss value [[332.57435899]] accuracy 0.7713498622589532\n","Iteration 19808 - loss value [[325.89297227]] accuracy 0.7796143250688705\n","Iteration 19809 - loss value [[324.69795197]] accuracy 0.7851239669421488\n","Iteration 19810 - loss value [[325.98704282]] accuracy 0.7823691460055097\n","Iteration 19811 - loss value [[325.05889604]] accuracy 0.7837465564738292\n","Iteration 19812 - loss value [[325.97492781]] accuracy 0.7865013774104683\n","Iteration 19813 - loss value [[325.32270482]] accuracy 0.7823691460055097\n","Iteration 19814 - loss value [[326.46587104]] accuracy 0.7837465564738292\n","Iteration 19815 - loss value [[326.70231129]] accuracy 0.7823691460055097\n","Iteration 19816 - loss value [[326.87233752]] accuracy 0.7823691460055097\n","Iteration 19817 - loss value [[325.94408767]] accuracy 0.7796143250688705\n","Iteration 19818 - loss value [[328.05022941]] accuracy 0.7837465564738292\n","Iteration 19819 - loss value [[330.44408192]] accuracy 0.7754820936639119\n","Iteration 19820 - loss value [[328.33592651]] accuracy 0.7823691460055097\n","Iteration 19821 - loss value [[329.75337743]] accuracy 0.7754820936639119\n","Iteration 19822 - loss value [[330.46600431]] accuracy 0.78099173553719\n","Iteration 19823 - loss value [[326.25920751]] accuracy 0.7796143250688705\n","Iteration 19824 - loss value [[325.03660131]] accuracy 0.7823691460055097\n","Iteration 19825 - loss value [[326.12989393]] accuracy 0.7796143250688705\n","Iteration 19826 - loss value [[328.25432613]] accuracy 0.7851239669421488\n","Iteration 19827 - loss value [[329.01888455]] accuracy 0.7768595041322314\n","Iteration 19828 - loss value [[333.49051326]] accuracy 0.7823691460055097\n","Iteration 19829 - loss value [[336.9801024]] accuracy 0.7741046831955923\n","Iteration 19830 - loss value [[332.33931053]] accuracy 0.7851239669421488\n","Iteration 19831 - loss value [[336.78891655]] accuracy 0.7741046831955923\n","Iteration 19832 - loss value [[336.2546448]] accuracy 0.7823691460055097\n","Iteration 19833 - loss value [[354.23376869]] accuracy 0.7548209366391184\n","Iteration 19834 - loss value [[324.45325406]] accuracy 0.7851239669421488\n","Iteration 19835 - loss value [[324.31256254]] accuracy 0.7865013774104683\n","Iteration 19836 - loss value [[324.60751331]] accuracy 0.7851239669421488\n","Iteration 19837 - loss value [[325.86036298]] accuracy 0.78099173553719\n","Iteration 19838 - loss value [[326.91074016]] accuracy 0.78099173553719\n","Iteration 19839 - loss value [[331.09172792]] accuracy 0.7796143250688705\n","Iteration 19840 - loss value [[327.23800571]] accuracy 0.7823691460055097\n","Iteration 19841 - loss value [[327.51716573]] accuracy 0.7865013774104683\n","Iteration 19842 - loss value [[326.05134115]] accuracy 0.7796143250688705\n","Iteration 19843 - loss value [[327.74334369]] accuracy 0.7865013774104683\n","Iteration 19844 - loss value [[327.62193574]] accuracy 0.778236914600551\n","Iteration 19845 - loss value [[330.96683056]] accuracy 0.7837465564738292\n","Iteration 19846 - loss value [[329.74733535]] accuracy 0.78099173553719\n","Iteration 19847 - loss value [[333.11129605]] accuracy 0.78099173553719\n","Iteration 19848 - loss value [[338.47362782]] accuracy 0.7713498622589532\n","Iteration 19849 - loss value [[333.99879916]] accuracy 0.7823691460055097\n","Iteration 19850 - loss value [[337.80591499]] accuracy 0.7713498622589532\n","Iteration 19851 - loss value [[332.02310093]] accuracy 0.7851239669421488\n","Iteration 19852 - loss value [[336.65964324]] accuracy 0.7713498622589532\n","Iteration 19853 - loss value [[329.67556982]] accuracy 0.78099173553719\n","Iteration 19854 - loss value [[326.09454658]] accuracy 0.778236914600551\n","Iteration 19855 - loss value [[325.07239262]] accuracy 0.7823691460055097\n","Iteration 19856 - loss value [[324.81337754]] accuracy 0.7837465564738292\n","Iteration 19857 - loss value [[325.06627793]] accuracy 0.7837465564738292\n","Iteration 19858 - loss value [[325.6117077]] accuracy 0.7823691460055097\n","Iteration 19859 - loss value [[325.72129978]] accuracy 0.7823691460055097\n","Iteration 19860 - loss value [[328.35151866]] accuracy 0.7837465564738292\n","Iteration 19861 - loss value [[330.1469208]] accuracy 0.7741046831955923\n","Iteration 19862 - loss value [[331.62236932]] accuracy 0.7865013774104683\n","Iteration 19863 - loss value [[335.68695793]] accuracy 0.7741046831955923\n","Iteration 19864 - loss value [[331.01699614]] accuracy 0.7823691460055097\n","Iteration 19865 - loss value [[328.22439658]] accuracy 0.7796143250688705\n","Iteration 19866 - loss value [[327.98361474]] accuracy 0.7851239669421488\n","Iteration 19867 - loss value [[325.99914236]] accuracy 0.78099173553719\n","Iteration 19868 - loss value [[325.35818342]] accuracy 0.7823691460055097\n","Iteration 19869 - loss value [[325.24962744]] accuracy 0.7823691460055097\n","Iteration 19870 - loss value [[325.74620021]] accuracy 0.78099173553719\n","Iteration 19871 - loss value [[329.84795427]] accuracy 0.7837465564738292\n","Iteration 19872 - loss value [[334.97615285]] accuracy 0.7741046831955923\n","Iteration 19873 - loss value [[330.35440703]] accuracy 0.7823691460055097\n","Iteration 19874 - loss value [[334.04116175]] accuracy 0.7727272727272727\n","Iteration 19875 - loss value [[326.84003844]] accuracy 0.7851239669421488\n","Iteration 19876 - loss value [[325.93329771]] accuracy 0.78099173553719\n","Iteration 19877 - loss value [[328.17640045]] accuracy 0.7837465564738292\n","Iteration 19878 - loss value [[326.67918187]] accuracy 0.7796143250688705\n","Iteration 19879 - loss value [[332.43264583]] accuracy 0.7796143250688705\n","Iteration 19880 - loss value [[329.93738773]] accuracy 0.778236914600551\n","Iteration 19881 - loss value [[331.46197523]] accuracy 0.7837465564738292\n","Iteration 19882 - loss value [[333.73207873]] accuracy 0.7741046831955923\n","Iteration 19883 - loss value [[328.96871252]] accuracy 0.7823691460055097\n","Iteration 19884 - loss value [[326.60111995]] accuracy 0.7768595041322314\n","Iteration 19885 - loss value [[325.09692532]] accuracy 0.7837465564738292\n","Iteration 19886 - loss value [[324.72919921]] accuracy 0.7823691460055097\n","Iteration 19887 - loss value [[324.47917075]] accuracy 0.7865013774104683\n","Iteration 19888 - loss value [[325.41413869]] accuracy 0.7865013774104683\n","Iteration 19889 - loss value [[325.15750718]] accuracy 0.78099173553719\n","Iteration 19890 - loss value [[326.7658089]] accuracy 0.7837465564738292\n","Iteration 19891 - loss value [[325.67523697]] accuracy 0.7823691460055097\n","Iteration 19892 - loss value [[327.42201718]] accuracy 0.7823691460055097\n","Iteration 19893 - loss value [[327.15063757]] accuracy 0.7796143250688705\n","Iteration 19894 - loss value [[328.90358211]] accuracy 0.78099173553719\n","Iteration 19895 - loss value [[325.95534947]] accuracy 0.7768595041322314\n","Iteration 19896 - loss value [[324.81477762]] accuracy 0.7851239669421488\n","Iteration 19897 - loss value [[324.23999058]] accuracy 0.7865013774104683\n","Iteration 19898 - loss value [[324.14949972]] accuracy 0.7878787878787878\n","Iteration 19899 - loss value [[324.1842991]] accuracy 0.7865013774104683\n","Iteration 19900 - loss value [[324.40641088]] accuracy 0.7865013774104683\n","Iteration 19901 - loss value [[325.67954405]] accuracy 0.78099173553719\n","Iteration 19902 - loss value [[329.43011508]] accuracy 0.78099173553719\n","Iteration 19903 - loss value [[326.1465082]] accuracy 0.7796143250688705\n","Iteration 19904 - loss value [[325.84600192]] accuracy 0.7851239669421488\n","Iteration 19905 - loss value [[324.69730339]] accuracy 0.7823691460055097\n","Iteration 19906 - loss value [[324.64320858]] accuracy 0.7865013774104683\n","Iteration 19907 - loss value [[325.48716755]] accuracy 0.7823691460055097\n","Iteration 19908 - loss value [[327.08690942]] accuracy 0.7851239669421488\n","Iteration 19909 - loss value [[326.09837403]] accuracy 0.7796143250688705\n","Iteration 19910 - loss value [[327.26791019]] accuracy 0.7823691460055097\n","Iteration 19911 - loss value [[326.89696813]] accuracy 0.7823691460055097\n","Iteration 19912 - loss value [[327.1101826]] accuracy 0.7823691460055097\n","Iteration 19913 - loss value [[325.31389637]] accuracy 0.7837465564738292\n","Iteration 19914 - loss value [[326.12073957]] accuracy 0.7865013774104683\n","Iteration 19915 - loss value [[325.51467333]] accuracy 0.7823691460055097\n","Iteration 19916 - loss value [[326.58828566]] accuracy 0.7837465564738292\n","Iteration 19917 - loss value [[326.82854466]] accuracy 0.7823691460055097\n","Iteration 19918 - loss value [[327.01117178]] accuracy 0.7837465564738292\n","Iteration 19919 - loss value [[326.6578939]] accuracy 0.778236914600551\n","Iteration 19920 - loss value [[329.39835298]] accuracy 0.7796143250688705\n","Iteration 19921 - loss value [[326.36243555]] accuracy 0.7796143250688705\n","Iteration 19922 - loss value [[325.24493466]] accuracy 0.7837465564738292\n","Iteration 19923 - loss value [[325.28648388]] accuracy 0.7837465564738292\n","Iteration 19924 - loss value [[327.24195723]] accuracy 0.7823691460055097\n","Iteration 19925 - loss value [[326.46357595]] accuracy 0.78099173553719\n","Iteration 19926 - loss value [[326.7725869]] accuracy 0.7823691460055097\n","Iteration 19927 - loss value [[325.76043116]] accuracy 0.7837465564738292\n","Iteration 19928 - loss value [[325.7313461]] accuracy 0.7837465564738292\n","Iteration 19929 - loss value [[324.9719992]] accuracy 0.7837465564738292\n","Iteration 19930 - loss value [[325.86053255]] accuracy 0.7837465564738292\n","Iteration 19931 - loss value [[325.28119174]] accuracy 0.7837465564738292\n","Iteration 19932 - loss value [[326.2037074]] accuracy 0.7865013774104683\n","Iteration 19933 - loss value [[325.87932762]] accuracy 0.7823691460055097\n","Iteration 19934 - loss value [[326.03337715]] accuracy 0.7823691460055097\n","Iteration 19935 - loss value [[325.61851406]] accuracy 0.7823691460055097\n","Iteration 19936 - loss value [[326.8920004]] accuracy 0.7837465564738292\n","Iteration 19937 - loss value [[326.56996435]] accuracy 0.7823691460055097\n","Iteration 19938 - loss value [[326.91289575]] accuracy 0.7837465564738292\n","Iteration 19939 - loss value [[325.96472271]] accuracy 0.7796143250688705\n","Iteration 19940 - loss value [[328.87417586]] accuracy 0.7851239669421488\n","Iteration 19941 - loss value [[331.1182165]] accuracy 0.7741046831955923\n","Iteration 19942 - loss value [[328.37362333]] accuracy 0.78099173553719\n","Iteration 19943 - loss value [[328.66856373]] accuracy 0.7741046831955923\n","Iteration 19944 - loss value [[335.73052282]] accuracy 0.7851239669421488\n","Iteration 19945 - loss value [[352.81564827]] accuracy 0.7548209366391184\n","Iteration 19946 - loss value [[324.4511694]] accuracy 0.7851239669421488\n","Iteration 19947 - loss value [[324.60871803]] accuracy 0.7851239669421488\n","Iteration 19948 - loss value [[325.74790982]] accuracy 0.78099173553719\n","Iteration 19949 - loss value [[329.4240854]] accuracy 0.7823691460055097\n","Iteration 19950 - loss value [[326.47120064]] accuracy 0.7768595041322314\n","Iteration 19951 - loss value [[325.79930375]] accuracy 0.7865013774104683\n","Iteration 19952 - loss value [[324.63621921]] accuracy 0.7823691460055097\n","Iteration 19953 - loss value [[324.50868893]] accuracy 0.7851239669421488\n","Iteration 19954 - loss value [[324.95739147]] accuracy 0.7823691460055097\n","Iteration 19955 - loss value [[324.91883529]] accuracy 0.7851239669421488\n","Iteration 19956 - loss value [[326.77161279]] accuracy 0.778236914600551\n","Iteration 19957 - loss value [[332.50070373]] accuracy 0.7796143250688705\n","Iteration 19958 - loss value [[329.23159137]] accuracy 0.78099173553719\n","Iteration 19959 - loss value [[329.83871324]] accuracy 0.78099173553719\n","Iteration 19960 - loss value [[326.90509315]] accuracy 0.7796143250688705\n","Iteration 19961 - loss value [[325.08530968]] accuracy 0.7823691460055097\n","Iteration 19962 - loss value [[324.52673963]] accuracy 0.7837465564738292\n","Iteration 19963 - loss value [[325.1030545]] accuracy 0.7865013774104683\n","Iteration 19964 - loss value [[327.44360415]] accuracy 0.778236914600551\n","Iteration 19965 - loss value [[334.89130891]] accuracy 0.78099173553719\n","Iteration 19966 - loss value [[336.13463318]] accuracy 0.7727272727272727\n","Iteration 19967 - loss value [[330.53263087]] accuracy 0.7865013774104683\n","Iteration 19968 - loss value [[330.74008544]] accuracy 0.7796143250688705\n","Iteration 19969 - loss value [[338.09075158]] accuracy 0.7823691460055097\n","Iteration 19970 - loss value [[355.10815692]] accuracy 0.7520661157024794\n","Iteration 19971 - loss value [[324.8042772]] accuracy 0.7837465564738292\n","Iteration 19972 - loss value [[325.92369257]] accuracy 0.778236914600551\n","Iteration 19973 - loss value [[329.87681589]] accuracy 0.7823691460055097\n","Iteration 19974 - loss value [[326.51803443]] accuracy 0.778236914600551\n","Iteration 19975 - loss value [[325.06205455]] accuracy 0.7823691460055097\n","Iteration 19976 - loss value [[324.39849646]] accuracy 0.7837465564738292\n","Iteration 19977 - loss value [[324.66810132]] accuracy 0.7878787878787878\n","Iteration 19978 - loss value [[326.03249396]] accuracy 0.7796143250688705\n","Iteration 19979 - loss value [[330.4550106]] accuracy 0.7823691460055097\n","Iteration 19980 - loss value [[327.78916355]] accuracy 0.7823691460055097\n","Iteration 19981 - loss value [[329.03061772]] accuracy 0.7823691460055097\n","Iteration 19982 - loss value [[326.35925857]] accuracy 0.78099173553719\n","Iteration 19983 - loss value [[326.03349304]] accuracy 0.7823691460055097\n","Iteration 19984 - loss value [[325.90688644]] accuracy 0.7823691460055097\n","Iteration 19985 - loss value [[326.178778]] accuracy 0.7823691460055097\n","Iteration 19986 - loss value [[325.62804039]] accuracy 0.78099173553719\n","Iteration 19987 - loss value [[325.55405638]] accuracy 0.7837465564738292\n","Iteration 19988 - loss value [[325.05899503]] accuracy 0.7823691460055097\n","Iteration 19989 - loss value [[324.4202305]] accuracy 0.7851239669421488\n","Iteration 19990 - loss value [[324.75123583]] accuracy 0.7851239669421488\n","Iteration 19991 - loss value [[325.38923131]] accuracy 0.7823691460055097\n","Iteration 19992 - loss value [[326.28590184]] accuracy 0.7823691460055097\n","Iteration 19993 - loss value [[327.59715407]] accuracy 0.7741046831955923\n","Iteration 19994 - loss value [[330.99647568]] accuracy 0.7823691460055097\n","Iteration 19995 - loss value [[327.79634153]] accuracy 0.78099173553719\n","Iteration 19996 - loss value [[327.77835394]] accuracy 0.7837465564738292\n","Iteration 19997 - loss value [[325.8846859]] accuracy 0.7837465564738292\n","Iteration 19998 - loss value [[325.19031357]] accuracy 0.7823691460055097\n","Iteration 19999 - loss value [[324.47018685]] accuracy 0.7851239669421488\n"]}]},{"cell_type":"markdown","source":["### **VALIDACION DEL MODELO LINEAR1**"],"metadata":{"id":"n7rxSLFT3eZy"}},{"cell_type":"code","source":["import numpy as np\n","Y_pred_linear1 = model.forward(X_test)\n","\n","regr = LinearRegression()\n","regr.fit(Y_test,Y_pred_linear1)\n","coef = regr.coef_\n","print(coef)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YA8TM4pA3dOc","executionInfo":{"status":"ok","timestamp":1664747101778,"user_tz":300,"elapsed":278,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"1c0217f2-b1a4-4746-99eb-2e9701d4a406"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.37063554]]\n"]}]},{"cell_type":"markdown","source":["## **APLICACION DEL MODELO ANTERIORMENTE EXPUESTO EN EL DATASET qsar_fish_toxicity**"],"metadata":{"id":"KA1RyOu82hRm"}},{"cell_type":"markdown","source":["### **NORMALIZACION DEL DATASET**"],"metadata":{"id":"65oKy-o924rG"}},{"cell_type":"code","source":["X = datos.iloc[:,0:6]\n","Y = datos.iloc[:,6]\n","X_n=np.asarray(normalized(X,np.max(X),np.min(X)))\n","Y_n = Y.values\n","Y_n = Y_n.reshape(-1,1)\n","Y_n=np.asarray(normalized(Y_n,np.max(Y_n),np.min(Y_n)))\n","Y_n=(Y_n > 0.5).astype('int') \n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X_n, Y_n, test_size = 0.2, random_state=5)\n","\n","print(X_n.shape,Y_n.shape)"],"metadata":{"id":"OC17CSb23dNc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239145052,"user_tz":300,"elapsed":14,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"e0a65d6d-003e-4303-a415-c34189566d6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(908, 6) (908, 1)\n"]}]},{"cell_type":"markdown","source":["### **CREACION DEL MODELO A PARTIR DEL ANTERIOR**"],"metadata":{"id":"CPuDOWok3D-f"}},{"cell_type":"code","source":["# Build a model\n","model = mlp(layersizes=[6, 7, 8, 1],\n","            activations=[relu, relu, sigmoid],\n","            derivatives=[drelu, drelu, dsigmoid],\n","            lossderiv=d_cross_entropy)\n","model.initialize()\n","\n","yhat = np.asarray(model.forward(X_train))\n","loss = cross_entropy(Y_train, yhat)\n","print(\"Before training - loss value {} accuracy {}\".format(loss, accuracy_score(Y_train, (yhat > 0.5))))"],"metadata":{"id":"nTQm4Dan2uDC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239145317,"user_tz":300,"elapsed":277,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"369144be-6905-4a19-edbd-d9e2d428898b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before training - loss value [[490.332445]] accuracy 0.7272727272727273\n"]}]},{"cell_type":"code","source":["# train for each epoch\n","n_epochs = 50000\n","learning_rate = 0.0005\n","for n in range(n_epochs):\n","    model.forward(X_train)\n","    yhat = model.a[-1]\n","    model.backward(Y_train, yhat)\n","    model.update(learning_rate)\n","    loss = cross_entropy(Y_train, yhat)\n","    print(\"Iteration {} - loss value {} accuracy {}\".format(n, loss, accuracy_score(Y_train, (yhat > 1))))"],"metadata":{"id":"h2iOMdhz56o2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239263946,"user_tz":300,"elapsed":118633,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"48ccb0b7-0835-41e6-bd2b-5fd1098430d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las ltimas 5000 lneas del flujo de salida.\u001b[0m\n","Iteration 45000 - loss value [[267.43453182]] accuracy 0.7272727272727273\n","Iteration 45001 - loss value [[267.66075877]] accuracy 0.7272727272727273\n","Iteration 45002 - loss value [[267.26310787]] accuracy 0.7272727272727273\n","Iteration 45003 - loss value [[268.58145157]] accuracy 0.7272727272727273\n","Iteration 45004 - loss value [[267.64857254]] accuracy 0.7272727272727273\n","Iteration 45005 - loss value [[267.89637174]] accuracy 0.7272727272727273\n","Iteration 45006 - loss value [[266.9669122]] accuracy 0.7272727272727273\n","Iteration 45007 - loss value [[267.82239096]] accuracy 0.7272727272727273\n","Iteration 45008 - loss value [[267.17238412]] accuracy 0.7272727272727273\n","Iteration 45009 - loss value [[267.80464727]] accuracy 0.7272727272727273\n","Iteration 45010 - loss value [[267.26080347]] accuracy 0.7272727272727273\n","Iteration 45011 - loss value [[268.39389785]] accuracy 0.7272727272727273\n","Iteration 45012 - loss value [[267.29507455]] accuracy 0.7272727272727273\n","Iteration 45013 - loss value [[268.14889787]] accuracy 0.7272727272727273\n","Iteration 45014 - loss value [[267.31350538]] accuracy 0.7272727272727273\n","Iteration 45015 - loss value [[268.08336372]] accuracy 0.7272727272727273\n","Iteration 45016 - loss value [[267.92505593]] accuracy 0.7272727272727273\n","Iteration 45017 - loss value [[270.2109746]] accuracy 0.7272727272727273\n","Iteration 45018 - loss value [[270.03308142]] accuracy 0.7272727272727273\n","Iteration 45019 - loss value [[272.89014402]] accuracy 0.7272727272727273\n","Iteration 45020 - loss value [[270.49435454]] accuracy 0.7272727272727273\n","Iteration 45021 - loss value [[272.82572629]] accuracy 0.7272727272727273\n","Iteration 45022 - loss value [[270.59296991]] accuracy 0.7272727272727273\n","Iteration 45023 - loss value [[272.4551025]] accuracy 0.7272727272727273\n","Iteration 45024 - loss value [[270.25446056]] accuracy 0.7272727272727273\n","Iteration 45025 - loss value [[271.72463623]] accuracy 0.7272727272727273\n","Iteration 45026 - loss value [[270.68308861]] accuracy 0.7272727272727273\n","Iteration 45027 - loss value [[272.96356177]] accuracy 0.7272727272727273\n","Iteration 45028 - loss value [[270.67185603]] accuracy 0.7272727272727273\n","Iteration 45029 - loss value [[272.17169952]] accuracy 0.7272727272727273\n","Iteration 45030 - loss value [[270.09967844]] accuracy 0.7272727272727273\n","Iteration 45031 - loss value [[271.67743113]] accuracy 0.7272727272727273\n","Iteration 45032 - loss value [[270.8268264]] accuracy 0.7272727272727273\n","Iteration 45033 - loss value [[274.48887908]] accuracy 0.7272727272727273\n","Iteration 45034 - loss value [[273.12280515]] accuracy 0.7272727272727273\n","Iteration 45035 - loss value [[275.38562555]] accuracy 0.7272727272727273\n","Iteration 45036 - loss value [[274.10532643]] accuracy 0.7272727272727273\n","Iteration 45037 - loss value [[276.31834212]] accuracy 0.7272727272727273\n","Iteration 45038 - loss value [[276.14928787]] accuracy 0.7272727272727273\n","Iteration 45039 - loss value [[279.65392753]] accuracy 0.7272727272727273\n","Iteration 45040 - loss value [[279.16770096]] accuracy 0.7272727272727273\n","Iteration 45041 - loss value [[280.74839023]] accuracy 0.7272727272727273\n","Iteration 45042 - loss value [[279.78927843]] accuracy 0.7272727272727273\n","Iteration 45043 - loss value [[281.09912339]] accuracy 0.7272727272727273\n","Iteration 45044 - loss value [[279.9673709]] accuracy 0.7272727272727273\n","Iteration 45045 - loss value [[281.06339488]] accuracy 0.7272727272727273\n","Iteration 45046 - loss value [[279.29957398]] accuracy 0.7272727272727273\n","Iteration 45047 - loss value [[280.06279524]] accuracy 0.7272727272727273\n","Iteration 45048 - loss value [[278.89740309]] accuracy 0.7272727272727273\n","Iteration 45049 - loss value [[279.85638546]] accuracy 0.7272727272727273\n","Iteration 45050 - loss value [[278.64368114]] accuracy 0.7272727272727273\n","Iteration 45051 - loss value [[280.11016671]] accuracy 0.7272727272727273\n","Iteration 45052 - loss value [[278.61605472]] accuracy 0.7272727272727273\n","Iteration 45053 - loss value [[279.9744918]] accuracy 0.7272727272727273\n","Iteration 45054 - loss value [[278.40653349]] accuracy 0.7272727272727273\n","Iteration 45055 - loss value [[279.59531825]] accuracy 0.7272727272727273\n","Iteration 45056 - loss value [[277.79852308]] accuracy 0.7272727272727273\n","Iteration 45057 - loss value [[278.21849181]] accuracy 0.7272727272727273\n","Iteration 45058 - loss value [[276.13048491]] accuracy 0.7272727272727273\n","Iteration 45059 - loss value [[276.16941307]] accuracy 0.7272727272727273\n","Iteration 45060 - loss value [[273.70761112]] accuracy 0.7272727272727273\n","Iteration 45061 - loss value [[273.77890952]] accuracy 0.7272727272727273\n","Iteration 45062 - loss value [[270.89365232]] accuracy 0.7272727272727273\n","Iteration 45063 - loss value [[271.70368998]] accuracy 0.7272727272727273\n","Iteration 45064 - loss value [[269.79718803]] accuracy 0.7272727272727273\n","Iteration 45065 - loss value [[270.80557313]] accuracy 0.7272727272727273\n","Iteration 45066 - loss value [[269.96181778]] accuracy 0.7272727272727273\n","Iteration 45067 - loss value [[272.91381232]] accuracy 0.7272727272727273\n","Iteration 45068 - loss value [[270.20031269]] accuracy 0.7272727272727273\n","Iteration 45069 - loss value [[270.6754124]] accuracy 0.7272727272727273\n","Iteration 45070 - loss value [[269.70232281]] accuracy 0.7272727272727273\n","Iteration 45071 - loss value [[272.14017646]] accuracy 0.7272727272727273\n","Iteration 45072 - loss value [[269.24818163]] accuracy 0.7272727272727273\n","Iteration 45073 - loss value [[269.1785788]] accuracy 0.7272727272727273\n","Iteration 45074 - loss value [[268.03403622]] accuracy 0.7272727272727273\n","Iteration 45075 - loss value [[268.47512666]] accuracy 0.7272727272727273\n","Iteration 45076 - loss value [[267.54108213]] accuracy 0.7272727272727273\n","Iteration 45077 - loss value [[268.21607729]] accuracy 0.7272727272727273\n","Iteration 45078 - loss value [[267.50660465]] accuracy 0.7272727272727273\n","Iteration 45079 - loss value [[268.61245854]] accuracy 0.7272727272727273\n","Iteration 45080 - loss value [[267.61466225]] accuracy 0.7272727272727273\n","Iteration 45081 - loss value [[268.44625911]] accuracy 0.7272727272727273\n","Iteration 45082 - loss value [[267.56427122]] accuracy 0.7272727272727273\n","Iteration 45083 - loss value [[268.21943796]] accuracy 0.7272727272727273\n","Iteration 45084 - loss value [[267.35194869]] accuracy 0.7272727272727273\n","Iteration 45085 - loss value [[267.70170219]] accuracy 0.7272727272727273\n","Iteration 45086 - loss value [[267.54723929]] accuracy 0.7272727272727273\n","Iteration 45087 - loss value [[269.91016395]] accuracy 0.7272727272727273\n","Iteration 45088 - loss value [[269.60206626]] accuracy 0.7272727272727273\n","Iteration 45089 - loss value [[272.39264848]] accuracy 0.7272727272727273\n","Iteration 45090 - loss value [[269.83855276]] accuracy 0.7272727272727273\n","Iteration 45091 - loss value [[271.43851648]] accuracy 0.7272727272727273\n","Iteration 45092 - loss value [[269.13898253]] accuracy 0.7272727272727273\n","Iteration 45093 - loss value [[270.39148153]] accuracy 0.7272727272727273\n","Iteration 45094 - loss value [[269.96072733]] accuracy 0.7272727272727273\n","Iteration 45095 - loss value [[273.35956008]] accuracy 0.7272727272727273\n","Iteration 45096 - loss value [[271.02309865]] accuracy 0.7272727272727273\n","Iteration 45097 - loss value [[273.09025495]] accuracy 0.7272727272727273\n","Iteration 45098 - loss value [[270.92128327]] accuracy 0.7272727272727273\n","Iteration 45099 - loss value [[273.11660249]] accuracy 0.7272727272727273\n","Iteration 45100 - loss value [[270.629843]] accuracy 0.7272727272727273\n","Iteration 45101 - loss value [[272.48102623]] accuracy 0.7272727272727273\n","Iteration 45102 - loss value [[270.02915344]] accuracy 0.7272727272727273\n","Iteration 45103 - loss value [[271.06245869]] accuracy 0.7272727272727273\n","Iteration 45104 - loss value [[270.34848684]] accuracy 0.7272727272727273\n","Iteration 45105 - loss value [[274.03677345]] accuracy 0.7272727272727273\n","Iteration 45106 - loss value [[272.80266295]] accuracy 0.7272727272727273\n","Iteration 45107 - loss value [[275.53405365]] accuracy 0.7272727272727273\n","Iteration 45108 - loss value [[275.0057209]] accuracy 0.7272727272727273\n","Iteration 45109 - loss value [[278.02800114]] accuracy 0.7272727272727273\n","Iteration 45110 - loss value [[278.18265102]] accuracy 0.7272727272727273\n","Iteration 45111 - loss value [[281.42977736]] accuracy 0.7272727272727273\n","Iteration 45112 - loss value [[280.76051047]] accuracy 0.7272727272727273\n","Iteration 45113 - loss value [[282.51556778]] accuracy 0.7272727272727273\n","Iteration 45114 - loss value [[281.43047771]] accuracy 0.7272727272727273\n","Iteration 45115 - loss value [[282.97257195]] accuracy 0.7272727272727273\n","Iteration 45116 - loss value [[281.76091918]] accuracy 0.7272727272727273\n","Iteration 45117 - loss value [[283.36286949]] accuracy 0.7272727272727273\n","Iteration 45118 - loss value [[281.81386775]] accuracy 0.7272727272727273\n","Iteration 45119 - loss value [[282.73563446]] accuracy 0.7272727272727273\n","Iteration 45120 - loss value [[280.14909245]] accuracy 0.7272727272727273\n","Iteration 45121 - loss value [[278.5242054]] accuracy 0.7272727272727273\n","Iteration 45122 - loss value [[277.12807658]] accuracy 0.7272727272727273\n","Iteration 45123 - loss value [[279.4987112]] accuracy 0.7272727272727273\n","Iteration 45124 - loss value [[277.9887023]] accuracy 0.7272727272727273\n","Iteration 45125 - loss value [[280.1749708]] accuracy 0.7272727272727273\n","Iteration 45126 - loss value [[278.77119791]] accuracy 0.7272727272727273\n","Iteration 45127 - loss value [[279.80086711]] accuracy 0.7272727272727273\n","Iteration 45128 - loss value [[277.74642038]] accuracy 0.7272727272727273\n","Iteration 45129 - loss value [[279.6388641]] accuracy 0.7272727272727273\n","Iteration 45130 - loss value [[277.37055764]] accuracy 0.7272727272727273\n","Iteration 45131 - loss value [[278.94838037]] accuracy 0.7272727272727273\n","Iteration 45132 - loss value [[277.50063374]] accuracy 0.7272727272727273\n","Iteration 45133 - loss value [[279.55789485]] accuracy 0.7272727272727273\n","Iteration 45134 - loss value [[277.97917686]] accuracy 0.7272727272727273\n","Iteration 45135 - loss value [[279.18978973]] accuracy 0.7272727272727273\n","Iteration 45136 - loss value [[277.88984342]] accuracy 0.7272727272727273\n","Iteration 45137 - loss value [[279.6516266]] accuracy 0.7272727272727273\n","Iteration 45138 - loss value [[277.61007659]] accuracy 0.7272727272727273\n","Iteration 45139 - loss value [[277.89774675]] accuracy 0.7272727272727273\n","Iteration 45140 - loss value [[275.14882643]] accuracy 0.7272727272727273\n","Iteration 45141 - loss value [[273.94792343]] accuracy 0.7272727272727273\n","Iteration 45142 - loss value [[270.87741419]] accuracy 0.7272727272727273\n","Iteration 45143 - loss value [[271.55407301]] accuracy 0.7272727272727273\n","Iteration 45144 - loss value [[269.66424052]] accuracy 0.7272727272727273\n","Iteration 45145 - loss value [[270.49512468]] accuracy 0.7272727272727273\n","Iteration 45146 - loss value [[269.47117185]] accuracy 0.7272727272727273\n","Iteration 45147 - loss value [[271.74639766]] accuracy 0.7272727272727273\n","Iteration 45148 - loss value [[269.83173509]] accuracy 0.7272727272727273\n","Iteration 45149 - loss value [[270.76339013]] accuracy 0.7272727272727273\n","Iteration 45150 - loss value [[269.92162478]] accuracy 0.7272727272727273\n","Iteration 45151 - loss value [[272.67319591]] accuracy 0.7272727272727273\n","Iteration 45152 - loss value [[269.23509605]] accuracy 0.7272727272727273\n","Iteration 45153 - loss value [[269.49539011]] accuracy 0.7272727272727273\n","Iteration 45154 - loss value [[268.48392062]] accuracy 0.7272727272727273\n","Iteration 45155 - loss value [[269.20980282]] accuracy 0.7272727272727273\n","Iteration 45156 - loss value [[267.84761623]] accuracy 0.7272727272727273\n","Iteration 45157 - loss value [[268.33059946]] accuracy 0.7272727272727273\n","Iteration 45158 - loss value [[267.53361173]] accuracy 0.7272727272727273\n","Iteration 45159 - loss value [[267.86928797]] accuracy 0.7272727272727273\n","Iteration 45160 - loss value [[267.30644405]] accuracy 0.7272727272727273\n","Iteration 45161 - loss value [[268.44435787]] accuracy 0.7272727272727273\n","Iteration 45162 - loss value [[267.40066171]] accuracy 0.7272727272727273\n","Iteration 45163 - loss value [[268.078926]] accuracy 0.7272727272727273\n","Iteration 45164 - loss value [[267.18619308]] accuracy 0.7272727272727273\n","Iteration 45165 - loss value [[267.86057497]] accuracy 0.7272727272727273\n","Iteration 45166 - loss value [[267.17675775]] accuracy 0.7272727272727273\n","Iteration 45167 - loss value [[268.40785865]] accuracy 0.7272727272727273\n","Iteration 45168 - loss value [[267.20528625]] accuracy 0.7272727272727273\n","Iteration 45169 - loss value [[267.65794303]] accuracy 0.7272727272727273\n","Iteration 45170 - loss value [[266.825433]] accuracy 0.7272727272727273\n","Iteration 45171 - loss value [[267.39473823]] accuracy 0.7272727272727273\n","Iteration 45172 - loss value [[266.58834211]] accuracy 0.7272727272727273\n","Iteration 45173 - loss value [[268.01260943]] accuracy 0.7272727272727273\n","Iteration 45174 - loss value [[266.68928615]] accuracy 0.7272727272727273\n","Iteration 45175 - loss value [[267.02927367]] accuracy 0.7272727272727273\n","Iteration 45176 - loss value [[266.34445121]] accuracy 0.7272727272727273\n","Iteration 45177 - loss value [[267.85299772]] accuracy 0.7272727272727273\n","Iteration 45178 - loss value [[266.54141322]] accuracy 0.7272727272727273\n","Iteration 45179 - loss value [[267.10114895]] accuracy 0.7272727272727273\n","Iteration 45180 - loss value [[266.71925238]] accuracy 0.7272727272727273\n","Iteration 45181 - loss value [[268.43382253]] accuracy 0.7272727272727273\n","Iteration 45182 - loss value [[267.75220025]] accuracy 0.7272727272727273\n","Iteration 45183 - loss value [[268.89993718]] accuracy 0.7272727272727273\n","Iteration 45184 - loss value [[268.82931033]] accuracy 0.7272727272727273\n","Iteration 45185 - loss value [[271.48334222]] accuracy 0.7272727272727273\n","Iteration 45186 - loss value [[269.69835335]] accuracy 0.7272727272727273\n","Iteration 45187 - loss value [[271.98380109]] accuracy 0.7272727272727273\n","Iteration 45188 - loss value [[270.16954868]] accuracy 0.7272727272727273\n","Iteration 45189 - loss value [[272.47306294]] accuracy 0.7272727272727273\n","Iteration 45190 - loss value [[270.68131057]] accuracy 0.7272727272727273\n","Iteration 45191 - loss value [[273.05123333]] accuracy 0.7272727272727273\n","Iteration 45192 - loss value [[270.6490294]] accuracy 0.7272727272727273\n","Iteration 45193 - loss value [[272.16907228]] accuracy 0.7272727272727273\n","Iteration 45194 - loss value [[270.34977551]] accuracy 0.7272727272727273\n","Iteration 45195 - loss value [[272.58609362]] accuracy 0.7272727272727273\n","Iteration 45196 - loss value [[270.50448801]] accuracy 0.7272727272727273\n","Iteration 45197 - loss value [[272.73924121]] accuracy 0.7272727272727273\n","Iteration 45198 - loss value [[270.59732186]] accuracy 0.7272727272727273\n","Iteration 45199 - loss value [[271.73879433]] accuracy 0.7272727272727273\n","Iteration 45200 - loss value [[271.06031748]] accuracy 0.7272727272727273\n","Iteration 45201 - loss value [[274.67719514]] accuracy 0.7272727272727273\n","Iteration 45202 - loss value [[273.20731121]] accuracy 0.7272727272727273\n","Iteration 45203 - loss value [[275.87577425]] accuracy 0.7272727272727273\n","Iteration 45204 - loss value [[275.62379]] accuracy 0.7272727272727273\n","Iteration 45205 - loss value [[278.60507588]] accuracy 0.7272727272727273\n","Iteration 45206 - loss value [[279.39731132]] accuracy 0.7272727272727273\n","Iteration 45207 - loss value [[281.45268146]] accuracy 0.7272727272727273\n","Iteration 45208 - loss value [[281.77882614]] accuracy 0.7272727272727273\n","Iteration 45209 - loss value [[283.98427988]] accuracy 0.7272727272727273\n","Iteration 45210 - loss value [[283.47683808]] accuracy 0.7272727272727273\n","Iteration 45211 - loss value [[285.13527261]] accuracy 0.7272727272727273\n","Iteration 45212 - loss value [[283.98024513]] accuracy 0.7272727272727273\n","Iteration 45213 - loss value [[284.24739657]] accuracy 0.7272727272727273\n","Iteration 45214 - loss value [[282.67512974]] accuracy 0.7272727272727273\n","Iteration 45215 - loss value [[282.63609675]] accuracy 0.7272727272727273\n","Iteration 45216 - loss value [[280.45124871]] accuracy 0.7272727272727273\n","Iteration 45217 - loss value [[280.50639535]] accuracy 0.7272727272727273\n","Iteration 45218 - loss value [[278.85319252]] accuracy 0.7272727272727273\n","Iteration 45219 - loss value [[279.49998664]] accuracy 0.7272727272727273\n","Iteration 45220 - loss value [[277.54047644]] accuracy 0.7272727272727273\n","Iteration 45221 - loss value [[279.50718826]] accuracy 0.7272727272727273\n","Iteration 45222 - loss value [[277.60126126]] accuracy 0.7272727272727273\n","Iteration 45223 - loss value [[279.78416188]] accuracy 0.7272727272727273\n","Iteration 45224 - loss value [[278.09366]] accuracy 0.7272727272727273\n","Iteration 45225 - loss value [[279.75357511]] accuracy 0.7272727272727273\n","Iteration 45226 - loss value [[277.95104908]] accuracy 0.7272727272727273\n","Iteration 45227 - loss value [[279.51444339]] accuracy 0.7272727272727273\n","Iteration 45228 - loss value [[277.67823805]] accuracy 0.7272727272727273\n","Iteration 45229 - loss value [[279.51416577]] accuracy 0.7272727272727273\n","Iteration 45230 - loss value [[277.33166914]] accuracy 0.7272727272727273\n","Iteration 45231 - loss value [[278.30350785]] accuracy 0.7272727272727273\n","Iteration 45232 - loss value [[276.52481842]] accuracy 0.7272727272727273\n","Iteration 45233 - loss value [[276.42702804]] accuracy 0.7272727272727273\n","Iteration 45234 - loss value [[274.29258732]] accuracy 0.7272727272727273\n","Iteration 45235 - loss value [[275.25334807]] accuracy 0.7272727272727273\n","Iteration 45236 - loss value [[271.88916275]] accuracy 0.7272727272727273\n","Iteration 45237 - loss value [[272.58835177]] accuracy 0.7272727272727273\n","Iteration 45238 - loss value [[269.30603925]] accuracy 0.7272727272727273\n","Iteration 45239 - loss value [[269.26549337]] accuracy 0.7272727272727273\n","Iteration 45240 - loss value [[268.24056279]] accuracy 0.7272727272727273\n","Iteration 45241 - loss value [[268.69388813]] accuracy 0.7272727272727273\n","Iteration 45242 - loss value [[267.44518751]] accuracy 0.7272727272727273\n","Iteration 45243 - loss value [[267.71342092]] accuracy 0.7272727272727273\n","Iteration 45244 - loss value [[267.16986393]] accuracy 0.7272727272727273\n","Iteration 45245 - loss value [[267.80513514]] accuracy 0.7272727272727273\n","Iteration 45246 - loss value [[266.6652689]] accuracy 0.7272727272727273\n","Iteration 45247 - loss value [[267.21100128]] accuracy 0.7272727272727273\n","Iteration 45248 - loss value [[266.96067238]] accuracy 0.7272727272727273\n","Iteration 45249 - loss value [[269.06828858]] accuracy 0.7272727272727273\n","Iteration 45250 - loss value [[268.69997203]] accuracy 0.7272727272727273\n","Iteration 45251 - loss value [[270.7881283]] accuracy 0.7272727272727273\n","Iteration 45252 - loss value [[270.02198253]] accuracy 0.7272727272727273\n","Iteration 45253 - loss value [[272.63499073]] accuracy 0.7272727272727273\n","Iteration 45254 - loss value [[269.85327361]] accuracy 0.7272727272727273\n","Iteration 45255 - loss value [[270.63279682]] accuracy 0.7272727272727273\n","Iteration 45256 - loss value [[269.97214169]] accuracy 0.7272727272727273\n","Iteration 45257 - loss value [[273.03967461]] accuracy 0.7272727272727273\n","Iteration 45258 - loss value [[270.64729315]] accuracy 0.7272727272727273\n","Iteration 45259 - loss value [[273.0086021]] accuracy 0.7272727272727273\n","Iteration 45260 - loss value [[270.08205051]] accuracy 0.7272727272727273\n","Iteration 45261 - loss value [[270.86497582]] accuracy 0.7272727272727273\n","Iteration 45262 - loss value [[269.81295099]] accuracy 0.7272727272727273\n","Iteration 45263 - loss value [[272.42733319]] accuracy 0.7272727272727273\n","Iteration 45264 - loss value [[269.72408031]] accuracy 0.7272727272727273\n","Iteration 45265 - loss value [[269.78657861]] accuracy 0.7272727272727273\n","Iteration 45266 - loss value [[269.02214906]] accuracy 0.7272727272727273\n","Iteration 45267 - loss value [[271.35194527]] accuracy 0.7272727272727273\n","Iteration 45268 - loss value [[268.78723154]] accuracy 0.7272727272727273\n","Iteration 45269 - loss value [[269.32188554]] accuracy 0.7272727272727273\n","Iteration 45270 - loss value [[268.93147069]] accuracy 0.7272727272727273\n","Iteration 45271 - loss value [[270.48594398]] accuracy 0.7272727272727273\n","Iteration 45272 - loss value [[270.07888161]] accuracy 0.7272727272727273\n","Iteration 45273 - loss value [[273.31252054]] accuracy 0.7272727272727273\n","Iteration 45274 - loss value [[271.12122076]] accuracy 0.7272727272727273\n","Iteration 45275 - loss value [[273.44238914]] accuracy 0.7272727272727273\n","Iteration 45276 - loss value [[270.79326147]] accuracy 0.7272727272727273\n","Iteration 45277 - loss value [[271.70536805]] accuracy 0.7272727272727273\n","Iteration 45278 - loss value [[270.78785813]] accuracy 0.7272727272727273\n","Iteration 45279 - loss value [[274.14075149]] accuracy 0.7272727272727273\n","Iteration 45280 - loss value [[272.14757305]] accuracy 0.7272727272727273\n","Iteration 45281 - loss value [[274.38702516]] accuracy 0.7272727272727273\n","Iteration 45282 - loss value [[272.93530796]] accuracy 0.7272727272727273\n","Iteration 45283 - loss value [[275.7454654]] accuracy 0.7272727272727273\n","Iteration 45284 - loss value [[274.96855589]] accuracy 0.7272727272727273\n","Iteration 45285 - loss value [[277.78653233]] accuracy 0.7272727272727273\n","Iteration 45286 - loss value [[277.53865929]] accuracy 0.7272727272727273\n","Iteration 45287 - loss value [[280.23210767]] accuracy 0.7272727272727273\n","Iteration 45288 - loss value [[280.38492903]] accuracy 0.7272727272727273\n","Iteration 45289 - loss value [[281.80391839]] accuracy 0.7272727272727273\n","Iteration 45290 - loss value [[281.14905064]] accuracy 0.7272727272727273\n","Iteration 45291 - loss value [[282.69935258]] accuracy 0.7272727272727273\n","Iteration 45292 - loss value [[281.16715278]] accuracy 0.7272727272727273\n","Iteration 45293 - loss value [[282.06028802]] accuracy 0.7272727272727273\n","Iteration 45294 - loss value [[279.97624179]] accuracy 0.7272727272727273\n","Iteration 45295 - loss value [[280.10874774]] accuracy 0.7272727272727273\n","Iteration 45296 - loss value [[278.76504276]] accuracy 0.7272727272727273\n","Iteration 45297 - loss value [[279.68626757]] accuracy 0.7272727272727273\n","Iteration 45298 - loss value [[277.67453203]] accuracy 0.7272727272727273\n","Iteration 45299 - loss value [[280.05380301]] accuracy 0.7272727272727273\n","Iteration 45300 - loss value [[278.3515363]] accuracy 0.7272727272727273\n","Iteration 45301 - loss value [[279.63447178]] accuracy 0.7272727272727273\n","Iteration 45302 - loss value [[277.54746476]] accuracy 0.7272727272727273\n","Iteration 45303 - loss value [[279.04155896]] accuracy 0.7272727272727273\n","Iteration 45304 - loss value [[277.61725716]] accuracy 0.7272727272727273\n","Iteration 45305 - loss value [[279.43433735]] accuracy 0.7272727272727273\n","Iteration 45306 - loss value [[277.4106775]] accuracy 0.7272727272727273\n","Iteration 45307 - loss value [[278.65821937]] accuracy 0.7272727272727273\n","Iteration 45308 - loss value [[277.03470137]] accuracy 0.7272727272727273\n","Iteration 45309 - loss value [[277.91390186]] accuracy 0.7272727272727273\n","Iteration 45310 - loss value [[276.52598833]] accuracy 0.7272727272727273\n","Iteration 45311 - loss value [[276.99498094]] accuracy 0.7272727272727273\n","Iteration 45312 - loss value [[274.46980114]] accuracy 0.7272727272727273\n","Iteration 45313 - loss value [[274.44285255]] accuracy 0.7272727272727273\n","Iteration 45314 - loss value [[271.32776781]] accuracy 0.7272727272727273\n","Iteration 45315 - loss value [[271.98207006]] accuracy 0.7272727272727273\n","Iteration 45316 - loss value [[269.82314736]] accuracy 0.7272727272727273\n","Iteration 45317 - loss value [[270.41371298]] accuracy 0.7272727272727273\n","Iteration 45318 - loss value [[269.26896459]] accuracy 0.7272727272727273\n","Iteration 45319 - loss value [[270.33078906]] accuracy 0.7272727272727273\n","Iteration 45320 - loss value [[269.10297613]] accuracy 0.7272727272727273\n","Iteration 45321 - loss value [[270.62747772]] accuracy 0.7272727272727273\n","Iteration 45322 - loss value [[269.47784432]] accuracy 0.7272727272727273\n","Iteration 45323 - loss value [[271.41742759]] accuracy 0.7272727272727273\n","Iteration 45324 - loss value [[269.65214322]] accuracy 0.7272727272727273\n","Iteration 45325 - loss value [[270.92489033]] accuracy 0.7272727272727273\n","Iteration 45326 - loss value [[270.13137999]] accuracy 0.7272727272727273\n","Iteration 45327 - loss value [[272.79254009]] accuracy 0.7272727272727273\n","Iteration 45328 - loss value [[269.81722101]] accuracy 0.7272727272727273\n","Iteration 45329 - loss value [[269.60469318]] accuracy 0.7272727272727273\n","Iteration 45330 - loss value [[268.97673099]] accuracy 0.7272727272727273\n","Iteration 45331 - loss value [[270.11436332]] accuracy 0.7272727272727273\n","Iteration 45332 - loss value [[268.86650275]] accuracy 0.7272727272727273\n","Iteration 45333 - loss value [[269.78494015]] accuracy 0.7272727272727273\n","Iteration 45334 - loss value [[269.22148316]] accuracy 0.7272727272727273\n","Iteration 45335 - loss value [[271.78703206]] accuracy 0.7272727272727273\n","Iteration 45336 - loss value [[269.03629608]] accuracy 0.7272727272727273\n","Iteration 45337 - loss value [[269.11694549]] accuracy 0.7272727272727273\n","Iteration 45338 - loss value [[268.09644394]] accuracy 0.7272727272727273\n","Iteration 45339 - loss value [[268.4686437]] accuracy 0.7272727272727273\n","Iteration 45340 - loss value [[267.71228015]] accuracy 0.7272727272727273\n","Iteration 45341 - loss value [[268.93414827]] accuracy 0.7272727272727273\n","Iteration 45342 - loss value [[268.27930464]] accuracy 0.7272727272727273\n","Iteration 45343 - loss value [[270.14597056]] accuracy 0.7272727272727273\n","Iteration 45344 - loss value [[269.90296363]] accuracy 0.7272727272727273\n","Iteration 45345 - loss value [[273.1263562]] accuracy 0.7272727272727273\n","Iteration 45346 - loss value [[270.84956681]] accuracy 0.7272727272727273\n","Iteration 45347 - loss value [[273.11779282]] accuracy 0.7272727272727273\n","Iteration 45348 - loss value [[270.21351573]] accuracy 0.7272727272727273\n","Iteration 45349 - loss value [[270.71228277]] accuracy 0.7272727272727273\n","Iteration 45350 - loss value [[270.30703578]] accuracy 0.7272727272727273\n","Iteration 45351 - loss value [[273.69840283]] accuracy 0.7272727272727273\n","Iteration 45352 - loss value [[271.41088726]] accuracy 0.7272727272727273\n","Iteration 45353 - loss value [[273.28327114]] accuracy 0.7272727272727273\n","Iteration 45354 - loss value [[270.94333779]] accuracy 0.7272727272727273\n","Iteration 45355 - loss value [[272.49782371]] accuracy 0.7272727272727273\n","Iteration 45356 - loss value [[270.39010278]] accuracy 0.7272727272727273\n","Iteration 45357 - loss value [[272.44954381]] accuracy 0.7272727272727273\n","Iteration 45358 - loss value [[270.12953486]] accuracy 0.7272727272727273\n","Iteration 45359 - loss value [[271.51750938]] accuracy 0.7272727272727273\n","Iteration 45360 - loss value [[270.48302314]] accuracy 0.7272727272727273\n","Iteration 45361 - loss value [[273.67886966]] accuracy 0.7272727272727273\n","Iteration 45362 - loss value [[271.69928658]] accuracy 0.7272727272727273\n","Iteration 45363 - loss value [[273.80199284]] accuracy 0.7272727272727273\n","Iteration 45364 - loss value [[271.06625826]] accuracy 0.7272727272727273\n","Iteration 45365 - loss value [[271.70035909]] accuracy 0.7272727272727273\n","Iteration 45366 - loss value [[270.49499529]] accuracy 0.7272727272727273\n","Iteration 45367 - loss value [[273.20624658]] accuracy 0.7272727272727273\n","Iteration 45368 - loss value [[270.78201139]] accuracy 0.7272727272727273\n","Iteration 45369 - loss value [[271.51845727]] accuracy 0.7272727272727273\n","Iteration 45370 - loss value [[270.52119597]] accuracy 0.7272727272727273\n","Iteration 45371 - loss value [[273.38966036]] accuracy 0.7272727272727273\n","Iteration 45372 - loss value [[271.52519124]] accuracy 0.7272727272727273\n","Iteration 45373 - loss value [[273.83223331]] accuracy 0.7272727272727273\n","Iteration 45374 - loss value [[271.66440569]] accuracy 0.7272727272727273\n","Iteration 45375 - loss value [[272.88833631]] accuracy 0.7272727272727273\n","Iteration 45376 - loss value [[270.53172406]] accuracy 0.7272727272727273\n","Iteration 45377 - loss value [[272.16230268]] accuracy 0.7272727272727273\n","Iteration 45378 - loss value [[270.75229235]] accuracy 0.7272727272727273\n","Iteration 45379 - loss value [[273.97614998]] accuracy 0.7272727272727273\n","Iteration 45380 - loss value [[271.95704642]] accuracy 0.7272727272727273\n","Iteration 45381 - loss value [[273.85833453]] accuracy 0.7272727272727273\n","Iteration 45382 - loss value [[271.66173221]] accuracy 0.7272727272727273\n","Iteration 45383 - loss value [[272.63340075]] accuracy 0.7272727272727273\n","Iteration 45384 - loss value [[270.2596184]] accuracy 0.7272727272727273\n","Iteration 45385 - loss value [[271.19378789]] accuracy 0.7272727272727273\n","Iteration 45386 - loss value [[270.10760471]] accuracy 0.7272727272727273\n","Iteration 45387 - loss value [[273.19383031]] accuracy 0.7272727272727273\n","Iteration 45388 - loss value [[270.95985889]] accuracy 0.7272727272727273\n","Iteration 45389 - loss value [[272.06968502]] accuracy 0.7272727272727273\n","Iteration 45390 - loss value [[271.06533406]] accuracy 0.7272727272727273\n","Iteration 45391 - loss value [[274.74372776]] accuracy 0.7272727272727273\n","Iteration 45392 - loss value [[273.38670751]] accuracy 0.7272727272727273\n","Iteration 45393 - loss value [[275.29939963]] accuracy 0.7272727272727273\n","Iteration 45394 - loss value [[274.18516356]] accuracy 0.7272727272727273\n","Iteration 45395 - loss value [[276.50089538]] accuracy 0.7272727272727273\n","Iteration 45396 - loss value [[276.24791469]] accuracy 0.7272727272727273\n","Iteration 45397 - loss value [[279.47823385]] accuracy 0.7272727272727273\n","Iteration 45398 - loss value [[279.50198019]] accuracy 0.7272727272727273\n","Iteration 45399 - loss value [[281.08844322]] accuracy 0.7272727272727273\n","Iteration 45400 - loss value [[280.48021753]] accuracy 0.7272727272727273\n","Iteration 45401 - loss value [[281.6431337]] accuracy 0.7272727272727273\n","Iteration 45402 - loss value [[280.93760762]] accuracy 0.7272727272727273\n","Iteration 45403 - loss value [[282.02591822]] accuracy 0.7272727272727273\n","Iteration 45404 - loss value [[281.01662752]] accuracy 0.7272727272727273\n","Iteration 45405 - loss value [[282.1092073]] accuracy 0.7272727272727273\n","Iteration 45406 - loss value [[280.78912953]] accuracy 0.7272727272727273\n","Iteration 45407 - loss value [[281.54024874]] accuracy 0.7272727272727273\n","Iteration 45408 - loss value [[280.26418]] accuracy 0.7272727272727273\n","Iteration 45409 - loss value [[280.97192705]] accuracy 0.7272727272727273\n","Iteration 45410 - loss value [[278.74655319]] accuracy 0.7272727272727273\n","Iteration 45411 - loss value [[279.81894983]] accuracy 0.7272727272727273\n","Iteration 45412 - loss value [[278.09067106]] accuracy 0.7272727272727273\n","Iteration 45413 - loss value [[280.26326883]] accuracy 0.7272727272727273\n","Iteration 45414 - loss value [[279.00798282]] accuracy 0.7272727272727273\n","Iteration 45415 - loss value [[279.94189638]] accuracy 0.7272727272727273\n","Iteration 45416 - loss value [[277.92929161]] accuracy 0.7272727272727273\n","Iteration 45417 - loss value [[279.59218774]] accuracy 0.7272727272727273\n","Iteration 45418 - loss value [[277.54736535]] accuracy 0.7272727272727273\n","Iteration 45419 - loss value [[278.66887704]] accuracy 0.7272727272727273\n","Iteration 45420 - loss value [[277.23674803]] accuracy 0.7272727272727273\n","Iteration 45421 - loss value [[278.9241863]] accuracy 0.7272727272727273\n","Iteration 45422 - loss value [[277.32826546]] accuracy 0.7272727272727273\n","Iteration 45423 - loss value [[279.03952669]] accuracy 0.7272727272727273\n","Iteration 45424 - loss value [[278.10505689]] accuracy 0.7272727272727273\n","Iteration 45425 - loss value [[279.51249111]] accuracy 0.7272727272727273\n","Iteration 45426 - loss value [[277.26950312]] accuracy 0.7272727272727273\n","Iteration 45427 - loss value [[277.98094214]] accuracy 0.7272727272727273\n","Iteration 45428 - loss value [[275.19002083]] accuracy 0.7272727272727273\n","Iteration 45429 - loss value [[273.87905391]] accuracy 0.7272727272727273\n","Iteration 45430 - loss value [[270.64903855]] accuracy 0.7272727272727273\n","Iteration 45431 - loss value [[271.33003154]] accuracy 0.7272727272727273\n","Iteration 45432 - loss value [[269.03837583]] accuracy 0.7272727272727273\n","Iteration 45433 - loss value [[269.13069452]] accuracy 0.7272727272727273\n","Iteration 45434 - loss value [[267.6544297]] accuracy 0.7272727272727273\n","Iteration 45435 - loss value [[267.83971448]] accuracy 0.7272727272727273\n","Iteration 45436 - loss value [[267.74622727]] accuracy 0.7272727272727273\n","Iteration 45437 - loss value [[268.96940834]] accuracy 0.7272727272727273\n","Iteration 45438 - loss value [[267.80686836]] accuracy 0.7272727272727273\n","Iteration 45439 - loss value [[268.0089125]] accuracy 0.7272727272727273\n","Iteration 45440 - loss value [[267.29863891]] accuracy 0.7272727272727273\n","Iteration 45441 - loss value [[268.21294883]] accuracy 0.7272727272727273\n","Iteration 45442 - loss value [[267.16885785]] accuracy 0.7272727272727273\n","Iteration 45443 - loss value [[267.76247744]] accuracy 0.7272727272727273\n","Iteration 45444 - loss value [[266.834792]] accuracy 0.7272727272727273\n","Iteration 45445 - loss value [[267.0353321]] accuracy 0.7272727272727273\n","Iteration 45446 - loss value [[266.15700647]] accuracy 0.7272727272727273\n","Iteration 45447 - loss value [[267.71790092]] accuracy 0.7272727272727273\n","Iteration 45448 - loss value [[266.63548292]] accuracy 0.7272727272727273\n","Iteration 45449 - loss value [[267.17425751]] accuracy 0.7272727272727273\n","Iteration 45450 - loss value [[266.6352893]] accuracy 0.7272727272727273\n","Iteration 45451 - loss value [[268.17432235]] accuracy 0.7272727272727273\n","Iteration 45452 - loss value [[266.95797339]] accuracy 0.7272727272727273\n","Iteration 45453 - loss value [[267.57916194]] accuracy 0.7272727272727273\n","Iteration 45454 - loss value [[266.93025471]] accuracy 0.7272727272727273\n","Iteration 45455 - loss value [[267.74199623]] accuracy 0.7272727272727273\n","Iteration 45456 - loss value [[267.72264565]] accuracy 0.7272727272727273\n","Iteration 45457 - loss value [[270.39981269]] accuracy 0.7272727272727273\n","Iteration 45458 - loss value [[270.20624212]] accuracy 0.7272727272727273\n","Iteration 45459 - loss value [[273.06516827]] accuracy 0.7272727272727273\n","Iteration 45460 - loss value [[270.94681946]] accuracy 0.7272727272727273\n","Iteration 45461 - loss value [[273.59549986]] accuracy 0.7272727272727273\n","Iteration 45462 - loss value [[271.2937054]] accuracy 0.7272727272727273\n","Iteration 45463 - loss value [[273.83541379]] accuracy 0.7272727272727273\n","Iteration 45464 - loss value [[271.71812933]] accuracy 0.7272727272727273\n","Iteration 45465 - loss value [[274.37554789]] accuracy 0.7272727272727273\n","Iteration 45466 - loss value [[271.85013782]] accuracy 0.7272727272727273\n","Iteration 45467 - loss value [[273.68702398]] accuracy 0.7272727272727273\n","Iteration 45468 - loss value [[271.23096961]] accuracy 0.7272727272727273\n","Iteration 45469 - loss value [[272.36932004]] accuracy 0.7272727272727273\n","Iteration 45470 - loss value [[269.89036171]] accuracy 0.7272727272727273\n","Iteration 45471 - loss value [[270.87660171]] accuracy 0.7272727272727273\n","Iteration 45472 - loss value [[269.72253923]] accuracy 0.7272727272727273\n","Iteration 45473 - loss value [[272.48199605]] accuracy 0.7272727272727273\n","Iteration 45474 - loss value [[270.08203838]] accuracy 0.7272727272727273\n","Iteration 45475 - loss value [[271.18689707]] accuracy 0.7272727272727273\n","Iteration 45476 - loss value [[270.14735488]] accuracy 0.7272727272727273\n","Iteration 45477 - loss value [[273.17171854]] accuracy 0.7272727272727273\n","Iteration 45478 - loss value [[270.64142058]] accuracy 0.7272727272727273\n","Iteration 45479 - loss value [[272.09104688]] accuracy 0.7272727272727273\n","Iteration 45480 - loss value [[271.01711868]] accuracy 0.7272727272727273\n","Iteration 45481 - loss value [[274.64257067]] accuracy 0.7272727272727273\n","Iteration 45482 - loss value [[272.37276318]] accuracy 0.7272727272727273\n","Iteration 45483 - loss value [[273.72685131]] accuracy 0.7272727272727273\n","Iteration 45484 - loss value [[271.36407173]] accuracy 0.7272727272727273\n","Iteration 45485 - loss value [[273.46418753]] accuracy 0.7272727272727273\n","Iteration 45486 - loss value [[271.46332677]] accuracy 0.7272727272727273\n","Iteration 45487 - loss value [[273.64645366]] accuracy 0.7272727272727273\n","Iteration 45488 - loss value [[272.08322482]] accuracy 0.7272727272727273\n","Iteration 45489 - loss value [[275.39644075]] accuracy 0.7272727272727273\n","Iteration 45490 - loss value [[274.23289579]] accuracy 0.7272727272727273\n","Iteration 45491 - loss value [[276.48901451]] accuracy 0.7272727272727273\n","Iteration 45492 - loss value [[275.84734392]] accuracy 0.7272727272727273\n","Iteration 45493 - loss value [[278.71611996]] accuracy 0.7272727272727273\n","Iteration 45494 - loss value [[279.68372466]] accuracy 0.7272727272727273\n","Iteration 45495 - loss value [[281.43938937]] accuracy 0.7272727272727273\n","Iteration 45496 - loss value [[280.9454733]] accuracy 0.7272727272727273\n","Iteration 45497 - loss value [[282.255816]] accuracy 0.7272727272727273\n","Iteration 45498 - loss value [[280.55937172]] accuracy 0.7272727272727273\n","Iteration 45499 - loss value [[281.23404295]] accuracy 0.7272727272727273\n","Iteration 45500 - loss value [[279.74818038]] accuracy 0.7272727272727273\n","Iteration 45501 - loss value [[280.40502823]] accuracy 0.7272727272727273\n","Iteration 45502 - loss value [[279.19697483]] accuracy 0.7272727272727273\n","Iteration 45503 - loss value [[279.9353434]] accuracy 0.7272727272727273\n","Iteration 45504 - loss value [[278.4485382]] accuracy 0.7272727272727273\n","Iteration 45505 - loss value [[279.7743296]] accuracy 0.7272727272727273\n","Iteration 45506 - loss value [[278.10142164]] accuracy 0.7272727272727273\n","Iteration 45507 - loss value [[279.83941941]] accuracy 0.7272727272727273\n","Iteration 45508 - loss value [[278.05137495]] accuracy 0.7272727272727273\n","Iteration 45509 - loss value [[279.24696087]] accuracy 0.7272727272727273\n","Iteration 45510 - loss value [[278.51960499]] accuracy 0.7272727272727273\n","Iteration 45511 - loss value [[280.16956845]] accuracy 0.7272727272727273\n","Iteration 45512 - loss value [[278.69532526]] accuracy 0.7272727272727273\n","Iteration 45513 - loss value [[279.04422511]] accuracy 0.7272727272727273\n","Iteration 45514 - loss value [[277.60907569]] accuracy 0.7272727272727273\n","Iteration 45515 - loss value [[278.64052295]] accuracy 0.7272727272727273\n","Iteration 45516 - loss value [[276.7985829]] accuracy 0.7272727272727273\n","Iteration 45517 - loss value [[277.56386222]] accuracy 0.7272727272727273\n","Iteration 45518 - loss value [[274.9088274]] accuracy 0.7272727272727273\n","Iteration 45519 - loss value [[274.39725959]] accuracy 0.7272727272727273\n","Iteration 45520 - loss value [[270.71222952]] accuracy 0.7272727272727273\n","Iteration 45521 - loss value [[271.06828717]] accuracy 0.7272727272727273\n","Iteration 45522 - loss value [[269.13537704]] accuracy 0.7272727272727273\n","Iteration 45523 - loss value [[269.12695255]] accuracy 0.7272727272727273\n","Iteration 45524 - loss value [[267.53669824]] accuracy 0.7272727272727273\n","Iteration 45525 - loss value [[267.57728175]] accuracy 0.7272727272727273\n","Iteration 45526 - loss value [[267.23812438]] accuracy 0.7272727272727273\n","Iteration 45527 - loss value [[268.96781928]] accuracy 0.7272727272727273\n","Iteration 45528 - loss value [[267.701947]] accuracy 0.7272727272727273\n","Iteration 45529 - loss value [[268.02605593]] accuracy 0.7272727272727273\n","Iteration 45530 - loss value [[267.27463014]] accuracy 0.7272727272727273\n","Iteration 45531 - loss value [[267.47649781]] accuracy 0.7272727272727273\n","Iteration 45532 - loss value [[266.83577061]] accuracy 0.7272727272727273\n","Iteration 45533 - loss value [[268.51059793]] accuracy 0.7272727272727273\n","Iteration 45534 - loss value [[267.17021886]] accuracy 0.7272727272727273\n","Iteration 45535 - loss value [[267.65920018]] accuracy 0.7272727272727273\n","Iteration 45536 - loss value [[267.50107943]] accuracy 0.7272727272727273\n","Iteration 45537 - loss value [[270.01954641]] accuracy 0.7272727272727273\n","Iteration 45538 - loss value [[269.57669822]] accuracy 0.7272727272727273\n","Iteration 45539 - loss value [[272.02967748]] accuracy 0.7272727272727273\n","Iteration 45540 - loss value [[269.25415331]] accuracy 0.7272727272727273\n","Iteration 45541 - loss value [[269.84379838]] accuracy 0.7272727272727273\n","Iteration 45542 - loss value [[270.14754457]] accuracy 0.7272727272727273\n","Iteration 45543 - loss value [[273.40099639]] accuracy 0.7272727272727273\n","Iteration 45544 - loss value [[270.76674086]] accuracy 0.7272727272727273\n","Iteration 45545 - loss value [[273.20418448]] accuracy 0.7272727272727273\n","Iteration 45546 - loss value [[270.45393386]] accuracy 0.7272727272727273\n","Iteration 45547 - loss value [[271.54624943]] accuracy 0.7272727272727273\n","Iteration 45548 - loss value [[270.39825596]] accuracy 0.7272727272727273\n","Iteration 45549 - loss value [[272.4476401]] accuracy 0.7272727272727273\n","Iteration 45550 - loss value [[270.07477069]] accuracy 0.7272727272727273\n","Iteration 45551 - loss value [[271.7493667]] accuracy 0.7272727272727273\n","Iteration 45552 - loss value [[270.3656959]] accuracy 0.7272727272727273\n","Iteration 45553 - loss value [[273.50530387]] accuracy 0.7272727272727273\n","Iteration 45554 - loss value [[270.92257623]] accuracy 0.7272727272727273\n","Iteration 45555 - loss value [[272.61656144]] accuracy 0.7272727272727273\n","Iteration 45556 - loss value [[269.95130542]] accuracy 0.7272727272727273\n","Iteration 45557 - loss value [[270.53179329]] accuracy 0.7272727272727273\n","Iteration 45558 - loss value [[269.75313118]] accuracy 0.7272727272727273\n","Iteration 45559 - loss value [[272.08423276]] accuracy 0.7272727272727273\n","Iteration 45560 - loss value [[269.67732393]] accuracy 0.7272727272727273\n","Iteration 45561 - loss value [[270.41241225]] accuracy 0.7272727272727273\n","Iteration 45562 - loss value [[269.54424193]] accuracy 0.7272727272727273\n","Iteration 45563 - loss value [[271.896068]] accuracy 0.7272727272727273\n","Iteration 45564 - loss value [[269.35658981]] accuracy 0.7272727272727273\n","Iteration 45565 - loss value [[269.82298209]] accuracy 0.7272727272727273\n","Iteration 45566 - loss value [[269.21196662]] accuracy 0.7272727272727273\n","Iteration 45567 - loss value [[271.98822886]] accuracy 0.7272727272727273\n","Iteration 45568 - loss value [[270.02940386]] accuracy 0.7272727272727273\n","Iteration 45569 - loss value [[271.97754455]] accuracy 0.7272727272727273\n","Iteration 45570 - loss value [[270.77748458]] accuracy 0.7272727272727273\n","Iteration 45571 - loss value [[274.15128308]] accuracy 0.7272727272727273\n","Iteration 45572 - loss value [[272.21270602]] accuracy 0.7272727272727273\n","Iteration 45573 - loss value [[274.66856412]] accuracy 0.7272727272727273\n","Iteration 45574 - loss value [[273.57864002]] accuracy 0.7272727272727273\n","Iteration 45575 - loss value [[275.68248274]] accuracy 0.7272727272727273\n","Iteration 45576 - loss value [[274.38598569]] accuracy 0.7272727272727273\n","Iteration 45577 - loss value [[276.41124189]] accuracy 0.7272727272727273\n","Iteration 45578 - loss value [[275.97792526]] accuracy 0.7272727272727273\n","Iteration 45579 - loss value [[278.4225515]] accuracy 0.7272727272727273\n","Iteration 45580 - loss value [[278.62600028]] accuracy 0.7272727272727273\n","Iteration 45581 - loss value [[280.73237522]] accuracy 0.7272727272727273\n","Iteration 45582 - loss value [[280.26273048]] accuracy 0.7272727272727273\n","Iteration 45583 - loss value [[281.84350328]] accuracy 0.7272727272727273\n","Iteration 45584 - loss value [[281.04507841]] accuracy 0.7272727272727273\n","Iteration 45585 - loss value [[281.7239359]] accuracy 0.7272727272727273\n","Iteration 45586 - loss value [[280.46395363]] accuracy 0.7272727272727273\n","Iteration 45587 - loss value [[281.26599058]] accuracy 0.7272727272727273\n","Iteration 45588 - loss value [[279.43299372]] accuracy 0.7272727272727273\n","Iteration 45589 - loss value [[279.40422706]] accuracy 0.7272727272727273\n","Iteration 45590 - loss value [[278.38681167]] accuracy 0.7272727272727273\n","Iteration 45591 - loss value [[279.18241992]] accuracy 0.7272727272727273\n","Iteration 45592 - loss value [[277.91126141]] accuracy 0.7272727272727273\n","Iteration 45593 - loss value [[278.71231387]] accuracy 0.7272727272727273\n","Iteration 45594 - loss value [[277.35416704]] accuracy 0.7272727272727273\n","Iteration 45595 - loss value [[278.81852902]] accuracy 0.7272727272727273\n","Iteration 45596 - loss value [[277.13348988]] accuracy 0.7272727272727273\n","Iteration 45597 - loss value [[279.13042328]] accuracy 0.7272727272727273\n","Iteration 45598 - loss value [[277.61754806]] accuracy 0.7272727272727273\n","Iteration 45599 - loss value [[278.53758241]] accuracy 0.7272727272727273\n","Iteration 45600 - loss value [[276.7394435]] accuracy 0.7272727272727273\n","Iteration 45601 - loss value [[278.50526544]] accuracy 0.7272727272727273\n","Iteration 45602 - loss value [[276.75594761]] accuracy 0.7272727272727273\n","Iteration 45603 - loss value [[277.32515388]] accuracy 0.7272727272727273\n","Iteration 45604 - loss value [[275.27920886]] accuracy 0.7272727272727273\n","Iteration 45605 - loss value [[275.28226282]] accuracy 0.7272727272727273\n","Iteration 45606 - loss value [[272.46656972]] accuracy 0.7272727272727273\n","Iteration 45607 - loss value [[272.43012076]] accuracy 0.7272727272727273\n","Iteration 45608 - loss value [[270.53799205]] accuracy 0.7272727272727273\n","Iteration 45609 - loss value [[272.11818371]] accuracy 0.7272727272727273\n","Iteration 45610 - loss value [[268.92552574]] accuracy 0.7272727272727273\n","Iteration 45611 - loss value [[269.19467834]] accuracy 0.7272727272727273\n","Iteration 45612 - loss value [[267.90335413]] accuracy 0.7272727272727273\n","Iteration 45613 - loss value [[267.9525803]] accuracy 0.7272727272727273\n","Iteration 45614 - loss value [[267.22304003]] accuracy 0.7272727272727273\n","Iteration 45615 - loss value [[267.71109236]] accuracy 0.7272727272727273\n","Iteration 45616 - loss value [[266.84705059]] accuracy 0.7272727272727273\n","Iteration 45617 - loss value [[267.38752381]] accuracy 0.7272727272727273\n","Iteration 45618 - loss value [[266.41625288]] accuracy 0.7272727272727273\n","Iteration 45619 - loss value [[267.64203067]] accuracy 0.7272727272727273\n","Iteration 45620 - loss value [[266.47243874]] accuracy 0.7272727272727273\n","Iteration 45621 - loss value [[266.85265758]] accuracy 0.7272727272727273\n","Iteration 45622 - loss value [[266.25772333]] accuracy 0.7272727272727273\n","Iteration 45623 - loss value [[267.48039293]] accuracy 0.7272727272727273\n","Iteration 45624 - loss value [[266.53307745]] accuracy 0.7272727272727273\n","Iteration 45625 - loss value [[267.36939912]] accuracy 0.7272727272727273\n","Iteration 45626 - loss value [[267.03605428]] accuracy 0.7272727272727273\n","Iteration 45627 - loss value [[269.3166196]] accuracy 0.7272727272727273\n","Iteration 45628 - loss value [[269.25059117]] accuracy 0.7272727272727273\n","Iteration 45629 - loss value [[272.01615838]] accuracy 0.7272727272727273\n","Iteration 45630 - loss value [[269.34608918]] accuracy 0.7272727272727273\n","Iteration 45631 - loss value [[270.53407568]] accuracy 0.7272727272727273\n","Iteration 45632 - loss value [[269.87356625]] accuracy 0.7272727272727273\n","Iteration 45633 - loss value [[273.05422217]] accuracy 0.7272727272727273\n","Iteration 45634 - loss value [[270.49395196]] accuracy 0.7272727272727273\n","Iteration 45635 - loss value [[272.04531336]] accuracy 0.7272727272727273\n","Iteration 45636 - loss value [[269.73402808]] accuracy 0.7272727272727273\n","Iteration 45637 - loss value [[270.90113035]] accuracy 0.7272727272727273\n","Iteration 45638 - loss value [[269.75592434]] accuracy 0.7272727272727273\n","Iteration 45639 - loss value [[272.39600819]] accuracy 0.7272727272727273\n","Iteration 45640 - loss value [[270.31380704]] accuracy 0.7272727272727273\n","Iteration 45641 - loss value [[272.34712508]] accuracy 0.7272727272727273\n","Iteration 45642 - loss value [[270.01763727]] accuracy 0.7272727272727273\n","Iteration 45643 - loss value [[271.19987984]] accuracy 0.7272727272727273\n","Iteration 45644 - loss value [[270.13181023]] accuracy 0.7272727272727273\n","Iteration 45645 - loss value [[273.33350926]] accuracy 0.7272727272727273\n","Iteration 45646 - loss value [[270.80843708]] accuracy 0.7272727272727273\n","Iteration 45647 - loss value [[272.59827002]] accuracy 0.7272727272727273\n","Iteration 45648 - loss value [[270.30030945]] accuracy 0.7272727272727273\n","Iteration 45649 - loss value [[272.12765749]] accuracy 0.7272727272727273\n","Iteration 45650 - loss value [[269.79111201]] accuracy 0.7272727272727273\n","Iteration 45651 - loss value [[271.1493917]] accuracy 0.7272727272727273\n","Iteration 45652 - loss value [[269.99646371]] accuracy 0.7272727272727273\n","Iteration 45653 - loss value [[273.01007033]] accuracy 0.7272727272727273\n","Iteration 45654 - loss value [[270.39448953]] accuracy 0.7272727272727273\n","Iteration 45655 - loss value [[271.70712396]] accuracy 0.7272727272727273\n","Iteration 45656 - loss value [[270.26803442]] accuracy 0.7272727272727273\n","Iteration 45657 - loss value [[272.48858465]] accuracy 0.7272727272727273\n","Iteration 45658 - loss value [[269.9642521]] accuracy 0.7272727272727273\n","Iteration 45659 - loss value [[270.34085921]] accuracy 0.7272727272727273\n","Iteration 45660 - loss value [[269.17309474]] accuracy 0.7272727272727273\n","Iteration 45661 - loss value [[270.8641197]] accuracy 0.7272727272727273\n","Iteration 45662 - loss value [[269.12322206]] accuracy 0.7272727272727273\n","Iteration 45663 - loss value [[269.46255124]] accuracy 0.7272727272727273\n","Iteration 45664 - loss value [[268.5563656]] accuracy 0.7272727272727273\n","Iteration 45665 - loss value [[270.85845446]] accuracy 0.7272727272727273\n","Iteration 45666 - loss value [[269.09463646]] accuracy 0.7272727272727273\n","Iteration 45667 - loss value [[269.97489335]] accuracy 0.7272727272727273\n","Iteration 45668 - loss value [[269.3721805]] accuracy 0.7272727272727273\n","Iteration 45669 - loss value [[272.5695326]] accuracy 0.7272727272727273\n","Iteration 45670 - loss value [[270.26819202]] accuracy 0.7272727272727273\n","Iteration 45671 - loss value [[272.16844369]] accuracy 0.7272727272727273\n","Iteration 45672 - loss value [[270.01096581]] accuracy 0.7272727272727273\n","Iteration 45673 - loss value [[271.65102182]] accuracy 0.7272727272727273\n","Iteration 45674 - loss value [[270.31197776]] accuracy 0.7272727272727273\n","Iteration 45675 - loss value [[272.64554004]] accuracy 0.7272727272727273\n","Iteration 45676 - loss value [[270.2077039]] accuracy 0.7272727272727273\n","Iteration 45677 - loss value [[271.09328849]] accuracy 0.7272727272727273\n","Iteration 45678 - loss value [[269.99044848]] accuracy 0.7272727272727273\n","Iteration 45679 - loss value [[273.51779433]] accuracy 0.7272727272727273\n","Iteration 45680 - loss value [[271.12678261]] accuracy 0.7272727272727273\n","Iteration 45681 - loss value [[272.51155259]] accuracy 0.7272727272727273\n","Iteration 45682 - loss value [[270.10162414]] accuracy 0.7272727272727273\n","Iteration 45683 - loss value [[270.62516636]] accuracy 0.7272727272727273\n","Iteration 45684 - loss value [[269.84643472]] accuracy 0.7272727272727273\n","Iteration 45685 - loss value [[272.78415267]] accuracy 0.7272727272727273\n","Iteration 45686 - loss value [[270.70071078]] accuracy 0.7272727272727273\n","Iteration 45687 - loss value [[272.74548608]] accuracy 0.7272727272727273\n","Iteration 45688 - loss value [[270.39542695]] accuracy 0.7272727272727273\n","Iteration 45689 - loss value [[271.85021764]] accuracy 0.7272727272727273\n","Iteration 45690 - loss value [[270.84946764]] accuracy 0.7272727272727273\n","Iteration 45691 - loss value [[274.42818789]] accuracy 0.7272727272727273\n","Iteration 45692 - loss value [[272.80770854]] accuracy 0.7272727272727273\n","Iteration 45693 - loss value [[276.00270506]] accuracy 0.7272727272727273\n","Iteration 45694 - loss value [[275.57767562]] accuracy 0.7272727272727273\n","Iteration 45695 - loss value [[278.69066621]] accuracy 0.7272727272727273\n","Iteration 45696 - loss value [[279.88175026]] accuracy 0.7272727272727273\n","Iteration 45697 - loss value [[282.26617264]] accuracy 0.7272727272727273\n","Iteration 45698 - loss value [[281.72380688]] accuracy 0.7272727272727273\n","Iteration 45699 - loss value [[283.26628002]] accuracy 0.7272727272727273\n","Iteration 45700 - loss value [[282.52813978]] accuracy 0.7272727272727273\n","Iteration 45701 - loss value [[284.18454888]] accuracy 0.7272727272727273\n","Iteration 45702 - loss value [[283.07720136]] accuracy 0.7272727272727273\n","Iteration 45703 - loss value [[284.24338748]] accuracy 0.7272727272727273\n","Iteration 45704 - loss value [[282.55761494]] accuracy 0.7272727272727273\n","Iteration 45705 - loss value [[283.55755609]] accuracy 0.7272727272727273\n","Iteration 45706 - loss value [[281.54834928]] accuracy 0.7272727272727273\n","Iteration 45707 - loss value [[281.81784239]] accuracy 0.7272727272727273\n","Iteration 45708 - loss value [[279.37642635]] accuracy 0.7272727272727273\n","Iteration 45709 - loss value [[279.00147563]] accuracy 0.7272727272727273\n","Iteration 45710 - loss value [[278.00370937]] accuracy 0.7272727272727273\n","Iteration 45711 - loss value [[279.87583778]] accuracy 0.7272727272727273\n","Iteration 45712 - loss value [[278.44028519]] accuracy 0.7272727272727273\n","Iteration 45713 - loss value [[279.86471358]] accuracy 0.7272727272727273\n","Iteration 45714 - loss value [[277.89152202]] accuracy 0.7272727272727273\n","Iteration 45715 - loss value [[279.94231213]] accuracy 0.7272727272727273\n","Iteration 45716 - loss value [[278.44915704]] accuracy 0.7272727272727273\n","Iteration 45717 - loss value [[279.68135018]] accuracy 0.7272727272727273\n","Iteration 45718 - loss value [[278.11885987]] accuracy 0.7272727272727273\n","Iteration 45719 - loss value [[279.80148811]] accuracy 0.7272727272727273\n","Iteration 45720 - loss value [[278.09791734]] accuracy 0.7272727272727273\n","Iteration 45721 - loss value [[279.51836016]] accuracy 0.7272727272727273\n","Iteration 45722 - loss value [[277.1336419]] accuracy 0.7272727272727273\n","Iteration 45723 - loss value [[278.15622771]] accuracy 0.7272727272727273\n","Iteration 45724 - loss value [[276.10839009]] accuracy 0.7272727272727273\n","Iteration 45725 - loss value [[276.1733025]] accuracy 0.7272727272727273\n","Iteration 45726 - loss value [[273.12406164]] accuracy 0.7272727272727273\n","Iteration 45727 - loss value [[273.17000624]] accuracy 0.7272727272727273\n","Iteration 45728 - loss value [[269.79338169]] accuracy 0.7272727272727273\n","Iteration 45729 - loss value [[269.81074879]] accuracy 0.7272727272727273\n","Iteration 45730 - loss value [[268.45647515]] accuracy 0.7272727272727273\n","Iteration 45731 - loss value [[269.20357755]] accuracy 0.7272727272727273\n","Iteration 45732 - loss value [[267.83013364]] accuracy 0.7272727272727273\n","Iteration 45733 - loss value [[267.83021201]] accuracy 0.7272727272727273\n","Iteration 45734 - loss value [[267.83644858]] accuracy 0.7272727272727273\n","Iteration 45735 - loss value [[269.28565881]] accuracy 0.7272727272727273\n","Iteration 45736 - loss value [[268.72683586]] accuracy 0.7272727272727273\n","Iteration 45737 - loss value [[270.91253007]] accuracy 0.7272727272727273\n","Iteration 45738 - loss value [[269.18495975]] accuracy 0.7272727272727273\n","Iteration 45739 - loss value [[270.94725657]] accuracy 0.7272727272727273\n","Iteration 45740 - loss value [[269.64500727]] accuracy 0.7272727272727273\n","Iteration 45741 - loss value [[272.05627643]] accuracy 0.7272727272727273\n","Iteration 45742 - loss value [[269.09866185]] accuracy 0.7272727272727273\n","Iteration 45743 - loss value [[269.35550382]] accuracy 0.7272727272727273\n","Iteration 45744 - loss value [[268.76426561]] accuracy 0.7272727272727273\n","Iteration 45745 - loss value [[270.33856078]] accuracy 0.7272727272727273\n","Iteration 45746 - loss value [[269.27709504]] accuracy 0.7272727272727273\n","Iteration 45747 - loss value [[272.16432566]] accuracy 0.7272727272727273\n","Iteration 45748 - loss value [[269.76738731]] accuracy 0.7272727272727273\n","Iteration 45749 - loss value [[270.63942457]] accuracy 0.7272727272727273\n","Iteration 45750 - loss value [[269.41913239]] accuracy 0.7272727272727273\n","Iteration 45751 - loss value [[271.78390598]] accuracy 0.7272727272727273\n","Iteration 45752 - loss value [[269.19178017]] accuracy 0.7272727272727273\n","Iteration 45753 - loss value [[269.32070416]] accuracy 0.7272727272727273\n","Iteration 45754 - loss value [[268.49156988]] accuracy 0.7272727272727273\n","Iteration 45755 - loss value [[269.23574777]] accuracy 0.7272727272727273\n","Iteration 45756 - loss value [[268.88977285]] accuracy 0.7272727272727273\n","Iteration 45757 - loss value [[271.31393301]] accuracy 0.7272727272727273\n","Iteration 45758 - loss value [[268.66978372]] accuracy 0.7272727272727273\n","Iteration 45759 - loss value [[268.99612855]] accuracy 0.7272727272727273\n","Iteration 45760 - loss value [[268.34400771]] accuracy 0.7272727272727273\n","Iteration 45761 - loss value [[269.54780791]] accuracy 0.7272727272727273\n","Iteration 45762 - loss value [[269.20689763]] accuracy 0.7272727272727273\n","Iteration 45763 - loss value [[271.644769]] accuracy 0.7272727272727273\n","Iteration 45764 - loss value [[269.1891004]] accuracy 0.7272727272727273\n","Iteration 45765 - loss value [[270.62244119]] accuracy 0.7272727272727273\n","Iteration 45766 - loss value [[270.03932775]] accuracy 0.7272727272727273\n","Iteration 45767 - loss value [[273.35859077]] accuracy 0.7272727272727273\n","Iteration 45768 - loss value [[270.90210901]] accuracy 0.7272727272727273\n","Iteration 45769 - loss value [[272.10842697]] accuracy 0.7272727272727273\n","Iteration 45770 - loss value [[269.7093164]] accuracy 0.7272727272727273\n","Iteration 45771 - loss value [[271.18720679]] accuracy 0.7272727272727273\n","Iteration 45772 - loss value [[270.47675078]] accuracy 0.7272727272727273\n","Iteration 45773 - loss value [[273.97976245]] accuracy 0.7272727272727273\n","Iteration 45774 - loss value [[271.6794582]] accuracy 0.7272727272727273\n","Iteration 45775 - loss value [[273.82630819]] accuracy 0.7272727272727273\n","Iteration 45776 - loss value [[271.23709318]] accuracy 0.7272727272727273\n","Iteration 45777 - loss value [[272.36309965]] accuracy 0.7272727272727273\n","Iteration 45778 - loss value [[270.00670174]] accuracy 0.7272727272727273\n","Iteration 45779 - loss value [[271.15431332]] accuracy 0.7272727272727273\n","Iteration 45780 - loss value [[269.91987233]] accuracy 0.7272727272727273\n","Iteration 45781 - loss value [[272.5280455]] accuracy 0.7272727272727273\n","Iteration 45782 - loss value [[270.38461057]] accuracy 0.7272727272727273\n","Iteration 45783 - loss value [[271.62970894]] accuracy 0.7272727272727273\n","Iteration 45784 - loss value [[270.49573249]] accuracy 0.7272727272727273\n","Iteration 45785 - loss value [[273.40245168]] accuracy 0.7272727272727273\n","Iteration 45786 - loss value [[271.67625038]] accuracy 0.7272727272727273\n","Iteration 45787 - loss value [[275.56214835]] accuracy 0.7272727272727273\n","Iteration 45788 - loss value [[274.20237641]] accuracy 0.7272727272727273\n","Iteration 45789 - loss value [[275.47709643]] accuracy 0.7272727272727273\n","Iteration 45790 - loss value [[274.51884454]] accuracy 0.7272727272727273\n","Iteration 45791 - loss value [[276.79421677]] accuracy 0.7272727272727273\n","Iteration 45792 - loss value [[275.91912011]] accuracy 0.7272727272727273\n","Iteration 45793 - loss value [[278.31159444]] accuracy 0.7272727272727273\n","Iteration 45794 - loss value [[278.26566255]] accuracy 0.7272727272727273\n","Iteration 45795 - loss value [[281.76176427]] accuracy 0.7272727272727273\n","Iteration 45796 - loss value [[280.39746393]] accuracy 0.7272727272727273\n","Iteration 45797 - loss value [[281.34347371]] accuracy 0.7272727272727273\n","Iteration 45798 - loss value [[279.83587017]] accuracy 0.7272727272727273\n","Iteration 45799 - loss value [[280.65958287]] accuracy 0.7272727272727273\n","Iteration 45800 - loss value [[279.47594617]] accuracy 0.7272727272727273\n","Iteration 45801 - loss value [[280.34639662]] accuracy 0.7272727272727273\n","Iteration 45802 - loss value [[279.50587561]] accuracy 0.7272727272727273\n","Iteration 45803 - loss value [[280.2145676]] accuracy 0.7272727272727273\n","Iteration 45804 - loss value [[278.29040188]] accuracy 0.7272727272727273\n","Iteration 45805 - loss value [[279.99798739]] accuracy 0.7272727272727273\n","Iteration 45806 - loss value [[278.30171341]] accuracy 0.7272727272727273\n","Iteration 45807 - loss value [[279.89718224]] accuracy 0.7272727272727273\n","Iteration 45808 - loss value [[278.08408293]] accuracy 0.7272727272727273\n","Iteration 45809 - loss value [[279.03642587]] accuracy 0.7272727272727273\n","Iteration 45810 - loss value [[278.11169551]] accuracy 0.7272727272727273\n","Iteration 45811 - loss value [[279.04054596]] accuracy 0.7272727272727273\n","Iteration 45812 - loss value [[277.91093433]] accuracy 0.7272727272727273\n","Iteration 45813 - loss value [[278.83308193]] accuracy 0.7272727272727273\n","Iteration 45814 - loss value [[277.07923325]] accuracy 0.7272727272727273\n","Iteration 45815 - loss value [[278.16922706]] accuracy 0.7272727272727273\n","Iteration 45816 - loss value [[275.83191066]] accuracy 0.7272727272727273\n","Iteration 45817 - loss value [[275.33004668]] accuracy 0.7272727272727273\n","Iteration 45818 - loss value [[272.69365221]] accuracy 0.7272727272727273\n","Iteration 45819 - loss value [[273.01570771]] accuracy 0.7272727272727273\n","Iteration 45820 - loss value [[269.94672656]] accuracy 0.7272727272727273\n","Iteration 45821 - loss value [[270.13733012]] accuracy 0.7272727272727273\n","Iteration 45822 - loss value [[268.64876538]] accuracy 0.7272727272727273\n","Iteration 45823 - loss value [[269.16098667]] accuracy 0.7272727272727273\n","Iteration 45824 - loss value [[267.60616647]] accuracy 0.7272727272727273\n","Iteration 45825 - loss value [[268.2764052]] accuracy 0.7272727272727273\n","Iteration 45826 - loss value [[267.09708393]] accuracy 0.7272727272727273\n","Iteration 45827 - loss value [[267.16115458]] accuracy 0.7272727272727273\n","Iteration 45828 - loss value [[266.7866752]] accuracy 0.7272727272727273\n","Iteration 45829 - loss value [[267.93721191]] accuracy 0.7272727272727273\n","Iteration 45830 - loss value [[267.13015362]] accuracy 0.7272727272727273\n","Iteration 45831 - loss value [[267.47490436]] accuracy 0.7272727272727273\n","Iteration 45832 - loss value [[267.31121378]] accuracy 0.7272727272727273\n","Iteration 45833 - loss value [[269.37330308]] accuracy 0.7272727272727273\n","Iteration 45834 - loss value [[268.72619515]] accuracy 0.7272727272727273\n","Iteration 45835 - loss value [[270.71465527]] accuracy 0.7272727272727273\n","Iteration 45836 - loss value [[269.67897831]] accuracy 0.7272727272727273\n","Iteration 45837 - loss value [[272.64801974]] accuracy 0.7272727272727273\n","Iteration 45838 - loss value [[270.35910256]] accuracy 0.7272727272727273\n","Iteration 45839 - loss value [[271.45186605]] accuracy 0.7272727272727273\n","Iteration 45840 - loss value [[270.5482307]] accuracy 0.7272727272727273\n","Iteration 45841 - loss value [[273.65145795]] accuracy 0.7272727272727273\n","Iteration 45842 - loss value [[270.99596691]] accuracy 0.7272727272727273\n","Iteration 45843 - loss value [[272.47570352]] accuracy 0.7272727272727273\n","Iteration 45844 - loss value [[269.88636068]] accuracy 0.7272727272727273\n","Iteration 45845 - loss value [[271.12335686]] accuracy 0.7272727272727273\n","Iteration 45846 - loss value [[270.16063208]] accuracy 0.7272727272727273\n","Iteration 45847 - loss value [[273.20408159]] accuracy 0.7272727272727273\n","Iteration 45848 - loss value [[270.48878139]] accuracy 0.7272727272727273\n","Iteration 45849 - loss value [[271.57545977]] accuracy 0.7272727272727273\n","Iteration 45850 - loss value [[270.14888223]] accuracy 0.7272727272727273\n","Iteration 45851 - loss value [[272.09262309]] accuracy 0.7272727272727273\n","Iteration 45852 - loss value [[270.35808698]] accuracy 0.7272727272727273\n","Iteration 45853 - loss value [[271.9778683]] accuracy 0.7272727272727273\n","Iteration 45854 - loss value [[270.64756896]] accuracy 0.7272727272727273\n","Iteration 45855 - loss value [[273.92035248]] accuracy 0.7272727272727273\n","Iteration 45856 - loss value [[271.74079688]] accuracy 0.7272727272727273\n","Iteration 45857 - loss value [[273.76630124]] accuracy 0.7272727272727273\n","Iteration 45858 - loss value [[271.0613683]] accuracy 0.7272727272727273\n","Iteration 45859 - loss value [[272.12935501]] accuracy 0.7272727272727273\n","Iteration 45860 - loss value [[270.48977777]] accuracy 0.7272727272727273\n","Iteration 45861 - loss value [[272.44477695]] accuracy 0.7272727272727273\n","Iteration 45862 - loss value [[269.56525019]] accuracy 0.7272727272727273\n","Iteration 45863 - loss value [[269.61929485]] accuracy 0.7272727272727273\n","Iteration 45864 - loss value [[268.39325284]] accuracy 0.7272727272727273\n","Iteration 45865 - loss value [[268.63848232]] accuracy 0.7272727272727273\n","Iteration 45866 - loss value [[268.68793173]] accuracy 0.7272727272727273\n","Iteration 45867 - loss value [[271.42255541]] accuracy 0.7272727272727273\n","Iteration 45868 - loss value [[268.37748889]] accuracy 0.7272727272727273\n","Iteration 45869 - loss value [[268.50718013]] accuracy 0.7272727272727273\n","Iteration 45870 - loss value [[267.80331438]] accuracy 0.7272727272727273\n","Iteration 45871 - loss value [[268.18721516]] accuracy 0.7272727272727273\n","Iteration 45872 - loss value [[267.80188507]] accuracy 0.7272727272727273\n","Iteration 45873 - loss value [[270.34886648]] accuracy 0.7272727272727273\n","Iteration 45874 - loss value [[269.3229488]] accuracy 0.7272727272727273\n","Iteration 45875 - loss value [[271.55770534]] accuracy 0.7272727272727273\n","Iteration 45876 - loss value [[270.23344588]] accuracy 0.7272727272727273\n","Iteration 45877 - loss value [[272.91835206]] accuracy 0.7272727272727273\n","Iteration 45878 - loss value [[270.05523678]] accuracy 0.7272727272727273\n","Iteration 45879 - loss value [[271.26073748]] accuracy 0.7272727272727273\n","Iteration 45880 - loss value [[269.81907648]] accuracy 0.7272727272727273\n","Iteration 45881 - loss value [[271.88667468]] accuracy 0.7272727272727273\n","Iteration 45882 - loss value [[270.52862806]] accuracy 0.7272727272727273\n","Iteration 45883 - loss value [[273.18468944]] accuracy 0.7272727272727273\n","Iteration 45884 - loss value [[270.62814888]] accuracy 0.7272727272727273\n","Iteration 45885 - loss value [[271.44700072]] accuracy 0.7272727272727273\n","Iteration 45886 - loss value [[270.15043259]] accuracy 0.7272727272727273\n","Iteration 45887 - loss value [[272.79983302]] accuracy 0.7272727272727273\n","Iteration 45888 - loss value [[269.98962142]] accuracy 0.7272727272727273\n","Iteration 45889 - loss value [[270.49382931]] accuracy 0.7272727272727273\n","Iteration 45890 - loss value [[269.61382051]] accuracy 0.7272727272727273\n","Iteration 45891 - loss value [[272.88027896]] accuracy 0.7272727272727273\n","Iteration 45892 - loss value [[270.45979006]] accuracy 0.7272727272727273\n","Iteration 45893 - loss value [[272.30096654]] accuracy 0.7272727272727273\n","Iteration 45894 - loss value [[269.8158603]] accuracy 0.7272727272727273\n","Iteration 45895 - loss value [[270.12378547]] accuracy 0.7272727272727273\n","Iteration 45896 - loss value [[268.69665806]] accuracy 0.7272727272727273\n","Iteration 45897 - loss value [[269.12968612]] accuracy 0.7272727272727273\n","Iteration 45898 - loss value [[268.40722724]] accuracy 0.7272727272727273\n","Iteration 45899 - loss value [[270.65787918]] accuracy 0.7272727272727273\n","Iteration 45900 - loss value [[268.94346557]] accuracy 0.7272727272727273\n","Iteration 45901 - loss value [[269.73373016]] accuracy 0.7272727272727273\n","Iteration 45902 - loss value [[268.93577505]] accuracy 0.7272727272727273\n","Iteration 45903 - loss value [[272.0512414]] accuracy 0.7272727272727273\n","Iteration 45904 - loss value [[269.68805026]] accuracy 0.7272727272727273\n","Iteration 45905 - loss value [[270.60281714]] accuracy 0.7272727272727273\n","Iteration 45906 - loss value [[270.07369936]] accuracy 0.7272727272727273\n","Iteration 45907 - loss value [[272.98606866]] accuracy 0.7272727272727273\n","Iteration 45908 - loss value [[271.18071752]] accuracy 0.7272727272727273\n","Iteration 45909 - loss value [[273.67403134]] accuracy 0.7272727272727273\n","Iteration 45910 - loss value [[271.26737943]] accuracy 0.7272727272727273\n","Iteration 45911 - loss value [[272.71286979]] accuracy 0.7272727272727273\n","Iteration 45912 - loss value [[270.30094204]] accuracy 0.7272727272727273\n","Iteration 45913 - loss value [[271.34223481]] accuracy 0.7272727272727273\n","Iteration 45914 - loss value [[270.35597395]] accuracy 0.7272727272727273\n","Iteration 45915 - loss value [[273.32733333]] accuracy 0.7272727272727273\n","Iteration 45916 - loss value [[271.47598144]] accuracy 0.7272727272727273\n","Iteration 45917 - loss value [[273.37590746]] accuracy 0.7272727272727273\n","Iteration 45918 - loss value [[271.25038075]] accuracy 0.7272727272727273\n","Iteration 45919 - loss value [[273.18843668]] accuracy 0.7272727272727273\n","Iteration 45920 - loss value [[270.99950907]] accuracy 0.7272727272727273\n","Iteration 45921 - loss value [[272.97367651]] accuracy 0.7272727272727273\n","Iteration 45922 - loss value [[270.80207448]] accuracy 0.7272727272727273\n","Iteration 45923 - loss value [[272.6914602]] accuracy 0.7272727272727273\n","Iteration 45924 - loss value [[270.16524044]] accuracy 0.7272727272727273\n","Iteration 45925 - loss value [[270.73305699]] accuracy 0.7272727272727273\n","Iteration 45926 - loss value [[269.71244156]] accuracy 0.7272727272727273\n","Iteration 45927 - loss value [[272.93574393]] accuracy 0.7272727272727273\n","Iteration 45928 - loss value [[270.86431533]] accuracy 0.7272727272727273\n","Iteration 45929 - loss value [[272.3333312]] accuracy 0.7272727272727273\n","Iteration 45930 - loss value [[269.78630458]] accuracy 0.7272727272727273\n","Iteration 45931 - loss value [[270.69192507]] accuracy 0.7272727272727273\n","Iteration 45932 - loss value [[269.72784816]] accuracy 0.7272727272727273\n","Iteration 45933 - loss value [[273.30860239]] accuracy 0.7272727272727273\n","Iteration 45934 - loss value [[271.53668029]] accuracy 0.7272727272727273\n","Iteration 45935 - loss value [[273.90341292]] accuracy 0.7272727272727273\n","Iteration 45936 - loss value [[272.35337186]] accuracy 0.7272727272727273\n","Iteration 45937 - loss value [[276.25654868]] accuracy 0.7272727272727273\n","Iteration 45938 - loss value [[276.2782494]] accuracy 0.7272727272727273\n","Iteration 45939 - loss value [[279.60773492]] accuracy 0.7272727272727273\n","Iteration 45940 - loss value [[280.4048709]] accuracy 0.7272727272727273\n","Iteration 45941 - loss value [[282.34255641]] accuracy 0.7272727272727273\n","Iteration 45942 - loss value [[281.70170885]] accuracy 0.7272727272727273\n","Iteration 45943 - loss value [[282.75322431]] accuracy 0.7272727272727273\n","Iteration 45944 - loss value [[281.68346181]] accuracy 0.7272727272727273\n","Iteration 45945 - loss value [[282.41810296]] accuracy 0.7272727272727273\n","Iteration 45946 - loss value [[281.04379081]] accuracy 0.7272727272727273\n","Iteration 45947 - loss value [[281.86439037]] accuracy 0.7272727272727273\n","Iteration 45948 - loss value [[280.16769666]] accuracy 0.7272727272727273\n","Iteration 45949 - loss value [[280.7566458]] accuracy 0.7272727272727273\n","Iteration 45950 - loss value [[278.91850233]] accuracy 0.7272727272727273\n","Iteration 45951 - loss value [[280.82120984]] accuracy 0.7272727272727273\n","Iteration 45952 - loss value [[278.83679988]] accuracy 0.7272727272727273\n","Iteration 45953 - loss value [[280.23980625]] accuracy 0.7272727272727273\n","Iteration 45954 - loss value [[278.68012112]] accuracy 0.7272727272727273\n","Iteration 45955 - loss value [[280.3846439]] accuracy 0.7272727272727273\n","Iteration 45956 - loss value [[278.73906768]] accuracy 0.7272727272727273\n","Iteration 45957 - loss value [[280.32397319]] accuracy 0.7272727272727273\n","Iteration 45958 - loss value [[278.16093218]] accuracy 0.7272727272727273\n","Iteration 45959 - loss value [[279.40372576]] accuracy 0.7272727272727273\n","Iteration 45960 - loss value [[276.72458467]] accuracy 0.7272727272727273\n","Iteration 45961 - loss value [[276.33154306]] accuracy 0.7272727272727273\n","Iteration 45962 - loss value [[273.96133563]] accuracy 0.7272727272727273\n","Iteration 45963 - loss value [[275.00950668]] accuracy 0.7272727272727273\n","Iteration 45964 - loss value [[272.1840492]] accuracy 0.7272727272727273\n","Iteration 45965 - loss value [[272.88052744]] accuracy 0.7272727272727273\n","Iteration 45966 - loss value [[269.86533887]] accuracy 0.7272727272727273\n","Iteration 45967 - loss value [[269.83892819]] accuracy 0.7272727272727273\n","Iteration 45968 - loss value [[268.63703614]] accuracy 0.7272727272727273\n","Iteration 45969 - loss value [[269.51492899]] accuracy 0.7272727272727273\n","Iteration 45970 - loss value [[268.43120365]] accuracy 0.7272727272727273\n","Iteration 45971 - loss value [[269.20874374]] accuracy 0.7272727272727273\n","Iteration 45972 - loss value [[268.33780239]] accuracy 0.7272727272727273\n","Iteration 45973 - loss value [[268.76599412]] accuracy 0.7272727272727273\n","Iteration 45974 - loss value [[268.2846006]] accuracy 0.7272727272727273\n","Iteration 45975 - loss value [[270.84442958]] accuracy 0.7272727272727273\n","Iteration 45976 - loss value [[269.04004384]] accuracy 0.7272727272727273\n","Iteration 45977 - loss value [[270.2304611]] accuracy 0.7272727272727273\n","Iteration 45978 - loss value [[268.93586145]] accuracy 0.7272727272727273\n","Iteration 45979 - loss value [[271.08707996]] accuracy 0.7272727272727273\n","Iteration 45980 - loss value [[269.39585238]] accuracy 0.7272727272727273\n","Iteration 45981 - loss value [[270.72692722]] accuracy 0.7272727272727273\n","Iteration 45982 - loss value [[269.86877752]] accuracy 0.7272727272727273\n","Iteration 45983 - loss value [[273.02807677]] accuracy 0.7272727272727273\n","Iteration 45984 - loss value [[270.3381119]] accuracy 0.7272727272727273\n","Iteration 45985 - loss value [[271.45413818]] accuracy 0.7272727272727273\n","Iteration 45986 - loss value [[269.92657093]] accuracy 0.7272727272727273\n","Iteration 45987 - loss value [[271.95511847]] accuracy 0.7272727272727273\n","Iteration 45988 - loss value [[270.19621026]] accuracy 0.7272727272727273\n","Iteration 45989 - loss value [[272.60374106]] accuracy 0.7272727272727273\n","Iteration 45990 - loss value [[269.7695946]] accuracy 0.7272727272727273\n","Iteration 45991 - loss value [[270.01208862]] accuracy 0.7272727272727273\n","Iteration 45992 - loss value [[268.95977949]] accuracy 0.7272727272727273\n","Iteration 45993 - loss value [[270.92082891]] accuracy 0.7272727272727273\n","Iteration 45994 - loss value [[269.09966533]] accuracy 0.7272727272727273\n","Iteration 45995 - loss value [[269.55198451]] accuracy 0.7272727272727273\n","Iteration 45996 - loss value [[268.33526402]] accuracy 0.7272727272727273\n","Iteration 45997 - loss value [[268.99958509]] accuracy 0.7272727272727273\n","Iteration 45998 - loss value [[269.08165911]] accuracy 0.7272727272727273\n","Iteration 45999 - loss value [[272.07614206]] accuracy 0.7272727272727273\n","Iteration 46000 - loss value [[270.10731007]] accuracy 0.7272727272727273\n","Iteration 46001 - loss value [[271.92677111]] accuracy 0.7272727272727273\n","Iteration 46002 - loss value [[270.17569409]] accuracy 0.7272727272727273\n","Iteration 46003 - loss value [[272.29163631]] accuracy 0.7272727272727273\n","Iteration 46004 - loss value [[269.88373782]] accuracy 0.7272727272727273\n","Iteration 46005 - loss value [[271.31792337]] accuracy 0.7272727272727273\n","Iteration 46006 - loss value [[269.54202764]] accuracy 0.7272727272727273\n","Iteration 46007 - loss value [[270.88437484]] accuracy 0.7272727272727273\n","Iteration 46008 - loss value [[269.87096797]] accuracy 0.7272727272727273\n","Iteration 46009 - loss value [[272.92565815]] accuracy 0.7272727272727273\n","Iteration 46010 - loss value [[270.4980862]] accuracy 0.7272727272727273\n","Iteration 46011 - loss value [[271.87461809]] accuracy 0.7272727272727273\n","Iteration 46012 - loss value [[270.37360772]] accuracy 0.7272727272727273\n","Iteration 46013 - loss value [[272.50115673]] accuracy 0.7272727272727273\n","Iteration 46014 - loss value [[269.94031329]] accuracy 0.7272727272727273\n","Iteration 46015 - loss value [[271.34414049]] accuracy 0.7272727272727273\n","Iteration 46016 - loss value [[269.78747061]] accuracy 0.7272727272727273\n","Iteration 46017 - loss value [[270.61314487]] accuracy 0.7272727272727273\n","Iteration 46018 - loss value [[269.63468922]] accuracy 0.7272727272727273\n","Iteration 46019 - loss value [[271.95925626]] accuracy 0.7272727272727273\n","Iteration 46020 - loss value [[269.90373558]] accuracy 0.7272727272727273\n","Iteration 46021 - loss value [[271.38364597]] accuracy 0.7272727272727273\n","Iteration 46022 - loss value [[270.23388295]] accuracy 0.7272727272727273\n","Iteration 46023 - loss value [[273.64569006]] accuracy 0.7272727272727273\n","Iteration 46024 - loss value [[271.55613668]] accuracy 0.7272727272727273\n","Iteration 46025 - loss value [[273.46550029]] accuracy 0.7272727272727273\n","Iteration 46026 - loss value [[270.94459217]] accuracy 0.7272727272727273\n","Iteration 46027 - loss value [[272.3024695]] accuracy 0.7272727272727273\n","Iteration 46028 - loss value [[269.70180922]] accuracy 0.7272727272727273\n","Iteration 46029 - loss value [[270.52500457]] accuracy 0.7272727272727273\n","Iteration 46030 - loss value [[269.09025836]] accuracy 0.7272727272727273\n","Iteration 46031 - loss value [[269.98310356]] accuracy 0.7272727272727273\n","Iteration 46032 - loss value [[269.12484175]] accuracy 0.7272727272727273\n","Iteration 46033 - loss value [[272.24198567]] accuracy 0.7272727272727273\n","Iteration 46034 - loss value [[270.01664613]] accuracy 0.7272727272727273\n","Iteration 46035 - loss value [[271.56636174]] accuracy 0.7272727272727273\n","Iteration 46036 - loss value [[269.93736261]] accuracy 0.7272727272727273\n","Iteration 46037 - loss value [[272.12081005]] accuracy 0.7272727272727273\n","Iteration 46038 - loss value [[269.57293077]] accuracy 0.7272727272727273\n","Iteration 46039 - loss value [[269.96308488]] accuracy 0.7272727272727273\n","Iteration 46040 - loss value [[268.5599831]] accuracy 0.7272727272727273\n","Iteration 46041 - loss value [[269.14615395]] accuracy 0.7272727272727273\n","Iteration 46042 - loss value [[268.55874221]] accuracy 0.7272727272727273\n","Iteration 46043 - loss value [[270.39249088]] accuracy 0.7272727272727273\n","Iteration 46044 - loss value [[268.93858841]] accuracy 0.7272727272727273\n","Iteration 46045 - loss value [[270.14989072]] accuracy 0.7272727272727273\n","Iteration 46046 - loss value [[269.58924666]] accuracy 0.7272727272727273\n","Iteration 46047 - loss value [[273.00378952]] accuracy 0.7272727272727273\n","Iteration 46048 - loss value [[270.73097925]] accuracy 0.7272727272727273\n","Iteration 46049 - loss value [[272.89717728]] accuracy 0.7272727272727273\n","Iteration 46050 - loss value [[270.66985313]] accuracy 0.7272727272727273\n","Iteration 46051 - loss value [[272.22595054]] accuracy 0.7272727272727273\n","Iteration 46052 - loss value [[270.08487009]] accuracy 0.7272727272727273\n","Iteration 46053 - loss value [[272.1321846]] accuracy 0.7272727272727273\n","Iteration 46054 - loss value [[269.9009835]] accuracy 0.7272727272727273\n","Iteration 46055 - loss value [[271.12810904]] accuracy 0.7272727272727273\n","Iteration 46056 - loss value [[269.75649419]] accuracy 0.7272727272727273\n","Iteration 46057 - loss value [[272.2759809]] accuracy 0.7272727272727273\n","Iteration 46058 - loss value [[270.26666326]] accuracy 0.7272727272727273\n","Iteration 46059 - loss value [[272.91578678]] accuracy 0.7272727272727273\n","Iteration 46060 - loss value [[270.9925831]] accuracy 0.7272727272727273\n","Iteration 46061 - loss value [[274.80142663]] accuracy 0.7272727272727273\n","Iteration 46062 - loss value [[273.2263851]] accuracy 0.7272727272727273\n","Iteration 46063 - loss value [[275.66528483]] accuracy 0.7272727272727273\n","Iteration 46064 - loss value [[275.28087449]] accuracy 0.7272727272727273\n","Iteration 46065 - loss value [[278.34183071]] accuracy 0.7272727272727273\n","Iteration 46066 - loss value [[278.64998645]] accuracy 0.7272727272727273\n","Iteration 46067 - loss value [[282.18375792]] accuracy 0.7272727272727273\n","Iteration 46068 - loss value [[281.61798001]] accuracy 0.7272727272727273\n","Iteration 46069 - loss value [[282.81065538]] accuracy 0.7272727272727273\n","Iteration 46070 - loss value [[281.84266502]] accuracy 0.7272727272727273\n","Iteration 46071 - loss value [[282.83126826]] accuracy 0.7272727272727273\n","Iteration 46072 - loss value [[281.53804577]] accuracy 0.7272727272727273\n","Iteration 46073 - loss value [[282.01556884]] accuracy 0.7272727272727273\n","Iteration 46074 - loss value [[280.49317924]] accuracy 0.7272727272727273\n","Iteration 46075 - loss value [[281.20859478]] accuracy 0.7272727272727273\n","Iteration 46076 - loss value [[279.40581485]] accuracy 0.7272727272727273\n","Iteration 46077 - loss value [[281.45099666]] accuracy 0.7272727272727273\n","Iteration 46078 - loss value [[279.48132888]] accuracy 0.7272727272727273\n","Iteration 46079 - loss value [[281.2577182]] accuracy 0.7272727272727273\n","Iteration 46080 - loss value [[279.13583167]] accuracy 0.7272727272727273\n","Iteration 46081 - loss value [[280.18580549]] accuracy 0.7272727272727273\n","Iteration 46082 - loss value [[278.3632811]] accuracy 0.7272727272727273\n","Iteration 46083 - loss value [[280.7486613]] accuracy 0.7272727272727273\n","Iteration 46084 - loss value [[278.4512948]] accuracy 0.7272727272727273\n","Iteration 46085 - loss value [[280.48190504]] accuracy 0.7272727272727273\n","Iteration 46086 - loss value [[277.91830836]] accuracy 0.7272727272727273\n","Iteration 46087 - loss value [[279.40798109]] accuracy 0.7272727272727273\n","Iteration 46088 - loss value [[277.02715349]] accuracy 0.7272727272727273\n","Iteration 46089 - loss value [[278.10771241]] accuracy 0.7272727272727273\n","Iteration 46090 - loss value [[276.00155675]] accuracy 0.7272727272727273\n","Iteration 46091 - loss value [[276.94456515]] accuracy 0.7272727272727273\n","Iteration 46092 - loss value [[274.44826346]] accuracy 0.7272727272727273\n","Iteration 46093 - loss value [[273.56505524]] accuracy 0.7272727272727273\n","Iteration 46094 - loss value [[270.23167315]] accuracy 0.7272727272727273\n","Iteration 46095 - loss value [[270.28981528]] accuracy 0.7272727272727273\n","Iteration 46096 - loss value [[268.99305619]] accuracy 0.7272727272727273\n","Iteration 46097 - loss value [[270.79811453]] accuracy 0.7272727272727273\n","Iteration 46098 - loss value [[268.5103568]] accuracy 0.7272727272727273\n","Iteration 46099 - loss value [[268.15725693]] accuracy 0.7272727272727273\n","Iteration 46100 - loss value [[267.09891474]] accuracy 0.7272727272727273\n","Iteration 46101 - loss value [[267.40314927]] accuracy 0.7272727272727273\n","Iteration 46102 - loss value [[266.81418017]] accuracy 0.7272727272727273\n","Iteration 46103 - loss value [[268.62672358]] accuracy 0.7272727272727273\n","Iteration 46104 - loss value [[267.95870803]] accuracy 0.7272727272727273\n","Iteration 46105 - loss value [[268.08000435]] accuracy 0.7272727272727273\n","Iteration 46106 - loss value [[268.20110319]] accuracy 0.7272727272727273\n","Iteration 46107 - loss value [[270.78306562]] accuracy 0.7272727272727273\n","Iteration 46108 - loss value [[269.41674673]] accuracy 0.7272727272727273\n","Iteration 46109 - loss value [[270.55444074]] accuracy 0.7272727272727273\n","Iteration 46110 - loss value [[269.407023]] accuracy 0.7272727272727273\n","Iteration 46111 - loss value [[272.38654751]] accuracy 0.7272727272727273\n","Iteration 46112 - loss value [[269.97023304]] accuracy 0.7272727272727273\n","Iteration 46113 - loss value [[271.72451127]] accuracy 0.7272727272727273\n","Iteration 46114 - loss value [[269.94215121]] accuracy 0.7272727272727273\n","Iteration 46115 - loss value [[271.99401263]] accuracy 0.7272727272727273\n","Iteration 46116 - loss value [[270.49864142]] accuracy 0.7272727272727273\n","Iteration 46117 - loss value [[273.3783088]] accuracy 0.7272727272727273\n","Iteration 46118 - loss value [[270.8364691]] accuracy 0.7272727272727273\n","Iteration 46119 - loss value [[271.97433028]] accuracy 0.7272727272727273\n","Iteration 46120 - loss value [[270.32120968]] accuracy 0.7272727272727273\n","Iteration 46121 - loss value [[272.61966699]] accuracy 0.7272727272727273\n","Iteration 46122 - loss value [[270.42340815]] accuracy 0.7272727272727273\n","Iteration 46123 - loss value [[271.1685964]] accuracy 0.7272727272727273\n","Iteration 46124 - loss value [[269.78333456]] accuracy 0.7272727272727273\n","Iteration 46125 - loss value [[271.70626379]] accuracy 0.7272727272727273\n","Iteration 46126 - loss value [[269.80188751]] accuracy 0.7272727272727273\n","Iteration 46127 - loss value [[271.17449844]] accuracy 0.7272727272727273\n","Iteration 46128 - loss value [[269.51899143]] accuracy 0.7272727272727273\n","Iteration 46129 - loss value [[270.63111759]] accuracy 0.7272727272727273\n","Iteration 46130 - loss value [[269.41752369]] accuracy 0.7272727272727273\n","Iteration 46131 - loss value [[272.11097601]] accuracy 0.7272727272727273\n","Iteration 46132 - loss value [[269.58931688]] accuracy 0.7272727272727273\n","Iteration 46133 - loss value [[270.43921713]] accuracy 0.7272727272727273\n","Iteration 46134 - loss value [[269.08989327]] accuracy 0.7272727272727273\n","Iteration 46135 - loss value [[271.26351062]] accuracy 0.7272727272727273\n","Iteration 46136 - loss value [[268.45884444]] accuracy 0.7272727272727273\n","Iteration 46137 - loss value [[268.71969101]] accuracy 0.7272727272727273\n","Iteration 46138 - loss value [[268.00213914]] accuracy 0.7272727272727273\n","Iteration 46139 - loss value [[268.1678402]] accuracy 0.7272727272727273\n","Iteration 46140 - loss value [[267.74348048]] accuracy 0.7272727272727273\n","Iteration 46141 - loss value [[270.6512183]] accuracy 0.7272727272727273\n","Iteration 46142 - loss value [[269.42239825]] accuracy 0.7272727272727273\n","Iteration 46143 - loss value [[270.91666819]] accuracy 0.7272727272727273\n","Iteration 46144 - loss value [[270.12124631]] accuracy 0.7272727272727273\n","Iteration 46145 - loss value [[273.86064947]] accuracy 0.7272727272727273\n","Iteration 46146 - loss value [[271.82420895]] accuracy 0.7272727272727273\n","Iteration 46147 - loss value [[274.49076998]] accuracy 0.7272727272727273\n","Iteration 46148 - loss value [[273.67144191]] accuracy 0.7272727272727273\n","Iteration 46149 - loss value [[276.20392371]] accuracy 0.7272727272727273\n","Iteration 46150 - loss value [[274.50115958]] accuracy 0.7272727272727273\n","Iteration 46151 - loss value [[275.30684023]] accuracy 0.7272727272727273\n","Iteration 46152 - loss value [[273.19484469]] accuracy 0.7272727272727273\n","Iteration 46153 - loss value [[275.32496498]] accuracy 0.7272727272727273\n","Iteration 46154 - loss value [[273.99511578]] accuracy 0.7272727272727273\n","Iteration 46155 - loss value [[276.2180911]] accuracy 0.7272727272727273\n","Iteration 46156 - loss value [[274.6191229]] accuracy 0.7272727272727273\n","Iteration 46157 - loss value [[276.30881908]] accuracy 0.7272727272727273\n","Iteration 46158 - loss value [[274.66835017]] accuracy 0.7272727272727273\n","Iteration 46159 - loss value [[274.72245277]] accuracy 0.7272727272727273\n","Iteration 46160 - loss value [[272.53121124]] accuracy 0.7272727272727273\n","Iteration 46161 - loss value [[273.82121618]] accuracy 0.7272727272727273\n","Iteration 46162 - loss value [[271.30612414]] accuracy 0.7272727272727273\n","Iteration 46163 - loss value [[272.33759684]] accuracy 0.7272727272727273\n","Iteration 46164 - loss value [[269.33167149]] accuracy 0.7272727272727273\n","Iteration 46165 - loss value [[269.85395097]] accuracy 0.7272727272727273\n","Iteration 46166 - loss value [[268.63316415]] accuracy 0.7272727272727273\n","Iteration 46167 - loss value [[269.16702338]] accuracy 0.7272727272727273\n","Iteration 46168 - loss value [[268.54938499]] accuracy 0.7272727272727273\n","Iteration 46169 - loss value [[271.3692708]] accuracy 0.7272727272727273\n","Iteration 46170 - loss value [[268.9768024]] accuracy 0.7272727272727273\n","Iteration 46171 - loss value [[269.24750778]] accuracy 0.7272727272727273\n","Iteration 46172 - loss value [[268.76686056]] accuracy 0.7272727272727273\n","Iteration 46173 - loss value [[271.65624493]] accuracy 0.7272727272727273\n","Iteration 46174 - loss value [[269.41592672]] accuracy 0.7272727272727273\n","Iteration 46175 - loss value [[271.30202215]] accuracy 0.7272727272727273\n","Iteration 46176 - loss value [[269.49268757]] accuracy 0.7272727272727273\n","Iteration 46177 - loss value [[271.01591654]] accuracy 0.7272727272727273\n","Iteration 46178 - loss value [[269.50707915]] accuracy 0.7272727272727273\n","Iteration 46179 - loss value [[271.29080948]] accuracy 0.7272727272727273\n","Iteration 46180 - loss value [[269.67801552]] accuracy 0.7272727272727273\n","Iteration 46181 - loss value [[272.12817665]] accuracy 0.7272727272727273\n","Iteration 46182 - loss value [[269.94442269]] accuracy 0.7272727272727273\n","Iteration 46183 - loss value [[271.40455209]] accuracy 0.7272727272727273\n","Iteration 46184 - loss value [[270.27287435]] accuracy 0.7272727272727273\n","Iteration 46185 - loss value [[273.71444093]] accuracy 0.7272727272727273\n","Iteration 46186 - loss value [[271.29124617]] accuracy 0.7272727272727273\n","Iteration 46187 - loss value [[272.66262419]] accuracy 0.7272727272727273\n","Iteration 46188 - loss value [[269.98648406]] accuracy 0.7272727272727273\n","Iteration 46189 - loss value [[271.00493967]] accuracy 0.7272727272727273\n","Iteration 46190 - loss value [[269.62854993]] accuracy 0.7272727272727273\n","Iteration 46191 - loss value [[271.22601755]] accuracy 0.7272727272727273\n","Iteration 46192 - loss value [[269.87486783]] accuracy 0.7272727272727273\n","Iteration 46193 - loss value [[272.53118815]] accuracy 0.7272727272727273\n","Iteration 46194 - loss value [[270.35727392]] accuracy 0.7272727272727273\n","Iteration 46195 - loss value [[272.42979373]] accuracy 0.7272727272727273\n","Iteration 46196 - loss value [[269.98099583]] accuracy 0.7272727272727273\n","Iteration 46197 - loss value [[271.0824222]] accuracy 0.7272727272727273\n","Iteration 46198 - loss value [[269.45803883]] accuracy 0.7272727272727273\n","Iteration 46199 - loss value [[270.34572697]] accuracy 0.7272727272727273\n","Iteration 46200 - loss value [[269.19577996]] accuracy 0.7272727272727273\n","Iteration 46201 - loss value [[272.11090329]] accuracy 0.7272727272727273\n","Iteration 46202 - loss value [[269.74373665]] accuracy 0.7272727272727273\n","Iteration 46203 - loss value [[270.60687032]] accuracy 0.7272727272727273\n","Iteration 46204 - loss value [[269.665448]] accuracy 0.7272727272727273\n","Iteration 46205 - loss value [[272.77631153]] accuracy 0.7272727272727273\n","Iteration 46206 - loss value [[270.79403909]] accuracy 0.7272727272727273\n","Iteration 46207 - loss value [[272.72610134]] accuracy 0.7272727272727273\n","Iteration 46208 - loss value [[270.274549]] accuracy 0.7272727272727273\n","Iteration 46209 - loss value [[271.33866867]] accuracy 0.7272727272727273\n","Iteration 46210 - loss value [[269.92500209]] accuracy 0.7272727272727273\n","Iteration 46211 - loss value [[272.3700802]] accuracy 0.7272727272727273\n","Iteration 46212 - loss value [[270.43658491]] accuracy 0.7272727272727273\n","Iteration 46213 - loss value [[272.89972682]] accuracy 0.7272727272727273\n","Iteration 46214 - loss value [[270.752167]] accuracy 0.7272727272727273\n","Iteration 46215 - loss value [[272.77849514]] accuracy 0.7272727272727273\n","Iteration 46216 - loss value [[270.58078065]] accuracy 0.7272727272727273\n","Iteration 46217 - loss value [[272.75036492]] accuracy 0.7272727272727273\n","Iteration 46218 - loss value [[271.12231548]] accuracy 0.7272727272727273\n","Iteration 46219 - loss value [[274.78767722]] accuracy 0.7272727272727273\n","Iteration 46220 - loss value [[273.12361114]] accuracy 0.7272727272727273\n","Iteration 46221 - loss value [[275.90100232]] accuracy 0.7272727272727273\n","Iteration 46222 - loss value [[274.60197989]] accuracy 0.7272727272727273\n","Iteration 46223 - loss value [[277.9398416]] accuracy 0.7272727272727273\n","Iteration 46224 - loss value [[278.47337327]] accuracy 0.7272727272727273\n","Iteration 46225 - loss value [[282.15015307]] accuracy 0.7272727272727273\n","Iteration 46226 - loss value [[281.40935451]] accuracy 0.7272727272727273\n","Iteration 46227 - loss value [[282.520017]] accuracy 0.7272727272727273\n","Iteration 46228 - loss value [[281.45566895]] accuracy 0.7272727272727273\n","Iteration 46229 - loss value [[282.44991471]] accuracy 0.7272727272727273\n","Iteration 46230 - loss value [[281.07050373]] accuracy 0.7272727272727273\n","Iteration 46231 - loss value [[282.10058092]] accuracy 0.7272727272727273\n","Iteration 46232 - loss value [[280.45641423]] accuracy 0.7272727272727273\n","Iteration 46233 - loss value [[281.06478426]] accuracy 0.7272727272727273\n","Iteration 46234 - loss value [[279.254909]] accuracy 0.7272727272727273\n","Iteration 46235 - loss value [[281.35625317]] accuracy 0.7272727272727273\n","Iteration 46236 - loss value [[279.3432978]] accuracy 0.7272727272727273\n","Iteration 46237 - loss value [[281.08137433]] accuracy 0.7272727272727273\n","Iteration 46238 - loss value [[278.92429951]] accuracy 0.7272727272727273\n","Iteration 46239 - loss value [[279.60649679]] accuracy 0.7272727272727273\n","Iteration 46240 - loss value [[277.88215083]] accuracy 0.7272727272727273\n","Iteration 46241 - loss value [[280.17011496]] accuracy 0.7272727272727273\n","Iteration 46242 - loss value [[277.70502397]] accuracy 0.7272727272727273\n","Iteration 46243 - loss value [[278.62625293]] accuracy 0.7272727272727273\n","Iteration 46244 - loss value [[277.05609315]] accuracy 0.7272727272727273\n","Iteration 46245 - loss value [[278.6418619]] accuracy 0.7272727272727273\n","Iteration 46246 - loss value [[276.87112119]] accuracy 0.7272727272727273\n","Iteration 46247 - loss value [[278.46590186]] accuracy 0.7272727272727273\n","Iteration 46248 - loss value [[276.39936914]] accuracy 0.7272727272727273\n","Iteration 46249 - loss value [[276.60567715]] accuracy 0.7272727272727273\n","Iteration 46250 - loss value [[273.83851107]] accuracy 0.7272727272727273\n","Iteration 46251 - loss value [[273.86005144]] accuracy 0.7272727272727273\n","Iteration 46252 - loss value [[270.69196413]] accuracy 0.7272727272727273\n","Iteration 46253 - loss value [[271.35731937]] accuracy 0.7272727272727273\n","Iteration 46254 - loss value [[269.25609792]] accuracy 0.7272727272727273\n","Iteration 46255 - loss value [[269.2157897]] accuracy 0.7272727272727273\n","Iteration 46256 - loss value [[268.38607924]] accuracy 0.7272727272727273\n","Iteration 46257 - loss value [[269.92758726]] accuracy 0.7272727272727273\n","Iteration 46258 - loss value [[268.36069343]] accuracy 0.7272727272727273\n","Iteration 46259 - loss value [[268.61644264]] accuracy 0.7272727272727273\n","Iteration 46260 - loss value [[268.30130216]] accuracy 0.7272727272727273\n","Iteration 46261 - loss value [[270.09892155]] accuracy 0.7272727272727273\n","Iteration 46262 - loss value [[268.77744566]] accuracy 0.7272727272727273\n","Iteration 46263 - loss value [[269.68538093]] accuracy 0.7272727272727273\n","Iteration 46264 - loss value [[268.87216175]] accuracy 0.7272727272727273\n","Iteration 46265 - loss value [[271.69241288]] accuracy 0.7272727272727273\n","Iteration 46266 - loss value [[269.08291077]] accuracy 0.7272727272727273\n","Iteration 46267 - loss value [[270.6438652]] accuracy 0.7272727272727273\n","Iteration 46268 - loss value [[269.37458563]] accuracy 0.7272727272727273\n","Iteration 46269 - loss value [[271.75597591]] accuracy 0.7272727272727273\n","Iteration 46270 - loss value [[268.92430832]] accuracy 0.7272727272727273\n","Iteration 46271 - loss value [[269.25698695]] accuracy 0.7272727272727273\n","Iteration 46272 - loss value [[268.74596724]] accuracy 0.7272727272727273\n","Iteration 46273 - loss value [[271.33909823]] accuracy 0.7272727272727273\n","Iteration 46274 - loss value [[268.86050851]] accuracy 0.7272727272727273\n","Iteration 46275 - loss value [[269.95601488]] accuracy 0.7272727272727273\n","Iteration 46276 - loss value [[268.81342742]] accuracy 0.7272727272727273\n","Iteration 46277 - loss value [[271.44418271]] accuracy 0.7272727272727273\n","Iteration 46278 - loss value [[268.56417053]] accuracy 0.7272727272727273\n","Iteration 46279 - loss value [[269.24847074]] accuracy 0.7272727272727273\n","Iteration 46280 - loss value [[269.44638716]] accuracy 0.7272727272727273\n","Iteration 46281 - loss value [[272.71551494]] accuracy 0.7272727272727273\n","Iteration 46282 - loss value [[270.19285695]] accuracy 0.7272727272727273\n","Iteration 46283 - loss value [[271.99922399]] accuracy 0.7272727272727273\n","Iteration 46284 - loss value [[269.45850733]] accuracy 0.7272727272727273\n","Iteration 46285 - loss value [[270.84882938]] accuracy 0.7272727272727273\n","Iteration 46286 - loss value [[269.56053474]] accuracy 0.7272727272727273\n","Iteration 46287 - loss value [[271.1532514]] accuracy 0.7272727272727273\n","Iteration 46288 - loss value [[269.99523424]] accuracy 0.7272727272727273\n","Iteration 46289 - loss value [[272.4024518]] accuracy 0.7272727272727273\n","Iteration 46290 - loss value [[270.19204309]] accuracy 0.7272727272727273\n","Iteration 46291 - loss value [[272.067531]] accuracy 0.7272727272727273\n","Iteration 46292 - loss value [[270.72887847]] accuracy 0.7272727272727273\n","Iteration 46293 - loss value [[274.28849639]] accuracy 0.7272727272727273\n","Iteration 46294 - loss value [[271.99565871]] accuracy 0.7272727272727273\n","Iteration 46295 - loss value [[274.15062191]] accuracy 0.7272727272727273\n","Iteration 46296 - loss value [[272.59730707]] accuracy 0.7272727272727273\n","Iteration 46297 - loss value [[276.52991063]] accuracy 0.7272727272727273\n","Iteration 46298 - loss value [[275.23498971]] accuracy 0.7272727272727273\n","Iteration 46299 - loss value [[276.87847899]] accuracy 0.7272727272727273\n","Iteration 46300 - loss value [[275.49331655]] accuracy 0.7272727272727273\n","Iteration 46301 - loss value [[277.67993408]] accuracy 0.7272727272727273\n","Iteration 46302 - loss value [[276.7934628]] accuracy 0.7272727272727273\n","Iteration 46303 - loss value [[279.39893177]] accuracy 0.7272727272727273\n","Iteration 46304 - loss value [[278.87465313]] accuracy 0.7272727272727273\n","Iteration 46305 - loss value [[281.34944686]] accuracy 0.7272727272727273\n","Iteration 46306 - loss value [[279.75455718]] accuracy 0.7272727272727273\n","Iteration 46307 - loss value [[281.780438]] accuracy 0.7272727272727273\n","Iteration 46308 - loss value [[279.86238877]] accuracy 0.7272727272727273\n","Iteration 46309 - loss value [[280.95511124]] accuracy 0.7272727272727273\n","Iteration 46310 - loss value [[278.64262504]] accuracy 0.7272727272727273\n","Iteration 46311 - loss value [[279.65648455]] accuracy 0.7272727272727273\n","Iteration 46312 - loss value [[278.34622431]] accuracy 0.7272727272727273\n","Iteration 46313 - loss value [[279.89404109]] accuracy 0.7272727272727273\n","Iteration 46314 - loss value [[277.6423403]] accuracy 0.7272727272727273\n","Iteration 46315 - loss value [[278.93358837]] accuracy 0.7272727272727273\n","Iteration 46316 - loss value [[276.42242091]] accuracy 0.7272727272727273\n","Iteration 46317 - loss value [[276.7932884]] accuracy 0.7272727272727273\n","Iteration 46318 - loss value [[274.25026592]] accuracy 0.7272727272727273\n","Iteration 46319 - loss value [[274.19705037]] accuracy 0.7272727272727273\n","Iteration 46320 - loss value [[271.25623911]] accuracy 0.7272727272727273\n","Iteration 46321 - loss value [[271.6192489]] accuracy 0.7272727272727273\n","Iteration 46322 - loss value [[269.64275794]] accuracy 0.7272727272727273\n","Iteration 46323 - loss value [[269.9692849]] accuracy 0.7272727272727273\n","Iteration 46324 - loss value [[268.35705977]] accuracy 0.7272727272727273\n","Iteration 46325 - loss value [[268.84210612]] accuracy 0.7272727272727273\n","Iteration 46326 - loss value [[267.84849725]] accuracy 0.7272727272727273\n","Iteration 46327 - loss value [[267.9202659]] accuracy 0.7272727272727273\n","Iteration 46328 - loss value [[267.62105689]] accuracy 0.7272727272727273\n","Iteration 46329 - loss value [[269.6244415]] accuracy 0.7272727272727273\n","Iteration 46330 - loss value [[268.66696958]] accuracy 0.7272727272727273\n","Iteration 46331 - loss value [[269.71941253]] accuracy 0.7272727272727273\n","Iteration 46332 - loss value [[268.63643506]] accuracy 0.7272727272727273\n","Iteration 46333 - loss value [[271.54370614]] accuracy 0.7272727272727273\n","Iteration 46334 - loss value [[268.92223234]] accuracy 0.7272727272727273\n","Iteration 46335 - loss value [[269.73146289]] accuracy 0.7272727272727273\n","Iteration 46336 - loss value [[268.45111065]] accuracy 0.7272727272727273\n","Iteration 46337 - loss value [[269.95518215]] accuracy 0.7272727272727273\n","Iteration 46338 - loss value [[269.17772881]] accuracy 0.7272727272727273\n","Iteration 46339 - loss value [[270.12994781]] accuracy 0.7272727272727273\n","Iteration 46340 - loss value [[269.20984067]] accuracy 0.7272727272727273\n","Iteration 46341 - loss value [[272.20491717]] accuracy 0.7272727272727273\n","Iteration 46342 - loss value [[269.93727221]] accuracy 0.7272727272727273\n","Iteration 46343 - loss value [[271.68224059]] accuracy 0.7272727272727273\n","Iteration 46344 - loss value [[269.87386042]] accuracy 0.7272727272727273\n","Iteration 46345 - loss value [[271.63295437]] accuracy 0.7272727272727273\n","Iteration 46346 - loss value [[269.63196727]] accuracy 0.7272727272727273\n","Iteration 46347 - loss value [[270.49309056]] accuracy 0.7272727272727273\n","Iteration 46348 - loss value [[269.49657633]] accuracy 0.7272727272727273\n","Iteration 46349 - loss value [[271.95294967]] accuracy 0.7272727272727273\n","Iteration 46350 - loss value [[269.32073293]] accuracy 0.7272727272727273\n","Iteration 46351 - loss value [[270.00060113]] accuracy 0.7272727272727273\n","Iteration 46352 - loss value [[268.87389554]] accuracy 0.7272727272727273\n","Iteration 46353 - loss value [[269.95520993]] accuracy 0.7272727272727273\n","Iteration 46354 - loss value [[269.15975179]] accuracy 0.7272727272727273\n","Iteration 46355 - loss value [[272.16638762]] accuracy 0.7272727272727273\n","Iteration 46356 - loss value [[269.76064945]] accuracy 0.7272727272727273\n","Iteration 46357 - loss value [[271.11939132]] accuracy 0.7272727272727273\n","Iteration 46358 - loss value [[269.7041333]] accuracy 0.7272727272727273\n","Iteration 46359 - loss value [[271.92680532]] accuracy 0.7272727272727273\n","Iteration 46360 - loss value [[269.39503157]] accuracy 0.7272727272727273\n","Iteration 46361 - loss value [[269.86625418]] accuracy 0.7272727272727273\n","Iteration 46362 - loss value [[268.62196922]] accuracy 0.7272727272727273\n","Iteration 46363 - loss value [[269.54432485]] accuracy 0.7272727272727273\n","Iteration 46364 - loss value [[269.06504817]] accuracy 0.7272727272727273\n","Iteration 46365 - loss value [[271.61059378]] accuracy 0.7272727272727273\n","Iteration 46366 - loss value [[269.48434254]] accuracy 0.7272727272727273\n","Iteration 46367 - loss value [[270.72939649]] accuracy 0.7272727272727273\n","Iteration 46368 - loss value [[269.874978]] accuracy 0.7272727272727273\n","Iteration 46369 - loss value [[273.04668527]] accuracy 0.7272727272727273\n","Iteration 46370 - loss value [[271.29394063]] accuracy 0.7272727272727273\n","Iteration 46371 - loss value [[274.55445148]] accuracy 0.7272727272727273\n","Iteration 46372 - loss value [[272.50479699]] accuracy 0.7272727272727273\n","Iteration 46373 - loss value [[275.01967413]] accuracy 0.7272727272727273\n","Iteration 46374 - loss value [[273.37390299]] accuracy 0.7272727272727273\n","Iteration 46375 - loss value [[275.90080231]] accuracy 0.7272727272727273\n","Iteration 46376 - loss value [[274.03326685]] accuracy 0.7272727272727273\n","Iteration 46377 - loss value [[276.20109675]] accuracy 0.7272727272727273\n","Iteration 46378 - loss value [[274.87703603]] accuracy 0.7272727272727273\n","Iteration 46379 - loss value [[277.65526035]] accuracy 0.7272727272727273\n","Iteration 46380 - loss value [[276.90442543]] accuracy 0.7272727272727273\n","Iteration 46381 - loss value [[280.00391524]] accuracy 0.7272727272727273\n","Iteration 46382 - loss value [[279.88993944]] accuracy 0.7272727272727273\n","Iteration 46383 - loss value [[281.19317294]] accuracy 0.7272727272727273\n","Iteration 46384 - loss value [[279.84956234]] accuracy 0.7272727272727273\n","Iteration 46385 - loss value [[280.92735735]] accuracy 0.7272727272727273\n","Iteration 46386 - loss value [[280.0977402]] accuracy 0.7272727272727273\n","Iteration 46387 - loss value [[280.90080872]] accuracy 0.7272727272727273\n","Iteration 46388 - loss value [[279.26729338]] accuracy 0.7272727272727273\n","Iteration 46389 - loss value [[280.33718484]] accuracy 0.7272727272727273\n","Iteration 46390 - loss value [[278.98335248]] accuracy 0.7272727272727273\n","Iteration 46391 - loss value [[279.93225504]] accuracy 0.7272727272727273\n","Iteration 46392 - loss value [[278.26253947]] accuracy 0.7272727272727273\n","Iteration 46393 - loss value [[279.77024606]] accuracy 0.7272727272727273\n","Iteration 46394 - loss value [[278.03888682]] accuracy 0.7272727272727273\n","Iteration 46395 - loss value [[279.47252619]] accuracy 0.7272727272727273\n","Iteration 46396 - loss value [[277.38344183]] accuracy 0.7272727272727273\n","Iteration 46397 - loss value [[278.5689519]] accuracy 0.7272727272727273\n","Iteration 46398 - loss value [[276.27029814]] accuracy 0.7272727272727273\n","Iteration 46399 - loss value [[276.62101497]] accuracy 0.7272727272727273\n","Iteration 46400 - loss value [[273.63212792]] accuracy 0.7272727272727273\n","Iteration 46401 - loss value [[273.51721122]] accuracy 0.7272727272727273\n","Iteration 46402 - loss value [[270.05479411]] accuracy 0.7272727272727273\n","Iteration 46403 - loss value [[270.06120965]] accuracy 0.7272727272727273\n","Iteration 46404 - loss value [[268.39336002]] accuracy 0.7272727272727273\n","Iteration 46405 - loss value [[268.06242804]] accuracy 0.7272727272727273\n","Iteration 46406 - loss value [[267.38686561]] accuracy 0.7272727272727273\n","Iteration 46407 - loss value [[268.90929212]] accuracy 0.7272727272727273\n","Iteration 46408 - loss value [[268.02913691]] accuracy 0.7272727272727273\n","Iteration 46409 - loss value [[268.07993726]] accuracy 0.7272727272727273\n","Iteration 46410 - loss value [[267.49777318]] accuracy 0.7272727272727273\n","Iteration 46411 - loss value [[269.89282507]] accuracy 0.7272727272727273\n","Iteration 46412 - loss value [[268.89774909]] accuracy 0.7272727272727273\n","Iteration 46413 - loss value [[270.42307242]] accuracy 0.7272727272727273\n","Iteration 46414 - loss value [[268.93454731]] accuracy 0.7272727272727273\n","Iteration 46415 - loss value [[271.21057484]] accuracy 0.7272727272727273\n","Iteration 46416 - loss value [[269.72984449]] accuracy 0.7272727272727273\n","Iteration 46417 - loss value [[271.09961622]] accuracy 0.7272727272727273\n","Iteration 46418 - loss value [[269.73862232]] accuracy 0.7272727272727273\n","Iteration 46419 - loss value [[271.22988235]] accuracy 0.7272727272727273\n","Iteration 46420 - loss value [[269.6605341]] accuracy 0.7272727272727273\n","Iteration 46421 - loss value [[271.75428845]] accuracy 0.7272727272727273\n","Iteration 46422 - loss value [[269.10839804]] accuracy 0.7272727272727273\n","Iteration 46423 - loss value [[269.73847268]] accuracy 0.7272727272727273\n","Iteration 46424 - loss value [[268.55516344]] accuracy 0.7272727272727273\n","Iteration 46425 - loss value [[270.79700555]] accuracy 0.7272727272727273\n","Iteration 46426 - loss value [[267.90457639]] accuracy 0.7272727272727273\n","Iteration 46427 - loss value [[268.11613273]] accuracy 0.7272727272727273\n","Iteration 46428 - loss value [[266.98910946]] accuracy 0.7272727272727273\n","Iteration 46429 - loss value [[267.33572034]] accuracy 0.7272727272727273\n","Iteration 46430 - loss value [[266.98505234]] accuracy 0.7272727272727273\n","Iteration 46431 - loss value [[269.43190792]] accuracy 0.7272727272727273\n","Iteration 46432 - loss value [[268.47726983]] accuracy 0.7272727272727273\n","Iteration 46433 - loss value [[270.4688644]] accuracy 0.7272727272727273\n","Iteration 46434 - loss value [[269.32335479]] accuracy 0.7272727272727273\n","Iteration 46435 - loss value [[271.86302286]] accuracy 0.7272727272727273\n","Iteration 46436 - loss value [[269.29253153]] accuracy 0.7272727272727273\n","Iteration 46437 - loss value [[270.73620479]] accuracy 0.7272727272727273\n","Iteration 46438 - loss value [[269.7096213]] accuracy 0.7272727272727273\n","Iteration 46439 - loss value [[272.95775806]] accuracy 0.7272727272727273\n","Iteration 46440 - loss value [[271.02264866]] accuracy 0.7272727272727273\n","Iteration 46441 - loss value [[273.72403081]] accuracy 0.7272727272727273\n","Iteration 46442 - loss value [[271.22607531]] accuracy 0.7272727272727273\n","Iteration 46443 - loss value [[273.52439907]] accuracy 0.7272727272727273\n","Iteration 46444 - loss value [[271.38812554]] accuracy 0.7272727272727273\n","Iteration 46445 - loss value [[273.95353879]] accuracy 0.7272727272727273\n","Iteration 46446 - loss value [[272.25685081]] accuracy 0.7272727272727273\n","Iteration 46447 - loss value [[275.24069795]] accuracy 0.7272727272727273\n","Iteration 46448 - loss value [[273.40207081]] accuracy 0.7272727272727273\n","Iteration 46449 - loss value [[275.20789815]] accuracy 0.7272727272727273\n","Iteration 46450 - loss value [[273.88842752]] accuracy 0.7272727272727273\n","Iteration 46451 - loss value [[276.00575233]] accuracy 0.7272727272727273\n","Iteration 46452 - loss value [[274.05942014]] accuracy 0.7272727272727273\n","Iteration 46453 - loss value [[275.2238287]] accuracy 0.7272727272727273\n","Iteration 46454 - loss value [[273.48831623]] accuracy 0.7272727272727273\n","Iteration 46455 - loss value [[275.47053343]] accuracy 0.7272727272727273\n","Iteration 46456 - loss value [[273.05243587]] accuracy 0.7272727272727273\n","Iteration 46457 - loss value [[274.2644857]] accuracy 0.7272727272727273\n","Iteration 46458 - loss value [[271.90455648]] accuracy 0.7272727272727273\n","Iteration 46459 - loss value [[273.09810181]] accuracy 0.7272727272727273\n","Iteration 46460 - loss value [[270.4004708]] accuracy 0.7272727272727273\n","Iteration 46461 - loss value [[271.21367949]] accuracy 0.7272727272727273\n","Iteration 46462 - loss value [[269.56675673]] accuracy 0.7272727272727273\n","Iteration 46463 - loss value [[270.72102935]] accuracy 0.7272727272727273\n","Iteration 46464 - loss value [[268.964026]] accuracy 0.7272727272727273\n","Iteration 46465 - loss value [[269.34639806]] accuracy 0.7272727272727273\n","Iteration 46466 - loss value [[268.17888915]] accuracy 0.7272727272727273\n","Iteration 46467 - loss value [[270.11404268]] accuracy 0.7272727272727273\n","Iteration 46468 - loss value [[268.7838361]] accuracy 0.7272727272727273\n","Iteration 46469 - loss value [[269.60471781]] accuracy 0.7272727272727273\n","Iteration 46470 - loss value [[268.46244727]] accuracy 0.7272727272727273\n","Iteration 46471 - loss value [[271.36862838]] accuracy 0.7272727272727273\n","Iteration 46472 - loss value [[268.87061715]] accuracy 0.7272727272727273\n","Iteration 46473 - loss value [[269.33723568]] accuracy 0.7272727272727273\n","Iteration 46474 - loss value [[268.03432122]] accuracy 0.7272727272727273\n","Iteration 46475 - loss value [[269.77925986]] accuracy 0.7272727272727273\n","Iteration 46476 - loss value [[268.83786148]] accuracy 0.7272727272727273\n","Iteration 46477 - loss value [[269.76850481]] accuracy 0.7272727272727273\n","Iteration 46478 - loss value [[268.94008196]] accuracy 0.7272727272727273\n","Iteration 46479 - loss value [[271.91074871]] accuracy 0.7272727272727273\n","Iteration 46480 - loss value [[269.79330771]] accuracy 0.7272727272727273\n","Iteration 46481 - loss value [[271.83231855]] accuracy 0.7272727272727273\n","Iteration 46482 - loss value [[269.21221786]] accuracy 0.7272727272727273\n","Iteration 46483 - loss value [[269.69510296]] accuracy 0.7272727272727273\n","Iteration 46484 - loss value [[268.71573548]] accuracy 0.7272727272727273\n","Iteration 46485 - loss value [[270.73334441]] accuracy 0.7272727272727273\n","Iteration 46486 - loss value [[269.45683511]] accuracy 0.7272727272727273\n","Iteration 46487 - loss value [[272.21062512]] accuracy 0.7272727272727273\n","Iteration 46488 - loss value [[269.67491992]] accuracy 0.7272727272727273\n","Iteration 46489 - loss value [[271.11617477]] accuracy 0.7272727272727273\n","Iteration 46490 - loss value [[269.78073953]] accuracy 0.7272727272727273\n","Iteration 46491 - loss value [[272.27249199]] accuracy 0.7272727272727273\n","Iteration 46492 - loss value [[270.11485713]] accuracy 0.7272727272727273\n","Iteration 46493 - loss value [[272.53449822]] accuracy 0.7272727272727273\n","Iteration 46494 - loss value [[269.86643305]] accuracy 0.7272727272727273\n","Iteration 46495 - loss value [[271.53688775]] accuracy 0.7272727272727273\n","Iteration 46496 - loss value [[269.10139718]] accuracy 0.7272727272727273\n","Iteration 46497 - loss value [[269.82979631]] accuracy 0.7272727272727273\n","Iteration 46498 - loss value [[268.75703528]] accuracy 0.7272727272727273\n","Iteration 46499 - loss value [[270.62936594]] accuracy 0.7272727272727273\n","Iteration 46500 - loss value [[268.8767839]] accuracy 0.7272727272727273\n","Iteration 46501 - loss value [[269.77637818]] accuracy 0.7272727272727273\n","Iteration 46502 - loss value [[269.55923891]] accuracy 0.7272727272727273\n","Iteration 46503 - loss value [[272.59153876]] accuracy 0.7272727272727273\n","Iteration 46504 - loss value [[270.77947289]] accuracy 0.7272727272727273\n","Iteration 46505 - loss value [[273.66165418]] accuracy 0.7272727272727273\n","Iteration 46506 - loss value [[272.17413062]] accuracy 0.7272727272727273\n","Iteration 46507 - loss value [[276.73163251]] accuracy 0.7272727272727273\n","Iteration 46508 - loss value [[276.05446974]] accuracy 0.7272727272727273\n","Iteration 46509 - loss value [[279.11344401]] accuracy 0.7272727272727273\n","Iteration 46510 - loss value [[278.31625764]] accuracy 0.7272727272727273\n","Iteration 46511 - loss value [[281.36447909]] accuracy 0.7272727272727273\n","Iteration 46512 - loss value [[280.37787746]] accuracy 0.7272727272727273\n","Iteration 46513 - loss value [[281.79685698]] accuracy 0.7272727272727273\n","Iteration 46514 - loss value [[280.60205923]] accuracy 0.7272727272727273\n","Iteration 46515 - loss value [[281.99013787]] accuracy 0.7272727272727273\n","Iteration 46516 - loss value [[280.52870441]] accuracy 0.7272727272727273\n","Iteration 46517 - loss value [[281.66089294]] accuracy 0.7272727272727273\n","Iteration 46518 - loss value [[279.97493494]] accuracy 0.7272727272727273\n","Iteration 46519 - loss value [[280.7298153]] accuracy 0.7272727272727273\n","Iteration 46520 - loss value [[278.4426884]] accuracy 0.7272727272727273\n","Iteration 46521 - loss value [[278.65890725]] accuracy 0.7272727272727273\n","Iteration 46522 - loss value [[277.93235145]] accuracy 0.7272727272727273\n","Iteration 46523 - loss value [[280.11148225]] accuracy 0.7272727272727273\n","Iteration 46524 - loss value [[279.01483779]] accuracy 0.7272727272727273\n","Iteration 46525 - loss value [[279.59036566]] accuracy 0.7272727272727273\n","Iteration 46526 - loss value [[276.8856734]] accuracy 0.7272727272727273\n","Iteration 46527 - loss value [[276.98992062]] accuracy 0.7272727272727273\n","Iteration 46528 - loss value [[274.45746587]] accuracy 0.7272727272727273\n","Iteration 46529 - loss value [[275.97788845]] accuracy 0.7272727272727273\n","Iteration 46530 - loss value [[273.29934996]] accuracy 0.7272727272727273\n","Iteration 46531 - loss value [[273.76038226]] accuracy 0.7272727272727273\n","Iteration 46532 - loss value [[270.62829814]] accuracy 0.7272727272727273\n","Iteration 46533 - loss value [[271.3653899]] accuracy 0.7272727272727273\n","Iteration 46534 - loss value [[269.42191688]] accuracy 0.7272727272727273\n","Iteration 46535 - loss value [[270.46508999]] accuracy 0.7272727272727273\n","Iteration 46536 - loss value [[268.44811429]] accuracy 0.7272727272727273\n","Iteration 46537 - loss value [[268.50469273]] accuracy 0.7272727272727273\n","Iteration 46538 - loss value [[268.16283667]] accuracy 0.7272727272727273\n","Iteration 46539 - loss value [[270.365231]] accuracy 0.7272727272727273\n","Iteration 46540 - loss value [[268.46329536]] accuracy 0.7272727272727273\n","Iteration 46541 - loss value [[268.56397777]] accuracy 0.7272727272727273\n","Iteration 46542 - loss value [[268.18592006]] accuracy 0.7272727272727273\n","Iteration 46543 - loss value [[270.81228553]] accuracy 0.7272727272727273\n","Iteration 46544 - loss value [[267.97680341]] accuracy 0.7272727272727273\n","Iteration 46545 - loss value [[268.57510517]] accuracy 0.7272727272727273\n","Iteration 46546 - loss value [[267.94633164]] accuracy 0.7272727272727273\n","Iteration 46547 - loss value [[270.08657817]] accuracy 0.7272727272727273\n","Iteration 46548 - loss value [[268.89644321]] accuracy 0.7272727272727273\n","Iteration 46549 - loss value [[270.77001995]] accuracy 0.7272727272727273\n","Iteration 46550 - loss value [[269.79321453]] accuracy 0.7272727272727273\n","Iteration 46551 - loss value [[273.16115963]] accuracy 0.7272727272727273\n","Iteration 46552 - loss value [[270.46735194]] accuracy 0.7272727272727273\n","Iteration 46553 - loss value [[271.88444238]] accuracy 0.7272727272727273\n","Iteration 46554 - loss value [[269.28686671]] accuracy 0.7272727272727273\n","Iteration 46555 - loss value [[270.09891073]] accuracy 0.7272727272727273\n","Iteration 46556 - loss value [[268.91641288]] accuracy 0.7272727272727273\n","Iteration 46557 - loss value [[271.05985024]] accuracy 0.7272727272727273\n","Iteration 46558 - loss value [[269.96660822]] accuracy 0.7272727272727273\n","Iteration 46559 - loss value [[272.92408435]] accuracy 0.7272727272727273\n","Iteration 46560 - loss value [[270.70997105]] accuracy 0.7272727272727273\n","Iteration 46561 - loss value [[272.45371662]] accuracy 0.7272727272727273\n","Iteration 46562 - loss value [[269.88519306]] accuracy 0.7272727272727273\n","Iteration 46563 - loss value [[271.71451907]] accuracy 0.7272727272727273\n","Iteration 46564 - loss value [[269.02163298]] accuracy 0.7272727272727273\n","Iteration 46565 - loss value [[269.48986635]] accuracy 0.7272727272727273\n","Iteration 46566 - loss value [[268.19637969]] accuracy 0.7272727272727273\n","Iteration 46567 - loss value [[268.60715792]] accuracy 0.7272727272727273\n","Iteration 46568 - loss value [[268.17189796]] accuracy 0.7272727272727273\n","Iteration 46569 - loss value [[270.8791045]] accuracy 0.7272727272727273\n","Iteration 46570 - loss value [[268.49328453]] accuracy 0.7272727272727273\n","Iteration 46571 - loss value [[269.02808952]] accuracy 0.7272727272727273\n","Iteration 46572 - loss value [[268.72570711]] accuracy 0.7272727272727273\n","Iteration 46573 - loss value [[271.72063332]] accuracy 0.7272727272727273\n","Iteration 46574 - loss value [[269.41201034]] accuracy 0.7272727272727273\n","Iteration 46575 - loss value [[271.37472762]] accuracy 0.7272727272727273\n","Iteration 46576 - loss value [[269.2139479]] accuracy 0.7272727272727273\n","Iteration 46577 - loss value [[270.67891185]] accuracy 0.7272727272727273\n","Iteration 46578 - loss value [[270.08661545]] accuracy 0.7272727272727273\n","Iteration 46579 - loss value [[273.77617992]] accuracy 0.7272727272727273\n","Iteration 46580 - loss value [[271.22776108]] accuracy 0.7272727272727273\n","Iteration 46581 - loss value [[273.2019559]] accuracy 0.7272727272727273\n","Iteration 46582 - loss value [[271.60684762]] accuracy 0.7272727272727273\n","Iteration 46583 - loss value [[275.38271138]] accuracy 0.7272727272727273\n","Iteration 46584 - loss value [[273.85172524]] accuracy 0.7272727272727273\n","Iteration 46585 - loss value [[275.97152204]] accuracy 0.7272727272727273\n","Iteration 46586 - loss value [[274.22582276]] accuracy 0.7272727272727273\n","Iteration 46587 - loss value [[276.86881925]] accuracy 0.7272727272727273\n","Iteration 46588 - loss value [[275.90017568]] accuracy 0.7272727272727273\n","Iteration 46589 - loss value [[278.81130561]] accuracy 0.7272727272727273\n","Iteration 46590 - loss value [[277.46421182]] accuracy 0.7272727272727273\n","Iteration 46591 - loss value [[279.80556088]] accuracy 0.7272727272727273\n","Iteration 46592 - loss value [[279.20019699]] accuracy 0.7272727272727273\n","Iteration 46593 - loss value [[281.39794267]] accuracy 0.7272727272727273\n","Iteration 46594 - loss value [[279.88293229]] accuracy 0.7272727272727273\n","Iteration 46595 - loss value [[281.39761222]] accuracy 0.7272727272727273\n","Iteration 46596 - loss value [[279.62060554]] accuracy 0.7272727272727273\n","Iteration 46597 - loss value [[281.28513943]] accuracy 0.7272727272727273\n","Iteration 46598 - loss value [[279.05118766]] accuracy 0.7272727272727273\n","Iteration 46599 - loss value [[279.41753345]] accuracy 0.7272727272727273\n","Iteration 46600 - loss value [[277.75429315]] accuracy 0.7272727272727273\n","Iteration 46601 - loss value [[280.05806075]] accuracy 0.7272727272727273\n","Iteration 46602 - loss value [[278.10719502]] accuracy 0.7272727272727273\n","Iteration 46603 - loss value [[280.20020491]] accuracy 0.7272727272727273\n","Iteration 46604 - loss value [[277.61940283]] accuracy 0.7272727272727273\n","Iteration 46605 - loss value [[278.4158962]] accuracy 0.7272727272727273\n","Iteration 46606 - loss value [[276.94370506]] accuracy 0.7272727272727273\n","Iteration 46607 - loss value [[278.80914292]] accuracy 0.7272727272727273\n","Iteration 46608 - loss value [[276.32940829]] accuracy 0.7272727272727273\n","Iteration 46609 - loss value [[275.93224178]] accuracy 0.7272727272727273\n","Iteration 46610 - loss value [[273.30407144]] accuracy 0.7272727272727273\n","Iteration 46611 - loss value [[273.90106772]] accuracy 0.7272727272727273\n","Iteration 46612 - loss value [[270.63459368]] accuracy 0.7272727272727273\n","Iteration 46613 - loss value [[270.49206511]] accuracy 0.7272727272727273\n","Iteration 46614 - loss value [[268.62608929]] accuracy 0.7272727272727273\n","Iteration 46615 - loss value [[268.41515055]] accuracy 0.7272727272727273\n","Iteration 46616 - loss value [[268.11379688]] accuracy 0.7272727272727273\n","Iteration 46617 - loss value [[269.64572885]] accuracy 0.7272727272727273\n","Iteration 46618 - loss value [[268.55629272]] accuracy 0.7272727272727273\n","Iteration 46619 - loss value [[268.90894907]] accuracy 0.7272727272727273\n","Iteration 46620 - loss value [[268.44605252]] accuracy 0.7272727272727273\n","Iteration 46621 - loss value [[271.2074571]] accuracy 0.7272727272727273\n","Iteration 46622 - loss value [[268.14060332]] accuracy 0.7272727272727273\n","Iteration 46623 - loss value [[268.61876671]] accuracy 0.7272727272727273\n","Iteration 46624 - loss value [[267.65698299]] accuracy 0.7272727272727273\n","Iteration 46625 - loss value [[268.09772468]] accuracy 0.7272727272727273\n","Iteration 46626 - loss value [[267.56858232]] accuracy 0.7272727272727273\n","Iteration 46627 - loss value [[269.94465205]] accuracy 0.7272727272727273\n","Iteration 46628 - loss value [[269.25270556]] accuracy 0.7272727272727273\n","Iteration 46629 - loss value [[271.29364478]] accuracy 0.7272727272727273\n","Iteration 46630 - loss value [[269.6299841]] accuracy 0.7272727272727273\n","Iteration 46631 - loss value [[272.14793352]] accuracy 0.7272727272727273\n","Iteration 46632 - loss value [[269.88853186]] accuracy 0.7272727272727273\n","Iteration 46633 - loss value [[271.6612293]] accuracy 0.7272727272727273\n","Iteration 46634 - loss value [[269.87684539]] accuracy 0.7272727272727273\n","Iteration 46635 - loss value [[271.99847251]] accuracy 0.7272727272727273\n","Iteration 46636 - loss value [[269.34408512]] accuracy 0.7272727272727273\n","Iteration 46637 - loss value [[269.87962206]] accuracy 0.7272727272727273\n","Iteration 46638 - loss value [[268.71180151]] accuracy 0.7272727272727273\n","Iteration 46639 - loss value [[270.82751071]] accuracy 0.7272727272727273\n","Iteration 46640 - loss value [[268.35125447]] accuracy 0.7272727272727273\n","Iteration 46641 - loss value [[269.01257609]] accuracy 0.7272727272727273\n","Iteration 46642 - loss value [[268.02674065]] accuracy 0.7272727272727273\n","Iteration 46643 - loss value [[268.30285838]] accuracy 0.7272727272727273\n","Iteration 46644 - loss value [[268.3373642]] accuracy 0.7272727272727273\n","Iteration 46645 - loss value [[271.15773111]] accuracy 0.7272727272727273\n","Iteration 46646 - loss value [[268.7284401]] accuracy 0.7272727272727273\n","Iteration 46647 - loss value [[270.34985257]] accuracy 0.7272727272727273\n","Iteration 46648 - loss value [[269.25520542]] accuracy 0.7272727272727273\n","Iteration 46649 - loss value [[272.34096764]] accuracy 0.7272727272727273\n","Iteration 46650 - loss value [[269.9947996]] accuracy 0.7272727272727273\n","Iteration 46651 - loss value [[271.42404572]] accuracy 0.7272727272727273\n","Iteration 46652 - loss value [[269.45324847]] accuracy 0.7272727272727273\n","Iteration 46653 - loss value [[272.0862598]] accuracy 0.7272727272727273\n","Iteration 46654 - loss value [[269.98332758]] accuracy 0.7272727272727273\n","Iteration 46655 - loss value [[272.64283328]] accuracy 0.7272727272727273\n","Iteration 46656 - loss value [[270.38247946]] accuracy 0.7272727272727273\n","Iteration 46657 - loss value [[273.24699088]] accuracy 0.7272727272727273\n","Iteration 46658 - loss value [[271.19927313]] accuracy 0.7272727272727273\n","Iteration 46659 - loss value [[273.91824596]] accuracy 0.7272727272727273\n","Iteration 46660 - loss value [[271.50063274]] accuracy 0.7272727272727273\n","Iteration 46661 - loss value [[273.83227934]] accuracy 0.7272727272727273\n","Iteration 46662 - loss value [[271.98576661]] accuracy 0.7272727272727273\n","Iteration 46663 - loss value [[275.38808931]] accuracy 0.7272727272727273\n","Iteration 46664 - loss value [[273.73342889]] accuracy 0.7272727272727273\n","Iteration 46665 - loss value [[275.38280295]] accuracy 0.7272727272727273\n","Iteration 46666 - loss value [[273.94749971]] accuracy 0.7272727272727273\n","Iteration 46667 - loss value [[275.33206499]] accuracy 0.7272727272727273\n","Iteration 46668 - loss value [[273.87806538]] accuracy 0.7272727272727273\n","Iteration 46669 - loss value [[275.42100417]] accuracy 0.7272727272727273\n","Iteration 46670 - loss value [[273.70827203]] accuracy 0.7272727272727273\n","Iteration 46671 - loss value [[275.23400273]] accuracy 0.7272727272727273\n","Iteration 46672 - loss value [[273.13197351]] accuracy 0.7272727272727273\n","Iteration 46673 - loss value [[275.35038278]] accuracy 0.7272727272727273\n","Iteration 46674 - loss value [[273.29388085]] accuracy 0.7272727272727273\n","Iteration 46675 - loss value [[275.50596614]] accuracy 0.7272727272727273\n","Iteration 46676 - loss value [[273.14724963]] accuracy 0.7272727272727273\n","Iteration 46677 - loss value [[274.18511591]] accuracy 0.7272727272727273\n","Iteration 46678 - loss value [[272.41779297]] accuracy 0.7272727272727273\n","Iteration 46679 - loss value [[275.13077782]] accuracy 0.7272727272727273\n","Iteration 46680 - loss value [[272.52216715]] accuracy 0.7272727272727273\n","Iteration 46681 - loss value [[273.68491625]] accuracy 0.7272727272727273\n","Iteration 46682 - loss value [[271.60198296]] accuracy 0.7272727272727273\n","Iteration 46683 - loss value [[272.71387222]] accuracy 0.7272727272727273\n","Iteration 46684 - loss value [[269.84085979]] accuracy 0.7272727272727273\n","Iteration 46685 - loss value [[270.33754035]] accuracy 0.7272727272727273\n","Iteration 46686 - loss value [[268.78288904]] accuracy 0.7272727272727273\n","Iteration 46687 - loss value [[268.8563646]] accuracy 0.7272727272727273\n","Iteration 46688 - loss value [[268.54181101]] accuracy 0.7272727272727273\n","Iteration 46689 - loss value [[271.37500828]] accuracy 0.7272727272727273\n","Iteration 46690 - loss value [[268.67971719]] accuracy 0.7272727272727273\n","Iteration 46691 - loss value [[269.32043543]] accuracy 0.7272727272727273\n","Iteration 46692 - loss value [[268.42752468]] accuracy 0.7272727272727273\n","Iteration 46693 - loss value [[269.73889391]] accuracy 0.7272727272727273\n","Iteration 46694 - loss value [[268.63191974]] accuracy 0.7272727272727273\n","Iteration 46695 - loss value [[269.60511559]] accuracy 0.7272727272727273\n","Iteration 46696 - loss value [[268.779132]] accuracy 0.7272727272727273\n","Iteration 46697 - loss value [[271.70324447]] accuracy 0.7272727272727273\n","Iteration 46698 - loss value [[269.40318136]] accuracy 0.7272727272727273\n","Iteration 46699 - loss value [[271.35003088]] accuracy 0.7272727272727273\n","Iteration 46700 - loss value [[269.90037735]] accuracy 0.7272727272727273\n","Iteration 46701 - loss value [[272.32770595]] accuracy 0.7272727272727273\n","Iteration 46702 - loss value [[270.2621268]] accuracy 0.7272727272727273\n","Iteration 46703 - loss value [[272.47472988]] accuracy 0.7272727272727273\n","Iteration 46704 - loss value [[269.72974195]] accuracy 0.7272727272727273\n","Iteration 46705 - loss value [[270.31726969]] accuracy 0.7272727272727273\n","Iteration 46706 - loss value [[268.88508188]] accuracy 0.7272727272727273\n","Iteration 46707 - loss value [[270.16793237]] accuracy 0.7272727272727273\n","Iteration 46708 - loss value [[269.6320308]] accuracy 0.7272727272727273\n","Iteration 46709 - loss value [[272.51964374]] accuracy 0.7272727272727273\n","Iteration 46710 - loss value [[270.2519373]] accuracy 0.7272727272727273\n","Iteration 46711 - loss value [[271.93277694]] accuracy 0.7272727272727273\n","Iteration 46712 - loss value [[270.65189435]] accuracy 0.7272727272727273\n","Iteration 46713 - loss value [[274.07231837]] accuracy 0.7272727272727273\n","Iteration 46714 - loss value [[271.65141886]] accuracy 0.7272727272727273\n","Iteration 46715 - loss value [[274.4123793]] accuracy 0.7272727272727273\n","Iteration 46716 - loss value [[272.78215406]] accuracy 0.7272727272727273\n","Iteration 46717 - loss value [[276.16842623]] accuracy 0.7272727272727273\n","Iteration 46718 - loss value [[274.92702657]] accuracy 0.7272727272727273\n","Iteration 46719 - loss value [[277.33105989]] accuracy 0.7272727272727273\n","Iteration 46720 - loss value [[277.07428911]] accuracy 0.7272727272727273\n","Iteration 46721 - loss value [[280.57880791]] accuracy 0.7272727272727273\n","Iteration 46722 - loss value [[279.146131]] accuracy 0.7272727272727273\n","Iteration 46723 - loss value [[281.77220293]] accuracy 0.7272727272727273\n","Iteration 46724 - loss value [[280.30649365]] accuracy 0.7272727272727273\n","Iteration 46725 - loss value [[282.06300045]] accuracy 0.7272727272727273\n","Iteration 46726 - loss value [[280.1416536]] accuracy 0.7272727272727273\n","Iteration 46727 - loss value [[280.83168518]] accuracy 0.7272727272727273\n","Iteration 46728 - loss value [[278.693998]] accuracy 0.7272727272727273\n","Iteration 46729 - loss value [[281.09068001]] accuracy 0.7272727272727273\n","Iteration 46730 - loss value [[278.86379549]] accuracy 0.7272727272727273\n","Iteration 46731 - loss value [[280.820367]] accuracy 0.7272727272727273\n","Iteration 46732 - loss value [[278.34385311]] accuracy 0.7272727272727273\n","Iteration 46733 - loss value [[278.77468388]] accuracy 0.7272727272727273\n","Iteration 46734 - loss value [[277.25219698]] accuracy 0.7272727272727273\n","Iteration 46735 - loss value [[279.15510611]] accuracy 0.7272727272727273\n","Iteration 46736 - loss value [[275.85938642]] accuracy 0.7272727272727273\n","Iteration 46737 - loss value [[275.22503507]] accuracy 0.7272727272727273\n","Iteration 46738 - loss value [[272.46816593]] accuracy 0.7272727272727273\n","Iteration 46739 - loss value [[273.40360831]] accuracy 0.7272727272727273\n","Iteration 46740 - loss value [[270.49156078]] accuracy 0.7272727272727273\n","Iteration 46741 - loss value [[270.64675896]] accuracy 0.7272727272727273\n","Iteration 46742 - loss value [[268.42390629]] accuracy 0.7272727272727273\n","Iteration 46743 - loss value [[268.13957936]] accuracy 0.7272727272727273\n","Iteration 46744 - loss value [[268.16951933]] accuracy 0.7272727272727273\n","Iteration 46745 - loss value [[270.38624598]] accuracy 0.7272727272727273\n","Iteration 46746 - loss value [[268.47815797]] accuracy 0.7272727272727273\n","Iteration 46747 - loss value [[268.30366312]] accuracy 0.7272727272727273\n","Iteration 46748 - loss value [[268.49349792]] accuracy 0.7272727272727273\n","Iteration 46749 - loss value [[271.11741079]] accuracy 0.7272727272727273\n","Iteration 46750 - loss value [[268.94040882]] accuracy 0.7272727272727273\n","Iteration 46751 - loss value [[269.01751274]] accuracy 0.7272727272727273\n","Iteration 46752 - loss value [[268.52622701]] accuracy 0.7272727272727273\n","Iteration 46753 - loss value [[270.64431681]] accuracy 0.7272727272727273\n","Iteration 46754 - loss value [[268.74728922]] accuracy 0.7272727272727273\n","Iteration 46755 - loss value [[269.14776746]] accuracy 0.7272727272727273\n","Iteration 46756 - loss value [[268.72120059]] accuracy 0.7272727272727273\n","Iteration 46757 - loss value [[271.65915029]] accuracy 0.7272727272727273\n","Iteration 46758 - loss value [[269.10213417]] accuracy 0.7272727272727273\n","Iteration 46759 - loss value [[270.46909524]] accuracy 0.7272727272727273\n","Iteration 46760 - loss value [[269.28782571]] accuracy 0.7272727272727273\n","Iteration 46761 - loss value [[272.13878222]] accuracy 0.7272727272727273\n","Iteration 46762 - loss value [[269.85236731]] accuracy 0.7272727272727273\n","Iteration 46763 - loss value [[271.11582472]] accuracy 0.7272727272727273\n","Iteration 46764 - loss value [[270.09271609]] accuracy 0.7272727272727273\n","Iteration 46765 - loss value [[272.98794555]] accuracy 0.7272727272727273\n","Iteration 46766 - loss value [[270.73737357]] accuracy 0.7272727272727273\n","Iteration 46767 - loss value [[272.64456754]] accuracy 0.7272727272727273\n","Iteration 46768 - loss value [[270.02557684]] accuracy 0.7272727272727273\n","Iteration 46769 - loss value [[271.84241286]] accuracy 0.7272727272727273\n","Iteration 46770 - loss value [[269.22038403]] accuracy 0.7272727272727273\n","Iteration 46771 - loss value [[269.60722168]] accuracy 0.7272727272727273\n","Iteration 46772 - loss value [[268.26539089]] accuracy 0.7272727272727273\n","Iteration 46773 - loss value [[268.57966613]] accuracy 0.7272727272727273\n","Iteration 46774 - loss value [[268.55895527]] accuracy 0.7272727272727273\n","Iteration 46775 - loss value [[271.29645565]] accuracy 0.7272727272727273\n","Iteration 46776 - loss value [[268.90169153]] accuracy 0.7272727272727273\n","Iteration 46777 - loss value [[270.77644712]] accuracy 0.7272727272727273\n","Iteration 46778 - loss value [[269.44646066]] accuracy 0.7272727272727273\n","Iteration 46779 - loss value [[272.45929668]] accuracy 0.7272727272727273\n","Iteration 46780 - loss value [[270.36937334]] accuracy 0.7272727272727273\n","Iteration 46781 - loss value [[272.27335551]] accuracy 0.7272727272727273\n","Iteration 46782 - loss value [[269.83742346]] accuracy 0.7272727272727273\n","Iteration 46783 - loss value [[271.33902584]] accuracy 0.7272727272727273\n","Iteration 46784 - loss value [[268.97526132]] accuracy 0.7272727272727273\n","Iteration 46785 - loss value [[269.64121043]] accuracy 0.7272727272727273\n","Iteration 46786 - loss value [[268.40280094]] accuracy 0.7272727272727273\n","Iteration 46787 - loss value [[270.47758492]] accuracy 0.7272727272727273\n","Iteration 46788 - loss value [[269.08150792]] accuracy 0.7272727272727273\n","Iteration 46789 - loss value [[271.67470755]] accuracy 0.7272727272727273\n","Iteration 46790 - loss value [[268.97869179]] accuracy 0.7272727272727273\n","Iteration 46791 - loss value [[269.80369915]] accuracy 0.7272727272727273\n","Iteration 46792 - loss value [[268.72134716]] accuracy 0.7272727272727273\n","Iteration 46793 - loss value [[271.07292959]] accuracy 0.7272727272727273\n","Iteration 46794 - loss value [[268.77021043]] accuracy 0.7272727272727273\n","Iteration 46795 - loss value [[269.72679945]] accuracy 0.7272727272727273\n","Iteration 46796 - loss value [[269.22665598]] accuracy 0.7272727272727273\n","Iteration 46797 - loss value [[272.23440587]] accuracy 0.7272727272727273\n","Iteration 46798 - loss value [[270.05704498]] accuracy 0.7272727272727273\n","Iteration 46799 - loss value [[272.13916897]] accuracy 0.7272727272727273\n","Iteration 46800 - loss value [[269.66298029]] accuracy 0.7272727272727273\n","Iteration 46801 - loss value [[271.58806661]] accuracy 0.7272727272727273\n","Iteration 46802 - loss value [[269.17314447]] accuracy 0.7272727272727273\n","Iteration 46803 - loss value [[270.62488102]] accuracy 0.7272727272727273\n","Iteration 46804 - loss value [[269.71628774]] accuracy 0.7272727272727273\n","Iteration 46805 - loss value [[273.11671956]] accuracy 0.7272727272727273\n","Iteration 46806 - loss value [[270.62872504]] accuracy 0.7272727272727273\n","Iteration 46807 - loss value [[272.29740886]] accuracy 0.7272727272727273\n","Iteration 46808 - loss value [[270.40884726]] accuracy 0.7272727272727273\n","Iteration 46809 - loss value [[273.931472]] accuracy 0.7272727272727273\n","Iteration 46810 - loss value [[271.37556653]] accuracy 0.7272727272727273\n","Iteration 46811 - loss value [[273.32057753]] accuracy 0.7272727272727273\n","Iteration 46812 - loss value [[271.60679915]] accuracy 0.7272727272727273\n","Iteration 46813 - loss value [[275.25091008]] accuracy 0.7272727272727273\n","Iteration 46814 - loss value [[274.20520843]] accuracy 0.7272727272727273\n","Iteration 46815 - loss value [[277.48895928]] accuracy 0.7272727272727273\n","Iteration 46816 - loss value [[277.10700295]] accuracy 0.7272727272727273\n","Iteration 46817 - loss value [[280.31200749]] accuracy 0.7272727272727273\n","Iteration 46818 - loss value [[279.47395724]] accuracy 0.7272727272727273\n","Iteration 46819 - loss value [[281.89877303]] accuracy 0.7272727272727273\n","Iteration 46820 - loss value [[280.75513464]] accuracy 0.7272727272727273\n","Iteration 46821 - loss value [[281.75652865]] accuracy 0.7272727272727273\n","Iteration 46822 - loss value [[280.29613832]] accuracy 0.7272727272727273\n","Iteration 46823 - loss value [[281.84812373]] accuracy 0.7272727272727273\n","Iteration 46824 - loss value [[280.0246886]] accuracy 0.7272727272727273\n","Iteration 46825 - loss value [[281.5641496]] accuracy 0.7272727272727273\n","Iteration 46826 - loss value [[279.28059219]] accuracy 0.7272727272727273\n","Iteration 46827 - loss value [[280.19765345]] accuracy 0.7272727272727273\n","Iteration 46828 - loss value [[277.45521658]] accuracy 0.7272727272727273\n","Iteration 46829 - loss value [[277.09243371]] accuracy 0.7272727272727273\n","Iteration 46830 - loss value [[276.08809655]] accuracy 0.7272727272727273\n","Iteration 46831 - loss value [[277.78526429]] accuracy 0.7272727272727273\n","Iteration 46832 - loss value [[276.57723893]] accuracy 0.7272727272727273\n","Iteration 46833 - loss value [[278.22418127]] accuracy 0.7272727272727273\n","Iteration 46834 - loss value [[276.95245361]] accuracy 0.7272727272727273\n","Iteration 46835 - loss value [[278.57269996]] accuracy 0.7272727272727273\n","Iteration 46836 - loss value [[276.35057917]] accuracy 0.7272727272727273\n","Iteration 46837 - loss value [[276.77700374]] accuracy 0.7272727272727273\n","Iteration 46838 - loss value [[274.27692222]] accuracy 0.7272727272727273\n","Iteration 46839 - loss value [[273.67748631]] accuracy 0.7272727272727273\n","Iteration 46840 - loss value [[270.51906475]] accuracy 0.7272727272727273\n","Iteration 46841 - loss value [[271.0224853]] accuracy 0.7272727272727273\n","Iteration 46842 - loss value [[268.88613848]] accuracy 0.7272727272727273\n","Iteration 46843 - loss value [[268.6463366]] accuracy 0.7272727272727273\n","Iteration 46844 - loss value [[267.82585803]] accuracy 0.7272727272727273\n","Iteration 46845 - loss value [[268.68287942]] accuracy 0.7272727272727273\n","Iteration 46846 - loss value [[267.89762169]] accuracy 0.7272727272727273\n","Iteration 46847 - loss value [[269.47278381]] accuracy 0.7272727272727273\n","Iteration 46848 - loss value [[268.51825653]] accuracy 0.7272727272727273\n","Iteration 46849 - loss value [[270.25350729]] accuracy 0.7272727272727273\n","Iteration 46850 - loss value [[268.77163562]] accuracy 0.7272727272727273\n","Iteration 46851 - loss value [[270.5057095]] accuracy 0.7272727272727273\n","Iteration 46852 - loss value [[269.10491641]] accuracy 0.7272727272727273\n","Iteration 46853 - loss value [[271.83852961]] accuracy 0.7272727272727273\n","Iteration 46854 - loss value [[269.35350033]] accuracy 0.7272727272727273\n","Iteration 46855 - loss value [[270.67713019]] accuracy 0.7272727272727273\n","Iteration 46856 - loss value [[269.62916129]] accuracy 0.7272727272727273\n","Iteration 46857 - loss value [[272.12154471]] accuracy 0.7272727272727273\n","Iteration 46858 - loss value [[269.90527865]] accuracy 0.7272727272727273\n","Iteration 46859 - loss value [[271.48193463]] accuracy 0.7272727272727273\n","Iteration 46860 - loss value [[269.8426475]] accuracy 0.7272727272727273\n","Iteration 46861 - loss value [[271.68461075]] accuracy 0.7272727272727273\n","Iteration 46862 - loss value [[269.05226916]] accuracy 0.7272727272727273\n","Iteration 46863 - loss value [[269.91875736]] accuracy 0.7272727272727273\n","Iteration 46864 - loss value [[268.64462418]] accuracy 0.7272727272727273\n","Iteration 46865 - loss value [[270.25191151]] accuracy 0.7272727272727273\n","Iteration 46866 - loss value [[268.87147384]] accuracy 0.7272727272727273\n","Iteration 46867 - loss value [[270.91437144]] accuracy 0.7272727272727273\n","Iteration 46868 - loss value [[269.42336371]] accuracy 0.7272727272727273\n","Iteration 46869 - loss value [[271.82203807]] accuracy 0.7272727272727273\n","Iteration 46870 - loss value [[269.0852327]] accuracy 0.7272727272727273\n","Iteration 46871 - loss value [[269.70417775]] accuracy 0.7272727272727273\n","Iteration 46872 - loss value [[268.50380971]] accuracy 0.7272727272727273\n","Iteration 46873 - loss value [[270.3634292]] accuracy 0.7272727272727273\n","Iteration 46874 - loss value [[268.99878546]] accuracy 0.7272727272727273\n","Iteration 46875 - loss value [[270.02472028]] accuracy 0.7272727272727273\n","Iteration 46876 - loss value [[268.85086026]] accuracy 0.7272727272727273\n","Iteration 46877 - loss value [[271.78017484]] accuracy 0.7272727272727273\n","Iteration 46878 - loss value [[269.66790807]] accuracy 0.7272727272727273\n","Iteration 46879 - loss value [[271.59640852]] accuracy 0.7272727272727273\n","Iteration 46880 - loss value [[270.07370014]] accuracy 0.7272727272727273\n","Iteration 46881 - loss value [[273.02270558]] accuracy 0.7272727272727273\n","Iteration 46882 - loss value [[270.77726367]] accuracy 0.7272727272727273\n","Iteration 46883 - loss value [[273.12449014]] accuracy 0.7272727272727273\n","Iteration 46884 - loss value [[270.8976123]] accuracy 0.7272727272727273\n","Iteration 46885 - loss value [[274.11758441]] accuracy 0.7272727272727273\n","Iteration 46886 - loss value [[271.47511783]] accuracy 0.7272727272727273\n","Iteration 46887 - loss value [[273.03466424]] accuracy 0.7272727272727273\n","Iteration 46888 - loss value [[270.77979729]] accuracy 0.7272727272727273\n","Iteration 46889 - loss value [[273.68635533]] accuracy 0.7272727272727273\n","Iteration 46890 - loss value [[271.45090058]] accuracy 0.7272727272727273\n","Iteration 46891 - loss value [[273.37233885]] accuracy 0.7272727272727273\n","Iteration 46892 - loss value [[270.43159395]] accuracy 0.7272727272727273\n","Iteration 46893 - loss value [[271.66678485]] accuracy 0.7272727272727273\n","Iteration 46894 - loss value [[269.12278333]] accuracy 0.7272727272727273\n","Iteration 46895 - loss value [[269.83209618]] accuracy 0.7272727272727273\n","Iteration 46896 - loss value [[268.66037487]] accuracy 0.7272727272727273\n","Iteration 46897 - loss value [[269.58354379]] accuracy 0.7272727272727273\n","Iteration 46898 - loss value [[268.42952338]] accuracy 0.7272727272727273\n","Iteration 46899 - loss value [[269.19512681]] accuracy 0.7272727272727273\n","Iteration 46900 - loss value [[269.09484117]] accuracy 0.7272727272727273\n","Iteration 46901 - loss value [[271.89896255]] accuracy 0.7272727272727273\n","Iteration 46902 - loss value [[269.88709047]] accuracy 0.7272727272727273\n","Iteration 46903 - loss value [[271.9033192]] accuracy 0.7272727272727273\n","Iteration 46904 - loss value [[269.43775231]] accuracy 0.7272727272727273\n","Iteration 46905 - loss value [[271.31498742]] accuracy 0.7272727272727273\n","Iteration 46906 - loss value [[268.80649313]] accuracy 0.7272727272727273\n","Iteration 46907 - loss value [[269.75908713]] accuracy 0.7272727272727273\n","Iteration 46908 - loss value [[269.07976475]] accuracy 0.7272727272727273\n","Iteration 46909 - loss value [[272.16348597]] accuracy 0.7272727272727273\n","Iteration 46910 - loss value [[270.11195936]] accuracy 0.7272727272727273\n","Iteration 46911 - loss value [[272.92579278]] accuracy 0.7272727272727273\n","Iteration 46912 - loss value [[270.67601917]] accuracy 0.7272727272727273\n","Iteration 46913 - loss value [[273.95560531]] accuracy 0.7272727272727273\n","Iteration 46914 - loss value [[271.28751594]] accuracy 0.7272727272727273\n","Iteration 46915 - loss value [[272.98527926]] accuracy 0.7272727272727273\n","Iteration 46916 - loss value [[270.57786801]] accuracy 0.7272727272727273\n","Iteration 46917 - loss value [[272.91377024]] accuracy 0.7272727272727273\n","Iteration 46918 - loss value [[270.59423376]] accuracy 0.7272727272727273\n","Iteration 46919 - loss value [[272.52309374]] accuracy 0.7272727272727273\n","Iteration 46920 - loss value [[270.03182959]] accuracy 0.7272727272727273\n","Iteration 46921 - loss value [[271.32696972]] accuracy 0.7272727272727273\n","Iteration 46922 - loss value [[268.99952771]] accuracy 0.7272727272727273\n","Iteration 46923 - loss value [[269.96096665]] accuracy 0.7272727272727273\n","Iteration 46924 - loss value [[268.83501231]] accuracy 0.7272727272727273\n","Iteration 46925 - loss value [[270.94921574]] accuracy 0.7272727272727273\n","Iteration 46926 - loss value [[268.35650311]] accuracy 0.7272727272727273\n","Iteration 46927 - loss value [[268.65281995]] accuracy 0.7272727272727273\n","Iteration 46928 - loss value [[268.05131386]] accuracy 0.7272727272727273\n","Iteration 46929 - loss value [[269.28980646]] accuracy 0.7272727272727273\n","Iteration 46930 - loss value [[268.52129135]] accuracy 0.7272727272727273\n","Iteration 46931 - loss value [[271.54130419]] accuracy 0.7272727272727273\n","Iteration 46932 - loss value [[269.53363732]] accuracy 0.7272727272727273\n","Iteration 46933 - loss value [[272.14371094]] accuracy 0.7272727272727273\n","Iteration 46934 - loss value [[270.24366535]] accuracy 0.7272727272727273\n","Iteration 46935 - loss value [[273.92920098]] accuracy 0.7272727272727273\n","Iteration 46936 - loss value [[271.49858169]] accuracy 0.7272727272727273\n","Iteration 46937 - loss value [[273.56489737]] accuracy 0.7272727272727273\n","Iteration 46938 - loss value [[272.01713364]] accuracy 0.7272727272727273\n","Iteration 46939 - loss value [[276.0344031]] accuracy 0.7272727272727273\n","Iteration 46940 - loss value [[275.33720977]] accuracy 0.7272727272727273\n","Iteration 46941 - loss value [[278.46806699]] accuracy 0.7272727272727273\n","Iteration 46942 - loss value [[278.38874788]] accuracy 0.7272727272727273\n","Iteration 46943 - loss value [[281.1665345]] accuracy 0.7272727272727273\n","Iteration 46944 - loss value [[279.60172405]] accuracy 0.7272727272727273\n","Iteration 46945 - loss value [[281.37658482]] accuracy 0.7272727272727273\n","Iteration 46946 - loss value [[280.3325129]] accuracy 0.7272727272727273\n","Iteration 46947 - loss value [[281.61982848]] accuracy 0.7272727272727273\n","Iteration 46948 - loss value [[280.25268787]] accuracy 0.7272727272727273\n","Iteration 46949 - loss value [[281.28471167]] accuracy 0.7272727272727273\n","Iteration 46950 - loss value [[279.36299793]] accuracy 0.7272727272727273\n","Iteration 46951 - loss value [[281.0931402]] accuracy 0.7272727272727273\n","Iteration 46952 - loss value [[278.8802773]] accuracy 0.7272727272727273\n","Iteration 46953 - loss value [[279.46021775]] accuracy 0.7272727272727273\n","Iteration 46954 - loss value [[278.40900395]] accuracy 0.7272727272727273\n","Iteration 46955 - loss value [[280.11575493]] accuracy 0.7272727272727273\n","Iteration 46956 - loss value [[276.88248286]] accuracy 0.7272727272727273\n","Iteration 46957 - loss value [[276.78041026]] accuracy 0.7272727272727273\n","Iteration 46958 - loss value [[274.22468556]] accuracy 0.7272727272727273\n","Iteration 46959 - loss value [[275.0206344]] accuracy 0.7272727272727273\n","Iteration 46960 - loss value [[272.29128249]] accuracy 0.7272727272727273\n","Iteration 46961 - loss value [[272.81675546]] accuracy 0.7272727272727273\n","Iteration 46962 - loss value [[270.27518813]] accuracy 0.7272727272727273\n","Iteration 46963 - loss value [[271.49814749]] accuracy 0.7272727272727273\n","Iteration 46964 - loss value [[269.15872512]] accuracy 0.7272727272727273\n","Iteration 46965 - loss value [[268.85603154]] accuracy 0.7272727272727273\n","Iteration 46966 - loss value [[267.8005878]] accuracy 0.7272727272727273\n","Iteration 46967 - loss value [[269.30184927]] accuracy 0.7272727272727273\n","Iteration 46968 - loss value [[268.3176516]] accuracy 0.7272727272727273\n","Iteration 46969 - loss value [[268.95611153]] accuracy 0.7272727272727273\n","Iteration 46970 - loss value [[268.50827339]] accuracy 0.7272727272727273\n","Iteration 46971 - loss value [[271.11431782]] accuracy 0.7272727272727273\n","Iteration 46972 - loss value [[268.50481548]] accuracy 0.7272727272727273\n","Iteration 46973 - loss value [[268.7275383]] accuracy 0.7272727272727273\n","Iteration 46974 - loss value [[268.41352832]] accuracy 0.7272727272727273\n","Iteration 46975 - loss value [[270.58434998]] accuracy 0.7272727272727273\n","Iteration 46976 - loss value [[269.34382032]] accuracy 0.7272727272727273\n","Iteration 46977 - loss value [[271.20186064]] accuracy 0.7272727272727273\n","Iteration 46978 - loss value [[269.739339]] accuracy 0.7272727272727273\n","Iteration 46979 - loss value [[272.82747059]] accuracy 0.7272727272727273\n","Iteration 46980 - loss value [[270.32608582]] accuracy 0.7272727272727273\n","Iteration 46981 - loss value [[272.24029201]] accuracy 0.7272727272727273\n","Iteration 46982 - loss value [[269.5388048]] accuracy 0.7272727272727273\n","Iteration 46983 - loss value [[270.65486213]] accuracy 0.7272727272727273\n","Iteration 46984 - loss value [[269.355232]] accuracy 0.7272727272727273\n","Iteration 46985 - loss value [[271.94780229]] accuracy 0.7272727272727273\n","Iteration 46986 - loss value [[269.66105711]] accuracy 0.7272727272727273\n","Iteration 46987 - loss value [[271.52547686]] accuracy 0.7272727272727273\n","Iteration 46988 - loss value [[269.9936042]] accuracy 0.7272727272727273\n","Iteration 46989 - loss value [[272.76840215]] accuracy 0.7272727272727273\n","Iteration 46990 - loss value [[270.35320947]] accuracy 0.7272727272727273\n","Iteration 46991 - loss value [[272.64915574]] accuracy 0.7272727272727273\n","Iteration 46992 - loss value [[269.77577128]] accuracy 0.7272727272727273\n","Iteration 46993 - loss value [[270.09512107]] accuracy 0.7272727272727273\n","Iteration 46994 - loss value [[268.73843312]] accuracy 0.7272727272727273\n","Iteration 46995 - loss value [[270.69024364]] accuracy 0.7272727272727273\n","Iteration 46996 - loss value [[268.18725528]] accuracy 0.7272727272727273\n","Iteration 46997 - loss value [[268.55511107]] accuracy 0.7272727272727273\n","Iteration 46998 - loss value [[267.46866618]] accuracy 0.7272727272727273\n","Iteration 46999 - loss value [[267.95288246]] accuracy 0.7272727272727273\n","Iteration 47000 - loss value [[268.80198973]] accuracy 0.7272727272727273\n","Iteration 47001 - loss value [[271.58475284]] accuracy 0.7272727272727273\n","Iteration 47002 - loss value [[269.36959721]] accuracy 0.7272727272727273\n","Iteration 47003 - loss value [[271.44371136]] accuracy 0.7272727272727273\n","Iteration 47004 - loss value [[268.92234419]] accuracy 0.7272727272727273\n","Iteration 47005 - loss value [[270.4159255]] accuracy 0.7272727272727273\n","Iteration 47006 - loss value [[269.78878177]] accuracy 0.7272727272727273\n","Iteration 47007 - loss value [[273.0538159]] accuracy 0.7272727272727273\n","Iteration 47008 - loss value [[270.35947705]] accuracy 0.7272727272727273\n","Iteration 47009 - loss value [[271.8846588]] accuracy 0.7272727272727273\n","Iteration 47010 - loss value [[270.17692]] accuracy 0.7272727272727273\n","Iteration 47011 - loss value [[273.43876506]] accuracy 0.7272727272727273\n","Iteration 47012 - loss value [[271.25522982]] accuracy 0.7272727272727273\n","Iteration 47013 - loss value [[273.48657384]] accuracy 0.7272727272727273\n","Iteration 47014 - loss value [[271.71543526]] accuracy 0.7272727272727273\n","Iteration 47015 - loss value [[274.97375928]] accuracy 0.7272727272727273\n","Iteration 47016 - loss value [[272.65027064]] accuracy 0.7272727272727273\n","Iteration 47017 - loss value [[275.4723988]] accuracy 0.7272727272727273\n","Iteration 47018 - loss value [[273.9868792]] accuracy 0.7272727272727273\n","Iteration 47019 - loss value [[276.90429172]] accuracy 0.7272727272727273\n","Iteration 47020 - loss value [[275.87827329]] accuracy 0.7272727272727273\n","Iteration 47021 - loss value [[278.55436758]] accuracy 0.7272727272727273\n","Iteration 47022 - loss value [[276.82650663]] accuracy 0.7272727272727273\n","Iteration 47023 - loss value [[278.17258523]] accuracy 0.7272727272727273\n","Iteration 47024 - loss value [[278.02971609]] accuracy 0.7272727272727273\n","Iteration 47025 - loss value [[281.10173398]] accuracy 0.7272727272727273\n","Iteration 47026 - loss value [[279.30016706]] accuracy 0.7272727272727273\n","Iteration 47027 - loss value [[281.2352809]] accuracy 0.7272727272727273\n","Iteration 47028 - loss value [[279.49052391]] accuracy 0.7272727272727273\n","Iteration 47029 - loss value [[281.11812153]] accuracy 0.7272727272727273\n","Iteration 47030 - loss value [[278.7643677]] accuracy 0.7272727272727273\n","Iteration 47031 - loss value [[278.30266989]] accuracy 0.7272727272727273\n","Iteration 47032 - loss value [[277.44460762]] accuracy 0.7272727272727273\n","Iteration 47033 - loss value [[279.49198028]] accuracy 0.7272727272727273\n","Iteration 47034 - loss value [[277.14369697]] accuracy 0.7272727272727273\n","Iteration 47035 - loss value [[277.50705988]] accuracy 0.7272727272727273\n","Iteration 47036 - loss value [[276.03543707]] accuracy 0.7272727272727273\n","Iteration 47037 - loss value [[277.67120386]] accuracy 0.7272727272727273\n","Iteration 47038 - loss value [[275.56260157]] accuracy 0.7272727272727273\n","Iteration 47039 - loss value [[276.13468929]] accuracy 0.7272727272727273\n","Iteration 47040 - loss value [[273.26474146]] accuracy 0.7272727272727273\n","Iteration 47041 - loss value [[273.46182999]] accuracy 0.7272727272727273\n","Iteration 47042 - loss value [[270.06371704]] accuracy 0.7272727272727273\n","Iteration 47043 - loss value [[270.02922817]] accuracy 0.7272727272727273\n","Iteration 47044 - loss value [[268.12700545]] accuracy 0.7272727272727273\n","Iteration 47045 - loss value [[267.65563368]] accuracy 0.7272727272727273\n","Iteration 47046 - loss value [[267.11935012]] accuracy 0.7272727272727273\n","Iteration 47047 - loss value [[268.61032808]] accuracy 0.7272727272727273\n","Iteration 47048 - loss value [[267.64535608]] accuracy 0.7272727272727273\n","Iteration 47049 - loss value [[268.06949014]] accuracy 0.7272727272727273\n","Iteration 47050 - loss value [[267.61328116]] accuracy 0.7272727272727273\n","Iteration 47051 - loss value [[268.52934464]] accuracy 0.7272727272727273\n","Iteration 47052 - loss value [[267.84556114]] accuracy 0.7272727272727273\n","Iteration 47053 - loss value [[269.27069931]] accuracy 0.7272727272727273\n","Iteration 47054 - loss value [[268.65048394]] accuracy 0.7272727272727273\n","Iteration 47055 - loss value [[270.82109714]] accuracy 0.7272727272727273\n","Iteration 47056 - loss value [[269.65943256]] accuracy 0.7272727272727273\n","Iteration 47057 - loss value [[272.6678385]] accuracy 0.7272727272727273\n","Iteration 47058 - loss value [[269.82148369]] accuracy 0.7272727272727273\n","Iteration 47059 - loss value [[271.09676652]] accuracy 0.7272727272727273\n","Iteration 47060 - loss value [[268.63103732]] accuracy 0.7272727272727273\n","Iteration 47061 - loss value [[269.38968201]] accuracy 0.7272727272727273\n","Iteration 47062 - loss value [[268.46763767]] accuracy 0.7272727272727273\n","Iteration 47063 - loss value [[271.09506195]] accuracy 0.7272727272727273\n","Iteration 47064 - loss value [[269.05136508]] accuracy 0.7272727272727273\n","Iteration 47065 - loss value [[271.94243205]] accuracy 0.7272727272727273\n","Iteration 47066 - loss value [[269.56367789]] accuracy 0.7272727272727273\n","Iteration 47067 - loss value [[271.41646467]] accuracy 0.7272727272727273\n","Iteration 47068 - loss value [[268.79559184]] accuracy 0.7272727272727273\n","Iteration 47069 - loss value [[269.56310141]] accuracy 0.7272727272727273\n","Iteration 47070 - loss value [[268.53083487]] accuracy 0.7272727272727273\n","Iteration 47071 - loss value [[270.93626791]] accuracy 0.7272727272727273\n","Iteration 47072 - loss value [[268.86882517]] accuracy 0.7272727272727273\n","Iteration 47073 - loss value [[271.37167544]] accuracy 0.7272727272727273\n","Iteration 47074 - loss value [[268.7563925]] accuracy 0.7272727272727273\n","Iteration 47075 - loss value [[269.59538768]] accuracy 0.7272727272727273\n","Iteration 47076 - loss value [[268.65911918]] accuracy 0.7272727272727273\n","Iteration 47077 - loss value [[271.40608448]] accuracy 0.7272727272727273\n","Iteration 47078 - loss value [[269.03888468]] accuracy 0.7272727272727273\n","Iteration 47079 - loss value [[270.82447492]] accuracy 0.7272727272727273\n","Iteration 47080 - loss value [[269.53450691]] accuracy 0.7272727272727273\n","Iteration 47081 - loss value [[272.76470843]] accuracy 0.7272727272727273\n","Iteration 47082 - loss value [[270.02073275]] accuracy 0.7272727272727273\n","Iteration 47083 - loss value [[272.08075244]] accuracy 0.7272727272727273\n","Iteration 47084 - loss value [[270.15338131]] accuracy 0.7272727272727273\n","Iteration 47085 - loss value [[272.77827987]] accuracy 0.7272727272727273\n","Iteration 47086 - loss value [[270.23142787]] accuracy 0.7272727272727273\n","Iteration 47087 - loss value [[271.95110848]] accuracy 0.7272727272727273\n","Iteration 47088 - loss value [[269.58339807]] accuracy 0.7272727272727273\n","Iteration 47089 - loss value [[271.1010017]] accuracy 0.7272727272727273\n","Iteration 47090 - loss value [[269.67693365]] accuracy 0.7272727272727273\n","Iteration 47091 - loss value [[272.3932107]] accuracy 0.7272727272727273\n","Iteration 47092 - loss value [[270.04272163]] accuracy 0.7272727272727273\n","Iteration 47093 - loss value [[272.37780746]] accuracy 0.7272727272727273\n","Iteration 47094 - loss value [[270.07446111]] accuracy 0.7272727272727273\n","Iteration 47095 - loss value [[272.92857119]] accuracy 0.7272727272727273\n","Iteration 47096 - loss value [[270.60180577]] accuracy 0.7272727272727273\n","Iteration 47097 - loss value [[273.01856254]] accuracy 0.7272727272727273\n","Iteration 47098 - loss value [[271.04882401]] accuracy 0.7272727272727273\n","Iteration 47099 - loss value [[274.17229137]] accuracy 0.7272727272727273\n","Iteration 47100 - loss value [[271.58802513]] accuracy 0.7272727272727273\n","Iteration 47101 - loss value [[273.8723]] accuracy 0.7272727272727273\n","Iteration 47102 - loss value [[271.67131033]] accuracy 0.7272727272727273\n","Iteration 47103 - loss value [[274.59398797]] accuracy 0.7272727272727273\n","Iteration 47104 - loss value [[272.66222553]] accuracy 0.7272727272727273\n","Iteration 47105 - loss value [[274.53821212]] accuracy 0.7272727272727273\n","Iteration 47106 - loss value [[273.1308796]] accuracy 0.7272727272727273\n","Iteration 47107 - loss value [[275.99306516]] accuracy 0.7272727272727273\n","Iteration 47108 - loss value [[274.93319858]] accuracy 0.7272727272727273\n","Iteration 47109 - loss value [[277.33591715]] accuracy 0.7272727272727273\n","Iteration 47110 - loss value [[276.81308199]] accuracy 0.7272727272727273\n","Iteration 47111 - loss value [[279.95397356]] accuracy 0.7272727272727273\n","Iteration 47112 - loss value [[278.35510589]] accuracy 0.7272727272727273\n","Iteration 47113 - loss value [[281.49797491]] accuracy 0.7272727272727273\n","Iteration 47114 - loss value [[279.53943332]] accuracy 0.7272727272727273\n","Iteration 47115 - loss value [[281.61554975]] accuracy 0.7272727272727273\n","Iteration 47116 - loss value [[279.547013]] accuracy 0.7272727272727273\n","Iteration 47117 - loss value [[280.58075389]] accuracy 0.7272727272727273\n","Iteration 47118 - loss value [[277.60438173]] accuracy 0.7272727272727273\n","Iteration 47119 - loss value [[277.64232636]] accuracy 0.7272727272727273\n","Iteration 47120 - loss value [[276.48750718]] accuracy 0.7272727272727273\n","Iteration 47121 - loss value [[278.74371227]] accuracy 0.7272727272727273\n","Iteration 47122 - loss value [[276.62814161]] accuracy 0.7272727272727273\n","Iteration 47123 - loss value [[277.18698678]] accuracy 0.7272727272727273\n","Iteration 47124 - loss value [[275.6582473]] accuracy 0.7272727272727273\n","Iteration 47125 - loss value [[276.83484915]] accuracy 0.7272727272727273\n","Iteration 47126 - loss value [[274.93260249]] accuracy 0.7272727272727273\n","Iteration 47127 - loss value [[274.4691349]] accuracy 0.7272727272727273\n","Iteration 47128 - loss value [[271.49119738]] accuracy 0.7272727272727273\n","Iteration 47129 - loss value [[271.69953375]] accuracy 0.7272727272727273\n","Iteration 47130 - loss value [[269.66449942]] accuracy 0.7272727272727273\n","Iteration 47131 - loss value [[270.7014574]] accuracy 0.7272727272727273\n","Iteration 47132 - loss value [[268.60738914]] accuracy 0.7272727272727273\n","Iteration 47133 - loss value [[268.69450259]] accuracy 0.7272727272727273\n","Iteration 47134 - loss value [[268.17377231]] accuracy 0.7272727272727273\n","Iteration 47135 - loss value [[269.88487032]] accuracy 0.7272727272727273\n","Iteration 47136 - loss value [[268.25492266]] accuracy 0.7272727272727273\n","Iteration 47137 - loss value [[268.52155792]] accuracy 0.7272727272727273\n","Iteration 47138 - loss value [[268.3400356]] accuracy 0.7272727272727273\n","Iteration 47139 - loss value [[270.94126478]] accuracy 0.7272727272727273\n","Iteration 47140 - loss value [[268.78931441]] accuracy 0.7272727272727273\n","Iteration 47141 - loss value [[269.33210048]] accuracy 0.7272727272727273\n","Iteration 47142 - loss value [[268.74845286]] accuracy 0.7272727272727273\n","Iteration 47143 - loss value [[271.52791224]] accuracy 0.7272727272727273\n","Iteration 47144 - loss value [[268.63186538]] accuracy 0.7272727272727273\n","Iteration 47145 - loss value [[269.52218912]] accuracy 0.7272727272727273\n","Iteration 47146 - loss value [[268.48375351]] accuracy 0.7272727272727273\n","Iteration 47147 - loss value [[270.89787974]] accuracy 0.7272727272727273\n","Iteration 47148 - loss value [[268.36904606]] accuracy 0.7272727272727273\n","Iteration 47149 - loss value [[268.90828478]] accuracy 0.7272727272727273\n","Iteration 47150 - loss value [[268.24917529]] accuracy 0.7272727272727273\n","Iteration 47151 - loss value [[271.04768735]] accuracy 0.7272727272727273\n","Iteration 47152 - loss value [[268.5147883]] accuracy 0.7272727272727273\n","Iteration 47153 - loss value [[269.17900784]] accuracy 0.7272727272727273\n","Iteration 47154 - loss value [[268.42491916]] accuracy 0.7272727272727273\n","Iteration 47155 - loss value [[271.07276135]] accuracy 0.7272727272727273\n","Iteration 47156 - loss value [[268.84022735]] accuracy 0.7272727272727273\n","Iteration 47157 - loss value [[270.60659845]] accuracy 0.7272727272727273\n","Iteration 47158 - loss value [[269.64598389]] accuracy 0.7272727272727273\n","Iteration 47159 - loss value [[272.69446866]] accuracy 0.7272727272727273\n","Iteration 47160 - loss value [[270.24167064]] accuracy 0.7272727272727273\n","Iteration 47161 - loss value [[272.24494025]] accuracy 0.7272727272727273\n","Iteration 47162 - loss value [[269.70248499]] accuracy 0.7272727272727273\n","Iteration 47163 - loss value [[271.32335043]] accuracy 0.7272727272727273\n","Iteration 47164 - loss value [[268.83404825]] accuracy 0.7272727272727273\n","Iteration 47165 - loss value [[269.96336292]] accuracy 0.7272727272727273\n","Iteration 47166 - loss value [[269.32193027]] accuracy 0.7272727272727273\n","Iteration 47167 - loss value [[272.78976077]] accuracy 0.7272727272727273\n","Iteration 47168 - loss value [[269.97018985]] accuracy 0.7272727272727273\n","Iteration 47169 - loss value [[271.95658102]] accuracy 0.7272727272727273\n","Iteration 47170 - loss value [[269.73457371]] accuracy 0.7272727272727273\n","Iteration 47171 - loss value [[272.76278507]] accuracy 0.7272727272727273\n","Iteration 47172 - loss value [[269.9714765]] accuracy 0.7272727272727273\n","Iteration 47173 - loss value [[271.42113999]] accuracy 0.7272727272727273\n","Iteration 47174 - loss value [[269.06418023]] accuracy 0.7272727272727273\n","Iteration 47175 - loss value [[270.18192756]] accuracy 0.7272727272727273\n","Iteration 47176 - loss value [[269.46438925]] accuracy 0.7272727272727273\n","Iteration 47177 - loss value [[272.64517542]] accuracy 0.7272727272727273\n","Iteration 47178 - loss value [[269.85744394]] accuracy 0.7272727272727273\n","Iteration 47179 - loss value [[271.33995758]] accuracy 0.7272727272727273\n","Iteration 47180 - loss value [[269.01855387]] accuracy 0.7272727272727273\n","Iteration 47181 - loss value [[270.19821581]] accuracy 0.7272727272727273\n","Iteration 47182 - loss value [[269.27065649]] accuracy 0.7272727272727273\n","Iteration 47183 - loss value [[272.58113514]] accuracy 0.7272727272727273\n","Iteration 47184 - loss value [[269.84169661]] accuracy 0.7272727272727273\n","Iteration 47185 - loss value [[271.36558559]] accuracy 0.7272727272727273\n","Iteration 47186 - loss value [[269.07528795]] accuracy 0.7272727272727273\n","Iteration 47187 - loss value [[270.93808638]] accuracy 0.7272727272727273\n","Iteration 47188 - loss value [[268.28400284]] accuracy 0.7272727272727273\n","Iteration 47189 - loss value [[268.11309829]] accuracy 0.7272727272727273\n","Iteration 47190 - loss value [[267.62040332]] accuracy 0.7272727272727273\n","Iteration 47191 - loss value [[268.77867592]] accuracy 0.7272727272727273\n","Iteration 47192 - loss value [[268.18898863]] accuracy 0.7272727272727273\n","Iteration 47193 - loss value [[270.57041288]] accuracy 0.7272727272727273\n","Iteration 47194 - loss value [[268.31204054]] accuracy 0.7272727272727273\n","Iteration 47195 - loss value [[269.39951389]] accuracy 0.7272727272727273\n","Iteration 47196 - loss value [[269.06223784]] accuracy 0.7272727272727273\n","Iteration 47197 - loss value [[272.47652798]] accuracy 0.7272727272727273\n","Iteration 47198 - loss value [[270.32121656]] accuracy 0.7272727272727273\n","Iteration 47199 - loss value [[273.23144779]] accuracy 0.7272727272727273\n","Iteration 47200 - loss value [[270.91456975]] accuracy 0.7272727272727273\n","Iteration 47201 - loss value [[273.94960092]] accuracy 0.7272727272727273\n","Iteration 47202 - loss value [[271.25530999]] accuracy 0.7272727272727273\n","Iteration 47203 - loss value [[273.71208958]] accuracy 0.7272727272727273\n","Iteration 47204 - loss value [[271.67122928]] accuracy 0.7272727272727273\n","Iteration 47205 - loss value [[274.9330958]] accuracy 0.7272727272727273\n","Iteration 47206 - loss value [[273.5863862]] accuracy 0.7272727272727273\n","Iteration 47207 - loss value [[276.95227549]] accuracy 0.7272727272727273\n","Iteration 47208 - loss value [[275.95946775]] accuracy 0.7272727272727273\n","Iteration 47209 - loss value [[278.88787774]] accuracy 0.7272727272727273\n","Iteration 47210 - loss value [[277.95371666]] accuracy 0.7272727272727273\n","Iteration 47211 - loss value [[280.8616135]] accuracy 0.7272727272727273\n","Iteration 47212 - loss value [[278.64772433]] accuracy 0.7272727272727273\n","Iteration 47213 - loss value [[279.12861032]] accuracy 0.7272727272727273\n","Iteration 47214 - loss value [[279.71569379]] accuracy 0.7272727272727273\n","Iteration 47215 - loss value [[282.07328979]] accuracy 0.7272727272727273\n","Iteration 47216 - loss value [[280.48579661]] accuracy 0.7272727272727273\n","Iteration 47217 - loss value [[281.16561668]] accuracy 0.7272727272727273\n","Iteration 47218 - loss value [[278.78381608]] accuracy 0.7272727272727273\n","Iteration 47219 - loss value [[280.15943809]] accuracy 0.7272727272727273\n","Iteration 47220 - loss value [[276.96516928]] accuracy 0.7272727272727273\n","Iteration 47221 - loss value [[276.34280116]] accuracy 0.7272727272727273\n","Iteration 47222 - loss value [[274.23213377]] accuracy 0.7272727272727273\n","Iteration 47223 - loss value [[275.24444095]] accuracy 0.7272727272727273\n","Iteration 47224 - loss value [[272.43674153]] accuracy 0.7272727272727273\n","Iteration 47225 - loss value [[273.64906714]] accuracy 0.7272727272727273\n","Iteration 47226 - loss value [[270.92630378]] accuracy 0.7272727272727273\n","Iteration 47227 - loss value [[273.15779197]] accuracy 0.7272727272727273\n","Iteration 47228 - loss value [[270.09870834]] accuracy 0.7272727272727273\n","Iteration 47229 - loss value [[270.26759923]] accuracy 0.7272727272727273\n","Iteration 47230 - loss value [[268.59661875]] accuracy 0.7272727272727273\n","Iteration 47231 - loss value [[269.153805]] accuracy 0.7272727272727273\n","Iteration 47232 - loss value [[267.51613873]] accuracy 0.7272727272727273\n","Iteration 47233 - loss value [[267.60736993]] accuracy 0.7272727272727273\n","Iteration 47234 - loss value [[267.18309581]] accuracy 0.7272727272727273\n","Iteration 47235 - loss value [[268.77840181]] accuracy 0.7272727272727273\n","Iteration 47236 - loss value [[268.22008515]] accuracy 0.7272727272727273\n","Iteration 47237 - loss value [[270.71000477]] accuracy 0.7272727272727273\n","Iteration 47238 - loss value [[268.88281474]] accuracy 0.7272727272727273\n","Iteration 47239 - loss value [[271.03123104]] accuracy 0.7272727272727273\n","Iteration 47240 - loss value [[268.35677171]] accuracy 0.7272727272727273\n","Iteration 47241 - loss value [[269.04261454]] accuracy 0.7272727272727273\n","Iteration 47242 - loss value [[268.27295054]] accuracy 0.7272727272727273\n","Iteration 47243 - loss value [[270.89213532]] accuracy 0.7272727272727273\n","Iteration 47244 - loss value [[268.28869792]] accuracy 0.7272727272727273\n","Iteration 47245 - loss value [[269.68035649]] accuracy 0.7272727272727273\n","Iteration 47246 - loss value [[268.78598953]] accuracy 0.7272727272727273\n","Iteration 47247 - loss value [[271.63395288]] accuracy 0.7272727272727273\n","Iteration 47248 - loss value [[269.19874079]] accuracy 0.7272727272727273\n","Iteration 47249 - loss value [[271.24342429]] accuracy 0.7272727272727273\n","Iteration 47250 - loss value [[269.41881228]] accuracy 0.7272727272727273\n","Iteration 47251 - loss value [[272.40779468]] accuracy 0.7272727272727273\n","Iteration 47252 - loss value [[269.79061371]] accuracy 0.7272727272727273\n","Iteration 47253 - loss value [[271.83352626]] accuracy 0.7272727272727273\n","Iteration 47254 - loss value [[269.65587171]] accuracy 0.7272727272727273\n","Iteration 47255 - loss value [[272.69821145]] accuracy 0.7272727272727273\n","Iteration 47256 - loss value [[269.98537179]] accuracy 0.7272727272727273\n","Iteration 47257 - loss value [[271.43860197]] accuracy 0.7272727272727273\n","Iteration 47258 - loss value [[269.09774496]] accuracy 0.7272727272727273\n","Iteration 47259 - loss value [[270.65005958]] accuracy 0.7272727272727273\n","Iteration 47260 - loss value [[269.51883852]] accuracy 0.7272727272727273\n","Iteration 47261 - loss value [[272.69844129]] accuracy 0.7272727272727273\n","Iteration 47262 - loss value [[270.20142121]] accuracy 0.7272727272727273\n","Iteration 47263 - loss value [[272.52435858]] accuracy 0.7272727272727273\n","Iteration 47264 - loss value [[270.03534457]] accuracy 0.7272727272727273\n","Iteration 47265 - loss value [[271.51672246]] accuracy 0.7272727272727273\n","Iteration 47266 - loss value [[269.03068002]] accuracy 0.7272727272727273\n","Iteration 47267 - loss value [[270.19702925]] accuracy 0.7272727272727273\n","Iteration 47268 - loss value [[269.44376442]] accuracy 0.7272727272727273\n","Iteration 47269 - loss value [[272.48610705]] accuracy 0.7272727272727273\n","Iteration 47270 - loss value [[270.03969209]] accuracy 0.7272727272727273\n","Iteration 47271 - loss value [[272.66494196]] accuracy 0.7272727272727273\n","Iteration 47272 - loss value [[270.03808414]] accuracy 0.7272727272727273\n","Iteration 47273 - loss value [[272.13281714]] accuracy 0.7272727272727273\n","Iteration 47274 - loss value [[270.10174659]] accuracy 0.7272727272727273\n","Iteration 47275 - loss value [[273.54286604]] accuracy 0.7272727272727273\n","Iteration 47276 - loss value [[271.04870516]] accuracy 0.7272727272727273\n","Iteration 47277 - loss value [[273.57954034]] accuracy 0.7272727272727273\n","Iteration 47278 - loss value [[271.19259351]] accuracy 0.7272727272727273\n","Iteration 47279 - loss value [[273.26826609]] accuracy 0.7272727272727273\n","Iteration 47280 - loss value [[271.32235276]] accuracy 0.7272727272727273\n","Iteration 47281 - loss value [[274.65011003]] accuracy 0.7272727272727273\n","Iteration 47282 - loss value [[272.81393952]] accuracy 0.7272727272727273\n","Iteration 47283 - loss value [[275.68296693]] accuracy 0.7272727272727273\n","Iteration 47284 - loss value [[274.80608133]] accuracy 0.7272727272727273\n","Iteration 47285 - loss value [[278.01069591]] accuracy 0.7272727272727273\n","Iteration 47286 - loss value [[277.84999254]] accuracy 0.7272727272727273\n","Iteration 47287 - loss value [[281.25299088]] accuracy 0.7272727272727273\n","Iteration 47288 - loss value [[279.55316492]] accuracy 0.7272727272727273\n","Iteration 47289 - loss value [[281.54009285]] accuracy 0.7272727272727273\n","Iteration 47290 - loss value [[279.74275433]] accuracy 0.7272727272727273\n","Iteration 47291 - loss value [[281.46583162]] accuracy 0.7272727272727273\n","Iteration 47292 - loss value [[279.34005105]] accuracy 0.7272727272727273\n","Iteration 47293 - loss value [[280.9569066]] accuracy 0.7272727272727273\n","Iteration 47294 - loss value [[278.53754882]] accuracy 0.7272727272727273\n","Iteration 47295 - loss value [[279.73683715]] accuracy 0.7272727272727273\n","Iteration 47296 - loss value [[277.15719494]] accuracy 0.7272727272727273\n","Iteration 47297 - loss value [[278.17867171]] accuracy 0.7272727272727273\n","Iteration 47298 - loss value [[276.83473151]] accuracy 0.7272727272727273\n","Iteration 47299 - loss value [[279.04608227]] accuracy 0.7272727272727273\n","Iteration 47300 - loss value [[275.85814676]] accuracy 0.7272727272727273\n","Iteration 47301 - loss value [[276.04829884]] accuracy 0.7272727272727273\n","Iteration 47302 - loss value [[274.11732939]] accuracy 0.7272727272727273\n","Iteration 47303 - loss value [[275.57666435]] accuracy 0.7272727272727273\n","Iteration 47304 - loss value [[273.35367395]] accuracy 0.7272727272727273\n","Iteration 47305 - loss value [[273.50941782]] accuracy 0.7272727272727273\n","Iteration 47306 - loss value [[270.68811561]] accuracy 0.7272727272727273\n","Iteration 47307 - loss value [[272.19926661]] accuracy 0.7272727272727273\n","Iteration 47308 - loss value [[268.93148507]] accuracy 0.7272727272727273\n","Iteration 47309 - loss value [[268.74372158]] accuracy 0.7272727272727273\n","Iteration 47310 - loss value [[267.50148165]] accuracy 0.7272727272727273\n","Iteration 47311 - loss value [[268.07333528]] accuracy 0.7272727272727273\n","Iteration 47312 - loss value [[267.6128865]] accuracy 0.7272727272727273\n","Iteration 47313 - loss value [[269.16989416]] accuracy 0.7272727272727273\n","Iteration 47314 - loss value [[268.09395449]] accuracy 0.7272727272727273\n","Iteration 47315 - loss value [[268.18160361]] accuracy 0.7272727272727273\n","Iteration 47316 - loss value [[268.12519829]] accuracy 0.7272727272727273\n","Iteration 47317 - loss value [[270.44986475]] accuracy 0.7272727272727273\n","Iteration 47318 - loss value [[269.55133001]] accuracy 0.7272727272727273\n","Iteration 47319 - loss value [[272.27555924]] accuracy 0.7272727272727273\n","Iteration 47320 - loss value [[270.10361804]] accuracy 0.7272727272727273\n","Iteration 47321 - loss value [[273.24429361]] accuracy 0.7272727272727273\n","Iteration 47322 - loss value [[270.4133275]] accuracy 0.7272727272727273\n","Iteration 47323 - loss value [[272.30682082]] accuracy 0.7272727272727273\n","Iteration 47324 - loss value [[270.27763907]] accuracy 0.7272727272727273\n","Iteration 47325 - loss value [[273.17693836]] accuracy 0.7272727272727273\n","Iteration 47326 - loss value [[270.39110066]] accuracy 0.7272727272727273\n","Iteration 47327 - loss value [[272.30791791]] accuracy 0.7272727272727273\n","Iteration 47328 - loss value [[269.73954009]] accuracy 0.7272727272727273\n","Iteration 47329 - loss value [[272.13071342]] accuracy 0.7272727272727273\n","Iteration 47330 - loss value [[269.78989475]] accuracy 0.7272727272727273\n","Iteration 47331 - loss value [[271.02914829]] accuracy 0.7272727272727273\n","Iteration 47332 - loss value [[269.25540669]] accuracy 0.7272727272727273\n","Iteration 47333 - loss value [[271.56715983]] accuracy 0.7272727272727273\n","Iteration 47334 - loss value [[268.94203658]] accuracy 0.7272727272727273\n","Iteration 47335 - loss value [[269.31501472]] accuracy 0.7272727272727273\n","Iteration 47336 - loss value [[268.07878194]] accuracy 0.7272727272727273\n","Iteration 47337 - loss value [[269.78360158]] accuracy 0.7272727272727273\n","Iteration 47338 - loss value [[268.52513228]] accuracy 0.7272727272727273\n","Iteration 47339 - loss value [[269.66373716]] accuracy 0.7272727272727273\n","Iteration 47340 - loss value [[268.29178254]] accuracy 0.7272727272727273\n","Iteration 47341 - loss value [[270.87320776]] accuracy 0.7272727272727273\n","Iteration 47342 - loss value [[268.50657038]] accuracy 0.7272727272727273\n","Iteration 47343 - loss value [[269.02699438]] accuracy 0.7272727272727273\n","Iteration 47344 - loss value [[268.05300133]] accuracy 0.7272727272727273\n","Iteration 47345 - loss value [[270.26735606]] accuracy 0.7272727272727273\n","Iteration 47346 - loss value [[268.44611136]] accuracy 0.7272727272727273\n","Iteration 47347 - loss value [[271.51374075]] accuracy 0.7272727272727273\n","Iteration 47348 - loss value [[269.36238995]] accuracy 0.7272727272727273\n","Iteration 47349 - loss value [[271.41294345]] accuracy 0.7272727272727273\n","Iteration 47350 - loss value [[268.67880979]] accuracy 0.7272727272727273\n","Iteration 47351 - loss value [[269.26242769]] accuracy 0.7272727272727273\n","Iteration 47352 - loss value [[268.36329136]] accuracy 0.7272727272727273\n","Iteration 47353 - loss value [[271.25505309]] accuracy 0.7272727272727273\n","Iteration 47354 - loss value [[269.1431486]] accuracy 0.7272727272727273\n","Iteration 47355 - loss value [[271.86797978]] accuracy 0.7272727272727273\n","Iteration 47356 - loss value [[269.85833101]] accuracy 0.7272727272727273\n","Iteration 47357 - loss value [[273.35333347]] accuracy 0.7272727272727273\n","Iteration 47358 - loss value [[270.84897451]] accuracy 0.7272727272727273\n","Iteration 47359 - loss value [[272.66300466]] accuracy 0.7272727272727273\n","Iteration 47360 - loss value [[271.11494594]] accuracy 0.7272727272727273\n","Iteration 47361 - loss value [[274.6566717]] accuracy 0.7272727272727273\n","Iteration 47362 - loss value [[272.29280094]] accuracy 0.7272727272727273\n","Iteration 47363 - loss value [[274.25530177]] accuracy 0.7272727272727273\n","Iteration 47364 - loss value [[272.83302701]] accuracy 0.7272727272727273\n","Iteration 47365 - loss value [[275.84482139]] accuracy 0.7272727272727273\n","Iteration 47366 - loss value [[274.46952258]] accuracy 0.7272727272727273\n","Iteration 47367 - loss value [[277.13875156]] accuracy 0.7272727272727273\n","Iteration 47368 - loss value [[276.8398444]] accuracy 0.7272727272727273\n","Iteration 47369 - loss value [[279.7238571]] accuracy 0.7272727272727273\n","Iteration 47370 - loss value [[278.46065717]] accuracy 0.7272727272727273\n","Iteration 47371 - loss value [[280.4843467]] accuracy 0.7272727272727273\n","Iteration 47372 - loss value [[278.07508394]] accuracy 0.7272727272727273\n","Iteration 47373 - loss value [[279.80473517]] accuracy 0.7272727272727273\n","Iteration 47374 - loss value [[278.74964195]] accuracy 0.7272727272727273\n","Iteration 47375 - loss value [[281.5877228]] accuracy 0.7272727272727273\n","Iteration 47376 - loss value [[279.42295428]] accuracy 0.7272727272727273\n","Iteration 47377 - loss value [[281.6898268]] accuracy 0.7272727272727273\n","Iteration 47378 - loss value [[279.1480252]] accuracy 0.7272727272727273\n","Iteration 47379 - loss value [[279.55887052]] accuracy 0.7272727272727273\n","Iteration 47380 - loss value [[278.11602316]] accuracy 0.7272727272727273\n","Iteration 47381 - loss value [[279.95987333]] accuracy 0.7272727272727273\n","Iteration 47382 - loss value [[276.69121333]] accuracy 0.7272727272727273\n","Iteration 47383 - loss value [[275.92989415]] accuracy 0.7272727272727273\n","Iteration 47384 - loss value [[274.21519187]] accuracy 0.7272727272727273\n","Iteration 47385 - loss value [[274.65841838]] accuracy 0.7272727272727273\n","Iteration 47386 - loss value [[271.57818824]] accuracy 0.7272727272727273\n","Iteration 47387 - loss value [[271.64080457]] accuracy 0.7272727272727273\n","Iteration 47388 - loss value [[269.15114352]] accuracy 0.7272727272727273\n","Iteration 47389 - loss value [[269.56135449]] accuracy 0.7272727272727273\n","Iteration 47390 - loss value [[267.60561581]] accuracy 0.7272727272727273\n","Iteration 47391 - loss value [[267.51656155]] accuracy 0.7272727272727273\n","Iteration 47392 - loss value [[266.90427715]] accuracy 0.7272727272727273\n","Iteration 47393 - loss value [[268.69151225]] accuracy 0.7272727272727273\n","Iteration 47394 - loss value [[267.46317333]] accuracy 0.7272727272727273\n","Iteration 47395 - loss value [[267.97545415]] accuracy 0.7272727272727273\n","Iteration 47396 - loss value [[267.71148291]] accuracy 0.7272727272727273\n","Iteration 47397 - loss value [[268.7502481]] accuracy 0.7272727272727273\n","Iteration 47398 - loss value [[268.04933954]] accuracy 0.7272727272727273\n","Iteration 47399 - loss value [[268.96276075]] accuracy 0.7272727272727273\n","Iteration 47400 - loss value [[268.2608158]] accuracy 0.7272727272727273\n","Iteration 47401 - loss value [[270.42874431]] accuracy 0.7272727272727273\n","Iteration 47402 - loss value [[269.27596687]] accuracy 0.7272727272727273\n","Iteration 47403 - loss value [[272.20957494]] accuracy 0.7272727272727273\n","Iteration 47404 - loss value [[269.62310035]] accuracy 0.7272727272727273\n","Iteration 47405 - loss value [[271.14916819]] accuracy 0.7272727272727273\n","Iteration 47406 - loss value [[268.24507915]] accuracy 0.7272727272727273\n","Iteration 47407 - loss value [[268.57265537]] accuracy 0.7272727272727273\n","Iteration 47408 - loss value [[267.61681433]] accuracy 0.7272727272727273\n","Iteration 47409 - loss value [[268.59301657]] accuracy 0.7272727272727273\n","Iteration 47410 - loss value [[267.97146305]] accuracy 0.7272727272727273\n","Iteration 47411 - loss value [[270.59690158]] accuracy 0.7272727272727273\n","Iteration 47412 - loss value [[268.32501015]] accuracy 0.7272727272727273\n","Iteration 47413 - loss value [[269.33310289]] accuracy 0.7272727272727273\n","Iteration 47414 - loss value [[268.77111488]] accuracy 0.7272727272727273\n","Iteration 47415 - loss value [[271.85455267]] accuracy 0.7272727272727273\n","Iteration 47416 - loss value [[269.62620444]] accuracy 0.7272727272727273\n","Iteration 47417 - loss value [[272.3972485]] accuracy 0.7272727272727273\n","Iteration 47418 - loss value [[270.13518132]] accuracy 0.7272727272727273\n","Iteration 47419 - loss value [[272.64804762]] accuracy 0.7272727272727273\n","Iteration 47420 - loss value [[270.49698288]] accuracy 0.7272727272727273\n","Iteration 47421 - loss value [[273.14922518]] accuracy 0.7272727272727273\n","Iteration 47422 - loss value [[270.70639386]] accuracy 0.7272727272727273\n","Iteration 47423 - loss value [[273.35185248]] accuracy 0.7272727272727273\n","Iteration 47424 - loss value [[270.64656991]] accuracy 0.7272727272727273\n","Iteration 47425 - loss value [[272.64021467]] accuracy 0.7272727272727273\n","Iteration 47426 - loss value [[270.84941253]] accuracy 0.7272727272727273\n","Iteration 47427 - loss value [[273.97246906]] accuracy 0.7272727272727273\n","Iteration 47428 - loss value [[271.11318751]] accuracy 0.7272727272727273\n","Iteration 47429 - loss value [[272.30166809]] accuracy 0.7272727272727273\n","Iteration 47430 - loss value [[269.54891212]] accuracy 0.7272727272727273\n","Iteration 47431 - loss value [[270.90685875]] accuracy 0.7272727272727273\n","Iteration 47432 - loss value [[269.36700238]] accuracy 0.7272727272727273\n","Iteration 47433 - loss value [[272.33703889]] accuracy 0.7272727272727273\n","Iteration 47434 - loss value [[269.43066423]] accuracy 0.7272727272727273\n","Iteration 47435 - loss value [[270.48693678]] accuracy 0.7272727272727273\n","Iteration 47436 - loss value [[269.42667033]] accuracy 0.7272727272727273\n","Iteration 47437 - loss value [[272.69689929]] accuracy 0.7272727272727273\n","Iteration 47438 - loss value [[270.02633486]] accuracy 0.7272727272727273\n","Iteration 47439 - loss value [[272.13153077]] accuracy 0.7272727272727273\n","Iteration 47440 - loss value [[270.20496157]] accuracy 0.7272727272727273\n","Iteration 47441 - loss value [[273.56270196]] accuracy 0.7272727272727273\n","Iteration 47442 - loss value [[270.64756796]] accuracy 0.7272727272727273\n","Iteration 47443 - loss value [[272.17293073]] accuracy 0.7272727272727273\n","Iteration 47444 - loss value [[270.11117208]] accuracy 0.7272727272727273\n","Iteration 47445 - loss value [[273.44846608]] accuracy 0.7272727272727273\n","Iteration 47446 - loss value [[270.53992987]] accuracy 0.7272727272727273\n","Iteration 47447 - loss value [[272.01908482]] accuracy 0.7272727272727273\n","Iteration 47448 - loss value [[269.38337885]] accuracy 0.7272727272727273\n","Iteration 47449 - loss value [[270.31428297]] accuracy 0.7272727272727273\n","Iteration 47450 - loss value [[269.09031821]] accuracy 0.7272727272727273\n","Iteration 47451 - loss value [[271.73600466]] accuracy 0.7272727272727273\n","Iteration 47452 - loss value [[269.28064601]] accuracy 0.7272727272727273\n","Iteration 47453 - loss value [[270.1993805]] accuracy 0.7272727272727273\n","Iteration 47454 - loss value [[269.02983768]] accuracy 0.7272727272727273\n","Iteration 47455 - loss value [[271.76343206]] accuracy 0.7272727272727273\n","Iteration 47456 - loss value [[269.7693687]] accuracy 0.7272727272727273\n","Iteration 47457 - loss value [[273.04504782]] accuracy 0.7272727272727273\n","Iteration 47458 - loss value [[270.408516]] accuracy 0.7272727272727273\n","Iteration 47459 - loss value [[272.61007642]] accuracy 0.7272727272727273\n","Iteration 47460 - loss value [[270.59264462]] accuracy 0.7272727272727273\n","Iteration 47461 - loss value [[274.01032284]] accuracy 0.7272727272727273\n","Iteration 47462 - loss value [[271.45175135]] accuracy 0.7272727272727273\n","Iteration 47463 - loss value [[273.63603125]] accuracy 0.7272727272727273\n","Iteration 47464 - loss value [[271.69594994]] accuracy 0.7272727272727273\n","Iteration 47465 - loss value [[274.34727685]] accuracy 0.7272727272727273\n","Iteration 47466 - loss value [[272.15741196]] accuracy 0.7272727272727273\n","Iteration 47467 - loss value [[274.56512534]] accuracy 0.7272727272727273\n","Iteration 47468 - loss value [[272.13293925]] accuracy 0.7272727272727273\n","Iteration 47469 - loss value [[273.47532897]] accuracy 0.7272727272727273\n","Iteration 47470 - loss value [[271.18632924]] accuracy 0.7272727272727273\n","Iteration 47471 - loss value [[273.11270765]] accuracy 0.7272727272727273\n","Iteration 47472 - loss value [[270.77010082]] accuracy 0.7272727272727273\n","Iteration 47473 - loss value [[273.48321707]] accuracy 0.7272727272727273\n","Iteration 47474 - loss value [[270.87892312]] accuracy 0.7272727272727273\n","Iteration 47475 - loss value [[272.16593796]] accuracy 0.7272727272727273\n","Iteration 47476 - loss value [[269.30768568]] accuracy 0.7272727272727273\n","Iteration 47477 - loss value [[269.63713541]] accuracy 0.7272727272727273\n","Iteration 47478 - loss value [[268.5050053]] accuracy 0.7272727272727273\n","Iteration 47479 - loss value [[270.06299579]] accuracy 0.7272727272727273\n","Iteration 47480 - loss value [[269.00524694]] accuracy 0.7272727272727273\n","Iteration 47481 - loss value [[272.1169607]] accuracy 0.7272727272727273\n","Iteration 47482 - loss value [[269.26351984]] accuracy 0.7272727272727273\n","Iteration 47483 - loss value [[269.81285308]] accuracy 0.7272727272727273\n","Iteration 47484 - loss value [[268.89782427]] accuracy 0.7272727272727273\n","Iteration 47485 - loss value [[272.02284015]] accuracy 0.7272727272727273\n","Iteration 47486 - loss value [[269.29182279]] accuracy 0.7272727272727273\n","Iteration 47487 - loss value [[270.27998806]] accuracy 0.7272727272727273\n","Iteration 47488 - loss value [[269.3297005]] accuracy 0.7272727272727273\n","Iteration 47489 - loss value [[272.57477178]] accuracy 0.7272727272727273\n","Iteration 47490 - loss value [[269.82600913]] accuracy 0.7272727272727273\n","Iteration 47491 - loss value [[270.85866984]] accuracy 0.7272727272727273\n","Iteration 47492 - loss value [[269.43058672]] accuracy 0.7272727272727273\n","Iteration 47493 - loss value [[272.36479752]] accuracy 0.7272727272727273\n","Iteration 47494 - loss value [[270.66247637]] accuracy 0.7272727272727273\n","Iteration 47495 - loss value [[274.11273873]] accuracy 0.7272727272727273\n","Iteration 47496 - loss value [[271.9113813]] accuracy 0.7272727272727273\n","Iteration 47497 - loss value [[274.37616693]] accuracy 0.7272727272727273\n","Iteration 47498 - loss value [[272.86196067]] accuracy 0.7272727272727273\n","Iteration 47499 - loss value [[275.73252343]] accuracy 0.7272727272727273\n","Iteration 47500 - loss value [[274.59898123]] accuracy 0.7272727272727273\n","Iteration 47501 - loss value [[278.00125756]] accuracy 0.7272727272727273\n","Iteration 47502 - loss value [[278.43977867]] accuracy 0.7272727272727273\n","Iteration 47503 - loss value [[282.09781853]] accuracy 0.7272727272727273\n","Iteration 47504 - loss value [[279.5890496]] accuracy 0.7272727272727273\n","Iteration 47505 - loss value [[280.41738877]] accuracy 0.7272727272727273\n","Iteration 47506 - loss value [[278.24236701]] accuracy 0.7272727272727273\n","Iteration 47507 - loss value [[281.08410315]] accuracy 0.7272727272727273\n","Iteration 47508 - loss value [[278.19638843]] accuracy 0.7272727272727273\n","Iteration 47509 - loss value [[280.39066487]] accuracy 0.7272727272727273\n","Iteration 47510 - loss value [[277.66110683]] accuracy 0.7272727272727273\n","Iteration 47511 - loss value [[277.37179966]] accuracy 0.7272727272727273\n","Iteration 47512 - loss value [[276.80233442]] accuracy 0.7272727272727273\n","Iteration 47513 - loss value [[278.94827183]] accuracy 0.7272727272727273\n","Iteration 47514 - loss value [[276.94555452]] accuracy 0.7272727272727273\n","Iteration 47515 - loss value [[278.28456758]] accuracy 0.7272727272727273\n","Iteration 47516 - loss value [[276.22856123]] accuracy 0.7272727272727273\n","Iteration 47517 - loss value [[276.59746338]] accuracy 0.7272727272727273\n","Iteration 47518 - loss value [[274.76023981]] accuracy 0.7272727272727273\n","Iteration 47519 - loss value [[275.24230995]] accuracy 0.7272727272727273\n","Iteration 47520 - loss value [[272.50748329]] accuracy 0.7272727272727273\n","Iteration 47521 - loss value [[273.43170172]] accuracy 0.7272727272727273\n","Iteration 47522 - loss value [[270.26972872]] accuracy 0.7272727272727273\n","Iteration 47523 - loss value [[270.59517779]] accuracy 0.7272727272727273\n","Iteration 47524 - loss value [[268.31642748]] accuracy 0.7272727272727273\n","Iteration 47525 - loss value [[268.51529656]] accuracy 0.7272727272727273\n","Iteration 47526 - loss value [[267.5583815]] accuracy 0.7272727272727273\n","Iteration 47527 - loss value [[268.18155003]] accuracy 0.7272727272727273\n","Iteration 47528 - loss value [[267.35420145]] accuracy 0.7272727272727273\n","Iteration 47529 - loss value [[268.24139121]] accuracy 0.7272727272727273\n","Iteration 47530 - loss value [[267.76107635]] accuracy 0.7272727272727273\n","Iteration 47531 - loss value [[268.79674034]] accuracy 0.7272727272727273\n","Iteration 47532 - loss value [[268.23167164]] accuracy 0.7272727272727273\n","Iteration 47533 - loss value [[270.55517691]] accuracy 0.7272727272727273\n","Iteration 47534 - loss value [[268.03589813]] accuracy 0.7272727272727273\n","Iteration 47535 - loss value [[268.61851607]] accuracy 0.7272727272727273\n","Iteration 47536 - loss value [[268.0169728]] accuracy 0.7272727272727273\n","Iteration 47537 - loss value [[270.43183947]] accuracy 0.7272727272727273\n","Iteration 47538 - loss value [[268.38887788]] accuracy 0.7272727272727273\n","Iteration 47539 - loss value [[269.95153665]] accuracy 0.7272727272727273\n","Iteration 47540 - loss value [[268.87924223]] accuracy 0.7272727272727273\n","Iteration 47541 - loss value [[271.82478906]] accuracy 0.7272727272727273\n","Iteration 47542 - loss value [[269.85836235]] accuracy 0.7272727272727273\n","Iteration 47543 - loss value [[273.04472045]] accuracy 0.7272727272727273\n","Iteration 47544 - loss value [[270.2501459]] accuracy 0.7272727272727273\n","Iteration 47545 - loss value [[272.25589584]] accuracy 0.7272727272727273\n","Iteration 47546 - loss value [[270.39533261]] accuracy 0.7272727272727273\n","Iteration 47547 - loss value [[273.57525298]] accuracy 0.7272727272727273\n","Iteration 47548 - loss value [[270.85452474]] accuracy 0.7272727272727273\n","Iteration 47549 - loss value [[272.82041459]] accuracy 0.7272727272727273\n","Iteration 47550 - loss value [[270.68018214]] accuracy 0.7272727272727273\n","Iteration 47551 - loss value [[273.8328945]] accuracy 0.7272727272727273\n","Iteration 47552 - loss value [[270.85487028]] accuracy 0.7272727272727273\n","Iteration 47553 - loss value [[272.11671923]] accuracy 0.7272727272727273\n","Iteration 47554 - loss value [[269.3805295]] accuracy 0.7272727272727273\n","Iteration 47555 - loss value [[270.26327691]] accuracy 0.7272727272727273\n","Iteration 47556 - loss value [[268.65780891]] accuracy 0.7272727272727273\n","Iteration 47557 - loss value [[270.22283845]] accuracy 0.7272727272727273\n","Iteration 47558 - loss value [[268.43173903]] accuracy 0.7272727272727273\n","Iteration 47559 - loss value [[269.50510901]] accuracy 0.7272727272727273\n","Iteration 47560 - loss value [[268.57839105]] accuracy 0.7272727272727273\n","Iteration 47561 - loss value [[271.32735962]] accuracy 0.7272727272727273\n","Iteration 47562 - loss value [[269.10962598]] accuracy 0.7272727272727273\n","Iteration 47563 - loss value [[270.81721911]] accuracy 0.7272727272727273\n","Iteration 47564 - loss value [[269.43911362]] accuracy 0.7272727272727273\n","Iteration 47565 - loss value [[272.52059406]] accuracy 0.7272727272727273\n","Iteration 47566 - loss value [[269.75739321]] accuracy 0.7272727272727273\n","Iteration 47567 - loss value [[271.5050009]] accuracy 0.7272727272727273\n","Iteration 47568 - loss value [[269.16731653]] accuracy 0.7272727272727273\n","Iteration 47569 - loss value [[270.41647869]] accuracy 0.7272727272727273\n","Iteration 47570 - loss value [[269.18259831]] accuracy 0.7272727272727273\n","Iteration 47571 - loss value [[272.44355164]] accuracy 0.7272727272727273\n","Iteration 47572 - loss value [[269.67042239]] accuracy 0.7272727272727273\n","Iteration 47573 - loss value [[271.32956641]] accuracy 0.7272727272727273\n","Iteration 47574 - loss value [[269.05053866]] accuracy 0.7272727272727273\n","Iteration 47575 - loss value [[270.43700105]] accuracy 0.7272727272727273\n","Iteration 47576 - loss value [[268.83221472]] accuracy 0.7272727272727273\n","Iteration 47577 - loss value [[271.76067115]] accuracy 0.7272727272727273\n","Iteration 47578 - loss value [[269.17986974]] accuracy 0.7272727272727273\n","Iteration 47579 - loss value [[270.15047067]] accuracy 0.7272727272727273\n","Iteration 47580 - loss value [[268.73033446]] accuracy 0.7272727272727273\n","Iteration 47581 - loss value [[271.25142945]] accuracy 0.7272727272727273\n","Iteration 47582 - loss value [[269.40941163]] accuracy 0.7272727272727273\n","Iteration 47583 - loss value [[272.29058706]] accuracy 0.7272727272727273\n","Iteration 47584 - loss value [[270.21206278]] accuracy 0.7272727272727273\n","Iteration 47585 - loss value [[273.74447107]] accuracy 0.7272727272727273\n","Iteration 47586 - loss value [[270.9394879]] accuracy 0.7272727272727273\n","Iteration 47587 - loss value [[272.50046144]] accuracy 0.7272727272727273\n","Iteration 47588 - loss value [[270.62620162]] accuracy 0.7272727272727273\n","Iteration 47589 - loss value [[274.20137584]] accuracy 0.7272727272727273\n","Iteration 47590 - loss value [[271.95349163]] accuracy 0.7272727272727273\n","Iteration 47591 - loss value [[274.31712133]] accuracy 0.7272727272727273\n","Iteration 47592 - loss value [[272.39570559]] accuracy 0.7272727272727273\n","Iteration 47593 - loss value [[275.05659521]] accuracy 0.7272727272727273\n","Iteration 47594 - loss value [[273.91362992]] accuracy 0.7272727272727273\n","Iteration 47595 - loss value [[276.5335931]] accuracy 0.7272727272727273\n","Iteration 47596 - loss value [[276.08896936]] accuracy 0.7272727272727273\n","Iteration 47597 - loss value [[278.82239524]] accuracy 0.7272727272727273\n","Iteration 47598 - loss value [[277.19334897]] accuracy 0.7272727272727273\n","Iteration 47599 - loss value [[279.22507129]] accuracy 0.7272727272727273\n","Iteration 47600 - loss value [[277.62187567]] accuracy 0.7272727272727273\n","Iteration 47601 - loss value [[279.7044247]] accuracy 0.7272727272727273\n","Iteration 47602 - loss value [[277.22671498]] accuracy 0.7272727272727273\n","Iteration 47603 - loss value [[277.78778177]] accuracy 0.7272727272727273\n","Iteration 47604 - loss value [[277.35421213]] accuracy 0.7272727272727273\n","Iteration 47605 - loss value [[279.84737892]] accuracy 0.7272727272727273\n","Iteration 47606 - loss value [[277.0570448]] accuracy 0.7272727272727273\n","Iteration 47607 - loss value [[276.56917502]] accuracy 0.7272727272727273\n","Iteration 47608 - loss value [[275.16440106]] accuracy 0.7272727272727273\n","Iteration 47609 - loss value [[277.06843433]] accuracy 0.7272727272727273\n","Iteration 47610 - loss value [[275.41568097]] accuracy 0.7272727272727273\n","Iteration 47611 - loss value [[276.48804237]] accuracy 0.7272727272727273\n","Iteration 47612 - loss value [[275.15562932]] accuracy 0.7272727272727273\n","Iteration 47613 - loss value [[276.23387122]] accuracy 0.7272727272727273\n","Iteration 47614 - loss value [[274.20677376]] accuracy 0.7272727272727273\n","Iteration 47615 - loss value [[275.03272572]] accuracy 0.7272727272727273\n","Iteration 47616 - loss value [[272.74782637]] accuracy 0.7272727272727273\n","Iteration 47617 - loss value [[274.32216362]] accuracy 0.7272727272727273\n","Iteration 47618 - loss value [[271.11688538]] accuracy 0.7272727272727273\n","Iteration 47619 - loss value [[271.58013483]] accuracy 0.7272727272727273\n","Iteration 47620 - loss value [[269.38260692]] accuracy 0.7272727272727273\n","Iteration 47621 - loss value [[269.45033399]] accuracy 0.7272727272727273\n","Iteration 47622 - loss value [[267.85303371]] accuracy 0.7272727272727273\n","Iteration 47623 - loss value [[268.54568433]] accuracy 0.7272727272727273\n","Iteration 47624 - loss value [[267.92413154]] accuracy 0.7272727272727273\n","Iteration 47625 - loss value [[268.91597436]] accuracy 0.7272727272727273\n","Iteration 47626 - loss value [[268.02618714]] accuracy 0.7272727272727273\n","Iteration 47627 - loss value [[269.43820365]] accuracy 0.7272727272727273\n","Iteration 47628 - loss value [[268.31011097]] accuracy 0.7272727272727273\n","Iteration 47629 - loss value [[269.16887585]] accuracy 0.7272727272727273\n","Iteration 47630 - loss value [[268.52662578]] accuracy 0.7272727272727273\n","Iteration 47631 - loss value [[271.22472574]] accuracy 0.7272727272727273\n","Iteration 47632 - loss value [[268.8313389]] accuracy 0.7272727272727273\n","Iteration 47633 - loss value [[270.16877266]] accuracy 0.7272727272727273\n","Iteration 47634 - loss value [[269.03015279]] accuracy 0.7272727272727273\n","Iteration 47635 - loss value [[272.2198715]] accuracy 0.7272727272727273\n","Iteration 47636 - loss value [[269.65326912]] accuracy 0.7272727272727273\n","Iteration 47637 - loss value [[271.2809071]] accuracy 0.7272727272727273\n","Iteration 47638 - loss value [[268.62891289]] accuracy 0.7272727272727273\n","Iteration 47639 - loss value [[269.75063114]] accuracy 0.7272727272727273\n","Iteration 47640 - loss value [[268.66742648]] accuracy 0.7272727272727273\n","Iteration 47641 - loss value [[271.0321827]] accuracy 0.7272727272727273\n","Iteration 47642 - loss value [[268.72063943]] accuracy 0.7272727272727273\n","Iteration 47643 - loss value [[270.25981295]] accuracy 0.7272727272727273\n","Iteration 47644 - loss value [[269.40334531]] accuracy 0.7272727272727273\n","Iteration 47645 - loss value [[272.80977917]] accuracy 0.7272727272727273\n","Iteration 47646 - loss value [[270.25274118]] accuracy 0.7272727272727273\n","Iteration 47647 - loss value [[272.40517914]] accuracy 0.7272727272727273\n","Iteration 47648 - loss value [[270.61986209]] accuracy 0.7272727272727273\n","Iteration 47649 - loss value [[273.9231902]] accuracy 0.7272727272727273\n","Iteration 47650 - loss value [[271.32230918]] accuracy 0.7272727272727273\n","Iteration 47651 - loss value [[273.42298951]] accuracy 0.7272727272727273\n","Iteration 47652 - loss value [[271.58261128]] accuracy 0.7272727272727273\n","Iteration 47653 - loss value [[273.75295961]] accuracy 0.7272727272727273\n","Iteration 47654 - loss value [[271.13210598]] accuracy 0.7272727272727273\n","Iteration 47655 - loss value [[272.21907439]] accuracy 0.7272727272727273\n","Iteration 47656 - loss value [[270.27225857]] accuracy 0.7272727272727273\n","Iteration 47657 - loss value [[273.3541456]] accuracy 0.7272727272727273\n","Iteration 47658 - loss value [[270.40573456]] accuracy 0.7272727272727273\n","Iteration 47659 - loss value [[271.90101695]] accuracy 0.7272727272727273\n","Iteration 47660 - loss value [[269.12985597]] accuracy 0.7272727272727273\n","Iteration 47661 - loss value [[269.52826051]] accuracy 0.7272727272727273\n","Iteration 47662 - loss value [[268.22501003]] accuracy 0.7272727272727273\n","Iteration 47663 - loss value [[269.77979538]] accuracy 0.7272727272727273\n","Iteration 47664 - loss value [[268.34690836]] accuracy 0.7272727272727273\n","Iteration 47665 - loss value [[269.81123007]] accuracy 0.7272727272727273\n","Iteration 47666 - loss value [[268.58374771]] accuracy 0.7272727272727273\n","Iteration 47667 - loss value [[271.35993411]] accuracy 0.7272727272727273\n","Iteration 47668 - loss value [[269.29256677]] accuracy 0.7272727272727273\n","Iteration 47669 - loss value [[272.47501212]] accuracy 0.7272727272727273\n","Iteration 47670 - loss value [[269.85552822]] accuracy 0.7272727272727273\n","Iteration 47671 - loss value [[271.64124063]] accuracy 0.7272727272727273\n","Iteration 47672 - loss value [[269.36131782]] accuracy 0.7272727272727273\n","Iteration 47673 - loss value [[271.81941281]] accuracy 0.7272727272727273\n","Iteration 47674 - loss value [[269.81577503]] accuracy 0.7272727272727273\n","Iteration 47675 - loss value [[273.30629668]] accuracy 0.7272727272727273\n","Iteration 47676 - loss value [[270.43182616]] accuracy 0.7272727272727273\n","Iteration 47677 - loss value [[272.02257245]] accuracy 0.7272727272727273\n","Iteration 47678 - loss value [[269.87563652]] accuracy 0.7272727272727273\n","Iteration 47679 - loss value [[273.30416009]] accuracy 0.7272727272727273\n","Iteration 47680 - loss value [[270.67346316]] accuracy 0.7272727272727273\n","Iteration 47681 - loss value [[272.8126699]] accuracy 0.7272727272727273\n","Iteration 47682 - loss value [[270.70935879]] accuracy 0.7272727272727273\n","Iteration 47683 - loss value [[273.92479656]] accuracy 0.7272727272727273\n","Iteration 47684 - loss value [[271.24152589]] accuracy 0.7272727272727273\n","Iteration 47685 - loss value [[272.48600162]] accuracy 0.7272727272727273\n","Iteration 47686 - loss value [[270.44064308]] accuracy 0.7272727272727273\n","Iteration 47687 - loss value [[273.29605323]] accuracy 0.7272727272727273\n","Iteration 47688 - loss value [[270.5312515]] accuracy 0.7272727272727273\n","Iteration 47689 - loss value [[272.02000447]] accuracy 0.7272727272727273\n","Iteration 47690 - loss value [[269.99646047]] accuracy 0.7272727272727273\n","Iteration 47691 - loss value [[273.44294788]] accuracy 0.7272727272727273\n","Iteration 47692 - loss value [[270.61406498]] accuracy 0.7272727272727273\n","Iteration 47693 - loss value [[272.19041813]] accuracy 0.7272727272727273\n","Iteration 47694 - loss value [[269.61492165]] accuracy 0.7272727272727273\n","Iteration 47695 - loss value [[271.33155471]] accuracy 0.7272727272727273\n","Iteration 47696 - loss value [[268.46578236]] accuracy 0.7272727272727273\n","Iteration 47697 - loss value [[268.54724727]] accuracy 0.7272727272727273\n","Iteration 47698 - loss value [[267.70284334]] accuracy 0.7272727272727273\n","Iteration 47699 - loss value [[269.74809768]] accuracy 0.7272727272727273\n","Iteration 47700 - loss value [[268.40514817]] accuracy 0.7272727272727273\n","Iteration 47701 - loss value [[270.98214924]] accuracy 0.7272727272727273\n","Iteration 47702 - loss value [[269.05930868]] accuracy 0.7272727272727273\n","Iteration 47703 - loss value [[271.20718885]] accuracy 0.7272727272727273\n","Iteration 47704 - loss value [[270.10827587]] accuracy 0.7272727272727273\n","Iteration 47705 - loss value [[273.58686107]] accuracy 0.7272727272727273\n","Iteration 47706 - loss value [[271.18859805]] accuracy 0.7272727272727273\n","Iteration 47707 - loss value [[273.58006539]] accuracy 0.7272727272727273\n","Iteration 47708 - loss value [[271.99548426]] accuracy 0.7272727272727273\n","Iteration 47709 - loss value [[274.63017517]] accuracy 0.7272727272727273\n","Iteration 47710 - loss value [[272.31743961]] accuracy 0.7272727272727273\n","Iteration 47711 - loss value [[274.79628979]] accuracy 0.7272727272727273\n","Iteration 47712 - loss value [[272.93851131]] accuracy 0.7272727272727273\n","Iteration 47713 - loss value [[275.20039804]] accuracy 0.7272727272727273\n","Iteration 47714 - loss value [[273.98660956]] accuracy 0.7272727272727273\n","Iteration 47715 - loss value [[276.60348513]] accuracy 0.7272727272727273\n","Iteration 47716 - loss value [[276.16590284]] accuracy 0.7272727272727273\n","Iteration 47717 - loss value [[279.10077496]] accuracy 0.7272727272727273\n","Iteration 47718 - loss value [[277.51749991]] accuracy 0.7272727272727273\n","Iteration 47719 - loss value [[279.94513574]] accuracy 0.7272727272727273\n","Iteration 47720 - loss value [[277.91745084]] accuracy 0.7272727272727273\n","Iteration 47721 - loss value [[280.21396644]] accuracy 0.7272727272727273\n","Iteration 47722 - loss value [[277.72808362]] accuracy 0.7272727272727273\n","Iteration 47723 - loss value [[278.52633544]] accuracy 0.7272727272727273\n","Iteration 47724 - loss value [[278.31836693]] accuracy 0.7272727272727273\n","Iteration 47725 - loss value [[280.8549812]] accuracy 0.7272727272727273\n","Iteration 47726 - loss value [[277.8744589]] accuracy 0.7272727272727273\n","Iteration 47727 - loss value [[277.97476296]] accuracy 0.7272727272727273\n","Iteration 47728 - loss value [[276.65728841]] accuracy 0.7272727272727273\n","Iteration 47729 - loss value [[278.13951263]] accuracy 0.7272727272727273\n","Iteration 47730 - loss value [[276.86211627]] accuracy 0.7272727272727273\n","Iteration 47731 - loss value [[278.60254768]] accuracy 0.7272727272727273\n","Iteration 47732 - loss value [[276.94639715]] accuracy 0.7272727272727273\n","Iteration 47733 - loss value [[277.59898128]] accuracy 0.7272727272727273\n","Iteration 47734 - loss value [[276.58729977]] accuracy 0.7272727272727273\n","Iteration 47735 - loss value [[278.16378738]] accuracy 0.7272727272727273\n","Iteration 47736 - loss value [[276.39911035]] accuracy 0.7272727272727273\n","Iteration 47737 - loss value [[278.00354037]] accuracy 0.7272727272727273\n","Iteration 47738 - loss value [[276.07316925]] accuracy 0.7272727272727273\n","Iteration 47739 - loss value [[276.39670286]] accuracy 0.7272727272727273\n","Iteration 47740 - loss value [[274.03928007]] accuracy 0.7272727272727273\n","Iteration 47741 - loss value [[274.19602809]] accuracy 0.7272727272727273\n","Iteration 47742 - loss value [[271.00638114]] accuracy 0.7272727272727273\n","Iteration 47743 - loss value [[271.57504313]] accuracy 0.7272727272727273\n","Iteration 47744 - loss value [[269.19431258]] accuracy 0.7272727272727273\n","Iteration 47745 - loss value [[269.60703076]] accuracy 0.7272727272727273\n","Iteration 47746 - loss value [[267.64286715]] accuracy 0.7272727272727273\n","Iteration 47747 - loss value [[267.6991512]] accuracy 0.7272727272727273\n","Iteration 47748 - loss value [[267.09914322]] accuracy 0.7272727272727273\n","Iteration 47749 - loss value [[268.62711432]] accuracy 0.7272727272727273\n","Iteration 47750 - loss value [[267.92160189]] accuracy 0.7272727272727273\n","Iteration 47751 - loss value [[268.61670437]] accuracy 0.7272727272727273\n","Iteration 47752 - loss value [[267.81053947]] accuracy 0.7272727272727273\n","Iteration 47753 - loss value [[268.46935982]] accuracy 0.7272727272727273\n","Iteration 47754 - loss value [[267.95766852]] accuracy 0.7272727272727273\n","Iteration 47755 - loss value [[269.78844044]] accuracy 0.7272727272727273\n","Iteration 47756 - loss value [[268.39013735]] accuracy 0.7272727272727273\n","Iteration 47757 - loss value [[269.58519399]] accuracy 0.7272727272727273\n","Iteration 47758 - loss value [[268.50634711]] accuracy 0.7272727272727273\n","Iteration 47759 - loss value [[271.25756668]] accuracy 0.7272727272727273\n","Iteration 47760 - loss value [[268.9071837]] accuracy 0.7272727272727273\n","Iteration 47761 - loss value [[270.98315578]] accuracy 0.7272727272727273\n","Iteration 47762 - loss value [[269.44148024]] accuracy 0.7272727272727273\n","Iteration 47763 - loss value [[272.39309993]] accuracy 0.7272727272727273\n","Iteration 47764 - loss value [[269.63176432]] accuracy 0.7272727272727273\n","Iteration 47765 - loss value [[271.6823455]] accuracy 0.7272727272727273\n","Iteration 47766 - loss value [[269.0535336]] accuracy 0.7272727272727273\n","Iteration 47767 - loss value [[269.82790024]] accuracy 0.7272727272727273\n","Iteration 47768 - loss value [[268.41124979]] accuracy 0.7272727272727273\n","Iteration 47769 - loss value [[270.46687009]] accuracy 0.7272727272727273\n","Iteration 47770 - loss value [[268.26558149]] accuracy 0.7272727272727273\n","Iteration 47771 - loss value [[269.6799166]] accuracy 0.7272727272727273\n","Iteration 47772 - loss value [[268.48316335]] accuracy 0.7272727272727273\n","Iteration 47773 - loss value [[271.3075956]] accuracy 0.7272727272727273\n","Iteration 47774 - loss value [[268.86431582]] accuracy 0.7272727272727273\n","Iteration 47775 - loss value [[269.73939153]] accuracy 0.7272727272727273\n","Iteration 47776 - loss value [[268.39693023]] accuracy 0.7272727272727273\n","Iteration 47777 - loss value [[271.33906372]] accuracy 0.7272727272727273\n","Iteration 47778 - loss value [[269.10309634]] accuracy 0.7272727272727273\n","Iteration 47779 - loss value [[271.6764048]] accuracy 0.7272727272727273\n","Iteration 47780 - loss value [[269.57764411]] accuracy 0.7272727272727273\n","Iteration 47781 - loss value [[272.9351741]] accuracy 0.7272727272727273\n","Iteration 47782 - loss value [[270.13668825]] accuracy 0.7272727272727273\n","Iteration 47783 - loss value [[271.60360921]] accuracy 0.7272727272727273\n","Iteration 47784 - loss value [[269.20856298]] accuracy 0.7272727272727273\n","Iteration 47785 - loss value [[271.19958331]] accuracy 0.7272727272727273\n","Iteration 47786 - loss value [[268.45215483]] accuracy 0.7272727272727273\n","Iteration 47787 - loss value [[269.00346029]] accuracy 0.7272727272727273\n","Iteration 47788 - loss value [[267.892938]] accuracy 0.7272727272727273\n","Iteration 47789 - loss value [[269.8030112]] accuracy 0.7272727272727273\n","Iteration 47790 - loss value [[268.39171657]] accuracy 0.7272727272727273\n","Iteration 47791 - loss value [[270.12331201]] accuracy 0.7272727272727273\n","Iteration 47792 - loss value [[269.20980991]] accuracy 0.7272727272727273\n","Iteration 47793 - loss value [[272.50146728]] accuracy 0.7272727272727273\n","Iteration 47794 - loss value [[269.96514408]] accuracy 0.7272727272727273\n","Iteration 47795 - loss value [[272.69812674]] accuracy 0.7272727272727273\n","Iteration 47796 - loss value [[270.18192237]] accuracy 0.7272727272727273\n","Iteration 47797 - loss value [[273.25187861]] accuracy 0.7272727272727273\n","Iteration 47798 - loss value [[270.66389516]] accuracy 0.7272727272727273\n","Iteration 47799 - loss value [[272.78144792]] accuracy 0.7272727272727273\n","Iteration 47800 - loss value [[270.71349213]] accuracy 0.7272727272727273\n","Iteration 47801 - loss value [[274.15207591]] accuracy 0.7272727272727273\n","Iteration 47802 - loss value [[271.50710034]] accuracy 0.7272727272727273\n","Iteration 47803 - loss value [[272.98068972]] accuracy 0.7272727272727273\n","Iteration 47804 - loss value [[271.02425167]] accuracy 0.7272727272727273\n","Iteration 47805 - loss value [[273.41577472]] accuracy 0.7272727272727273\n","Iteration 47806 - loss value [[270.83072137]] accuracy 0.7272727272727273\n","Iteration 47807 - loss value [[273.0761732]] accuracy 0.7272727272727273\n","Iteration 47808 - loss value [[270.50464634]] accuracy 0.7272727272727273\n","Iteration 47809 - loss value [[273.17874753]] accuracy 0.7272727272727273\n","Iteration 47810 - loss value [[270.26223753]] accuracy 0.7272727272727273\n","Iteration 47811 - loss value [[271.56025863]] accuracy 0.7272727272727273\n","Iteration 47812 - loss value [[268.81335034]] accuracy 0.7272727272727273\n","Iteration 47813 - loss value [[269.46803869]] accuracy 0.7272727272727273\n","Iteration 47814 - loss value [[268.56984805]] accuracy 0.7272727272727273\n","Iteration 47815 - loss value [[271.43165307]] accuracy 0.7272727272727273\n","Iteration 47816 - loss value [[268.75959893]] accuracy 0.7272727272727273\n","Iteration 47817 - loss value [[269.71446832]] accuracy 0.7272727272727273\n","Iteration 47818 - loss value [[268.75050343]] accuracy 0.7272727272727273\n","Iteration 47819 - loss value [[271.32718986]] accuracy 0.7272727272727273\n","Iteration 47820 - loss value [[269.47265673]] accuracy 0.7272727272727273\n","Iteration 47821 - loss value [[273.00153056]] accuracy 0.7272727272727273\n","Iteration 47822 - loss value [[270.37425432]] accuracy 0.7272727272727273\n","Iteration 47823 - loss value [[272.56529888]] accuracy 0.7272727272727273\n","Iteration 47824 - loss value [[270.55746051]] accuracy 0.7272727272727273\n","Iteration 47825 - loss value [[273.83509864]] accuracy 0.7272727272727273\n","Iteration 47826 - loss value [[270.98167673]] accuracy 0.7272727272727273\n","Iteration 47827 - loss value [[272.31996357]] accuracy 0.7272727272727273\n","Iteration 47828 - loss value [[270.32910399]] accuracy 0.7272727272727273\n","Iteration 47829 - loss value [[273.37042313]] accuracy 0.7272727272727273\n","Iteration 47830 - loss value [[271.08913527]] accuracy 0.7272727272727273\n","Iteration 47831 - loss value [[272.74538414]] accuracy 0.7272727272727273\n","Iteration 47832 - loss value [[270.82337164]] accuracy 0.7272727272727273\n","Iteration 47833 - loss value [[273.81471298]] accuracy 0.7272727272727273\n","Iteration 47834 - loss value [[271.23060157]] accuracy 0.7272727272727273\n","Iteration 47835 - loss value [[272.28192338]] accuracy 0.7272727272727273\n","Iteration 47836 - loss value [[269.46692346]] accuracy 0.7272727272727273\n","Iteration 47837 - loss value [[270.82749164]] accuracy 0.7272727272727273\n","Iteration 47838 - loss value [[269.30621373]] accuracy 0.7272727272727273\n","Iteration 47839 - loss value [[271.95024347]] accuracy 0.7272727272727273\n","Iteration 47840 - loss value [[269.81785661]] accuracy 0.7272727272727273\n","Iteration 47841 - loss value [[272.57615022]] accuracy 0.7272727272727273\n","Iteration 47842 - loss value [[270.03168661]] accuracy 0.7272727272727273\n","Iteration 47843 - loss value [[271.43173393]] accuracy 0.7272727272727273\n","Iteration 47844 - loss value [[268.98909281]] accuracy 0.7272727272727273\n","Iteration 47845 - loss value [[269.64396148]] accuracy 0.7272727272727273\n","Iteration 47846 - loss value [[268.32672784]] accuracy 0.7272727272727273\n","Iteration 47847 - loss value [[270.32633118]] accuracy 0.7272727272727273\n","Iteration 47848 - loss value [[268.03559482]] accuracy 0.7272727272727273\n","Iteration 47849 - loss value [[269.47236057]] accuracy 0.7272727272727273\n","Iteration 47850 - loss value [[268.07300028]] accuracy 0.7272727272727273\n","Iteration 47851 - loss value [[269.9766216]] accuracy 0.7272727272727273\n","Iteration 47852 - loss value [[268.78045119]] accuracy 0.7272727272727273\n","Iteration 47853 - loss value [[270.53452849]] accuracy 0.7272727272727273\n","Iteration 47854 - loss value [[269.22388523]] accuracy 0.7272727272727273\n","Iteration 47855 - loss value [[272.61013838]] accuracy 0.7272727272727273\n","Iteration 47856 - loss value [[270.16779746]] accuracy 0.7272727272727273\n","Iteration 47857 - loss value [[272.69375386]] accuracy 0.7272727272727273\n","Iteration 47858 - loss value [[270.79387353]] accuracy 0.7272727272727273\n","Iteration 47859 - loss value [[273.99683091]] accuracy 0.7272727272727273\n","Iteration 47860 - loss value [[271.4550455]] accuracy 0.7272727272727273\n","Iteration 47861 - loss value [[272.93488457]] accuracy 0.7272727272727273\n","Iteration 47862 - loss value [[271.3787829]] accuracy 0.7272727272727273\n","Iteration 47863 - loss value [[273.90582727]] accuracy 0.7272727272727273\n","Iteration 47864 - loss value [[271.66224264]] accuracy 0.7272727272727273\n","Iteration 47865 - loss value [[274.30702933]] accuracy 0.7272727272727273\n","Iteration 47866 - loss value [[272.19490535]] accuracy 0.7272727272727273\n","Iteration 47867 - loss value [[274.12311833]] accuracy 0.7272727272727273\n","Iteration 47868 - loss value [[272.95684489]] accuracy 0.7272727272727273\n","Iteration 47869 - loss value [[276.09206612]] accuracy 0.7272727272727273\n","Iteration 47870 - loss value [[275.0792523]] accuracy 0.7272727272727273\n","Iteration 47871 - loss value [[278.61965684]] accuracy 0.7272727272727273\n","Iteration 47872 - loss value [[278.33632752]] accuracy 0.7272727272727273\n","Iteration 47873 - loss value [[281.71927309]] accuracy 0.7272727272727273\n","Iteration 47874 - loss value [[278.88085356]] accuracy 0.7272727272727273\n","Iteration 47875 - loss value [[279.29917818]] accuracy 0.7272727272727273\n","Iteration 47876 - loss value [[279.09364119]] accuracy 0.7272727272727273\n","Iteration 47877 - loss value [[282.30155703]] accuracy 0.7272727272727273\n","Iteration 47878 - loss value [[279.05274674]] accuracy 0.7272727272727273\n","Iteration 47879 - loss value [[279.04364842]] accuracy 0.7272727272727273\n","Iteration 47880 - loss value [[278.10147045]] accuracy 0.7272727272727273\n","Iteration 47881 - loss value [[280.13559189]] accuracy 0.7272727272727273\n","Iteration 47882 - loss value [[277.22476839]] accuracy 0.7272727272727273\n","Iteration 47883 - loss value [[277.18213424]] accuracy 0.7272727272727273\n","Iteration 47884 - loss value [[275.6615774]] accuracy 0.7272727272727273\n","Iteration 47885 - loss value [[276.69348708]] accuracy 0.7272727272727273\n","Iteration 47886 - loss value [[274.96690451]] accuracy 0.7272727272727273\n","Iteration 47887 - loss value [[276.5617021]] accuracy 0.7272727272727273\n","Iteration 47888 - loss value [[275.16674744]] accuracy 0.7272727272727273\n","Iteration 47889 - loss value [[275.92581896]] accuracy 0.7272727272727273\n","Iteration 47890 - loss value [[274.24393991]] accuracy 0.7272727272727273\n","Iteration 47891 - loss value [[275.82150944]] accuracy 0.7272727272727273\n","Iteration 47892 - loss value [[273.84320381]] accuracy 0.7272727272727273\n","Iteration 47893 - loss value [[274.77607713]] accuracy 0.7272727272727273\n","Iteration 47894 - loss value [[272.3880734]] accuracy 0.7272727272727273\n","Iteration 47895 - loss value [[273.75091931]] accuracy 0.7272727272727273\n","Iteration 47896 - loss value [[270.55172714]] accuracy 0.7272727272727273\n","Iteration 47897 - loss value [[270.43859809]] accuracy 0.7272727272727273\n","Iteration 47898 - loss value [[268.38059266]] accuracy 0.7272727272727273\n","Iteration 47899 - loss value [[268.48997547]] accuracy 0.7272727272727273\n","Iteration 47900 - loss value [[268.07593337]] accuracy 0.7272727272727273\n","Iteration 47901 - loss value [[269.66123954]] accuracy 0.7272727272727273\n","Iteration 47902 - loss value [[267.7581333]] accuracy 0.7272727272727273\n","Iteration 47903 - loss value [[268.15193427]] accuracy 0.7272727272727273\n","Iteration 47904 - loss value [[267.7551549]] accuracy 0.7272727272727273\n","Iteration 47905 - loss value [[269.25730292]] accuracy 0.7272727272727273\n","Iteration 47906 - loss value [[267.68422651]] accuracy 0.7272727272727273\n","Iteration 47907 - loss value [[268.00424613]] accuracy 0.7272727272727273\n","Iteration 47908 - loss value [[267.69920309]] accuracy 0.7272727272727273\n","Iteration 47909 - loss value [[270.6692327]] accuracy 0.7272727272727273\n","Iteration 47910 - loss value [[268.22768686]] accuracy 0.7272727272727273\n","Iteration 47911 - loss value [[269.18275307]] accuracy 0.7272727272727273\n","Iteration 47912 - loss value [[268.63937049]] accuracy 0.7272727272727273\n","Iteration 47913 - loss value [[271.51102464]] accuracy 0.7272727272727273\n","Iteration 47914 - loss value [[269.1163149]] accuracy 0.7272727272727273\n","Iteration 47915 - loss value [[271.08102089]] accuracy 0.7272727272727273\n","Iteration 47916 - loss value [[268.78747949]] accuracy 0.7272727272727273\n","Iteration 47917 - loss value [[269.90395376]] accuracy 0.7272727272727273\n","Iteration 47918 - loss value [[269.11029203]] accuracy 0.7272727272727273\n","Iteration 47919 - loss value [[272.23112909]] accuracy 0.7272727272727273\n","Iteration 47920 - loss value [[269.82776354]] accuracy 0.7272727272727273\n","Iteration 47921 - loss value [[271.92302712]] accuracy 0.7272727272727273\n","Iteration 47922 - loss value [[270.09331253]] accuracy 0.7272727272727273\n","Iteration 47923 - loss value [[273.37566286]] accuracy 0.7272727272727273\n","Iteration 47924 - loss value [[270.69362779]] accuracy 0.7272727272727273\n","Iteration 47925 - loss value [[271.8832756]] accuracy 0.7272727272727273\n","Iteration 47926 - loss value [[269.4854365]] accuracy 0.7272727272727273\n","Iteration 47927 - loss value [[271.4818418]] accuracy 0.7272727272727273\n","Iteration 47928 - loss value [[268.73957583]] accuracy 0.7272727272727273\n","Iteration 47929 - loss value [[268.94401027]] accuracy 0.7272727272727273\n","Iteration 47930 - loss value [[267.77651801]] accuracy 0.7272727272727273\n","Iteration 47931 - loss value [[269.2378907]] accuracy 0.7272727272727273\n","Iteration 47932 - loss value [[268.06270367]] accuracy 0.7272727272727273\n","Iteration 47933 - loss value [[270.72701533]] accuracy 0.7272727272727273\n","Iteration 47934 - loss value [[268.30964556]] accuracy 0.7272727272727273\n","Iteration 47935 - loss value [[269.92292302]] accuracy 0.7272727272727273\n","Iteration 47936 - loss value [[268.98159034]] accuracy 0.7272727272727273\n","Iteration 47937 - loss value [[272.01894524]] accuracy 0.7272727272727273\n","Iteration 47938 - loss value [[269.6882361]] accuracy 0.7272727272727273\n","Iteration 47939 - loss value [[271.46899309]] accuracy 0.7272727272727273\n","Iteration 47940 - loss value [[269.21820654]] accuracy 0.7272727272727273\n","Iteration 47941 - loss value [[272.10653577]] accuracy 0.7272727272727273\n","Iteration 47942 - loss value [[269.82049751]] accuracy 0.7272727272727273\n","Iteration 47943 - loss value [[273.56334887]] accuracy 0.7272727272727273\n","Iteration 47944 - loss value [[270.74050983]] accuracy 0.7272727272727273\n","Iteration 47945 - loss value [[272.24240818]] accuracy 0.7272727272727273\n","Iteration 47946 - loss value [[270.42577837]] accuracy 0.7272727272727273\n","Iteration 47947 - loss value [[273.64673242]] accuracy 0.7272727272727273\n","Iteration 47948 - loss value [[271.15951714]] accuracy 0.7272727272727273\n","Iteration 47949 - loss value [[274.2001949]] accuracy 0.7272727272727273\n","Iteration 47950 - loss value [[271.39781897]] accuracy 0.7272727272727273\n","Iteration 47951 - loss value [[272.61230389]] accuracy 0.7272727272727273\n","Iteration 47952 - loss value [[270.49160793]] accuracy 0.7272727272727273\n","Iteration 47953 - loss value [[273.39183841]] accuracy 0.7272727272727273\n","Iteration 47954 - loss value [[270.64410698]] accuracy 0.7272727272727273\n","Iteration 47955 - loss value [[271.7351228]] accuracy 0.7272727272727273\n","Iteration 47956 - loss value [[269.01245384]] accuracy 0.7272727272727273\n","Iteration 47957 - loss value [[270.27197299]] accuracy 0.7272727272727273\n","Iteration 47958 - loss value [[269.09078274]] accuracy 0.7272727272727273\n","Iteration 47959 - loss value [[272.2031592]] accuracy 0.7272727272727273\n","Iteration 47960 - loss value [[269.53907536]] accuracy 0.7272727272727273\n","Iteration 47961 - loss value [[271.19197431]] accuracy 0.7272727272727273\n","Iteration 47962 - loss value [[268.34069211]] accuracy 0.7272727272727273\n","Iteration 47963 - loss value [[268.44217756]] accuracy 0.7272727272727273\n","Iteration 47964 - loss value [[267.59324696]] accuracy 0.7272727272727273\n","Iteration 47965 - loss value [[269.64971392]] accuracy 0.7272727272727273\n","Iteration 47966 - loss value [[268.59594003]] accuracy 0.7272727272727273\n","Iteration 47967 - loss value [[271.73497815]] accuracy 0.7272727272727273\n","Iteration 47968 - loss value [[269.43718021]] accuracy 0.7272727272727273\n","Iteration 47969 - loss value [[271.08947073]] accuracy 0.7272727272727273\n","Iteration 47970 - loss value [[268.71872158]] accuracy 0.7272727272727273\n","Iteration 47971 - loss value [[269.94273937]] accuracy 0.7272727272727273\n","Iteration 47972 - loss value [[269.10413792]] accuracy 0.7272727272727273\n","Iteration 47973 - loss value [[272.27805301]] accuracy 0.7272727272727273\n","Iteration 47974 - loss value [[269.82966172]] accuracy 0.7272727272727273\n","Iteration 47975 - loss value [[272.44204601]] accuracy 0.7272727272727273\n","Iteration 47976 - loss value [[269.99989844]] accuracy 0.7272727272727273\n","Iteration 47977 - loss value [[273.10214452]] accuracy 0.7272727272727273\n","Iteration 47978 - loss value [[270.55254127]] accuracy 0.7272727272727273\n","Iteration 47979 - loss value [[273.2351158]] accuracy 0.7272727272727273\n","Iteration 47980 - loss value [[270.93408108]] accuracy 0.7272727272727273\n","Iteration 47981 - loss value [[274.15379625]] accuracy 0.7272727272727273\n","Iteration 47982 - loss value [[271.58162597]] accuracy 0.7272727272727273\n","Iteration 47983 - loss value [[272.89107149]] accuracy 0.7272727272727273\n","Iteration 47984 - loss value [[271.01052632]] accuracy 0.7272727272727273\n","Iteration 47985 - loss value [[273.5046919]] accuracy 0.7272727272727273\n","Iteration 47986 - loss value [[270.98098598]] accuracy 0.7272727272727273\n","Iteration 47987 - loss value [[272.66641848]] accuracy 0.7272727272727273\n","Iteration 47988 - loss value [[270.32180217]] accuracy 0.7272727272727273\n","Iteration 47989 - loss value [[273.01857786]] accuracy 0.7272727272727273\n","Iteration 47990 - loss value [[270.41707705]] accuracy 0.7272727272727273\n","Iteration 47991 - loss value [[272.14151879]] accuracy 0.7272727272727273\n","Iteration 47992 - loss value [[269.97752307]] accuracy 0.7272727272727273\n","Iteration 47993 - loss value [[272.19893345]] accuracy 0.7272727272727273\n","Iteration 47994 - loss value [[269.73325819]] accuracy 0.7272727272727273\n","Iteration 47995 - loss value [[271.0395353]] accuracy 0.7272727272727273\n","Iteration 47996 - loss value [[269.44596504]] accuracy 0.7272727272727273\n","Iteration 47997 - loss value [[272.306236]] accuracy 0.7272727272727273\n","Iteration 47998 - loss value [[269.74665488]] accuracy 0.7272727272727273\n","Iteration 47999 - loss value [[270.70001176]] accuracy 0.7272727272727273\n","Iteration 48000 - loss value [[269.44420246]] accuracy 0.7272727272727273\n","Iteration 48001 - loss value [[272.28709368]] accuracy 0.7272727272727273\n","Iteration 48002 - loss value [[269.85731382]] accuracy 0.7272727272727273\n","Iteration 48003 - loss value [[271.77706093]] accuracy 0.7272727272727273\n","Iteration 48004 - loss value [[269.65127985]] accuracy 0.7272727272727273\n","Iteration 48005 - loss value [[272.59832532]] accuracy 0.7272727272727273\n","Iteration 48006 - loss value [[270.20294765]] accuracy 0.7272727272727273\n","Iteration 48007 - loss value [[271.80650082]] accuracy 0.7272727272727273\n","Iteration 48008 - loss value [[269.95178561]] accuracy 0.7272727272727273\n","Iteration 48009 - loss value [[272.85153777]] accuracy 0.7272727272727273\n","Iteration 48010 - loss value [[270.30294287]] accuracy 0.7272727272727273\n","Iteration 48011 - loss value [[272.08606098]] accuracy 0.7272727272727273\n","Iteration 48012 - loss value [[269.93920536]] accuracy 0.7272727272727273\n","Iteration 48013 - loss value [[272.5501514]] accuracy 0.7272727272727273\n","Iteration 48014 - loss value [[270.10612225]] accuracy 0.7272727272727273\n","Iteration 48015 - loss value [[271.42914228]] accuracy 0.7272727272727273\n","Iteration 48016 - loss value [[268.82739853]] accuracy 0.7272727272727273\n","Iteration 48017 - loss value [[269.65537671]] accuracy 0.7272727272727273\n","Iteration 48018 - loss value [[268.33552009]] accuracy 0.7272727272727273\n","Iteration 48019 - loss value [[270.19237887]] accuracy 0.7272727272727273\n","Iteration 48020 - loss value [[268.58268543]] accuracy 0.7272727272727273\n","Iteration 48021 - loss value [[270.24490107]] accuracy 0.7272727272727273\n","Iteration 48022 - loss value [[269.14363201]] accuracy 0.7272727272727273\n","Iteration 48023 - loss value [[272.47716704]] accuracy 0.7272727272727273\n","Iteration 48024 - loss value [[269.96014223]] accuracy 0.7272727272727273\n","Iteration 48025 - loss value [[272.08278371]] accuracy 0.7272727272727273\n","Iteration 48026 - loss value [[270.35156534]] accuracy 0.7272727272727273\n","Iteration 48027 - loss value [[273.56938469]] accuracy 0.7272727272727273\n","Iteration 48028 - loss value [[271.30514914]] accuracy 0.7272727272727273\n","Iteration 48029 - loss value [[273.90393133]] accuracy 0.7272727272727273\n","Iteration 48030 - loss value [[271.60224052]] accuracy 0.7272727272727273\n","Iteration 48031 - loss value [[273.39737118]] accuracy 0.7272727272727273\n","Iteration 48032 - loss value [[271.60575469]] accuracy 0.7272727272727273\n","Iteration 48033 - loss value [[274.42373233]] accuracy 0.7272727272727273\n","Iteration 48034 - loss value [[272.71027687]] accuracy 0.7272727272727273\n","Iteration 48035 - loss value [[275.38601565]] accuracy 0.7272727272727273\n","Iteration 48036 - loss value [[273.88207487]] accuracy 0.7272727272727273\n","Iteration 48037 - loss value [[277.13551947]] accuracy 0.7272727272727273\n","Iteration 48038 - loss value [[277.10451115]] accuracy 0.7272727272727273\n","Iteration 48039 - loss value [[280.08411056]] accuracy 0.7272727272727273\n","Iteration 48040 - loss value [[278.16043443]] accuracy 0.7272727272727273\n","Iteration 48041 - loss value [[280.24381223]] accuracy 0.7272727272727273\n","Iteration 48042 - loss value [[278.11677221]] accuracy 0.7272727272727273\n","Iteration 48043 - loss value [[280.20884066]] accuracy 0.7272727272727273\n","Iteration 48044 - loss value [[277.50747062]] accuracy 0.7272727272727273\n","Iteration 48045 - loss value [[278.72717581]] accuracy 0.7272727272727273\n","Iteration 48046 - loss value [[276.21589043]] accuracy 0.7272727272727273\n","Iteration 48047 - loss value [[277.13492688]] accuracy 0.7272727272727273\n","Iteration 48048 - loss value [[275.81352369]] accuracy 0.7272727272727273\n","Iteration 48049 - loss value [[278.67607452]] accuracy 0.7272727272727273\n","Iteration 48050 - loss value [[275.82438799]] accuracy 0.7272727272727273\n","Iteration 48051 - loss value [[276.65821623]] accuracy 0.7272727272727273\n","Iteration 48052 - loss value [[275.27147343]] accuracy 0.7272727272727273\n","Iteration 48053 - loss value [[277.15863158]] accuracy 0.7272727272727273\n","Iteration 48054 - loss value [[276.22931871]] accuracy 0.7272727272727273\n","Iteration 48055 - loss value [[277.53826291]] accuracy 0.7272727272727273\n","Iteration 48056 - loss value [[275.56596272]] accuracy 0.7272727272727273\n","Iteration 48057 - loss value [[276.99937338]] accuracy 0.7272727272727273\n","Iteration 48058 - loss value [[275.01882101]] accuracy 0.7272727272727273\n","Iteration 48059 - loss value [[276.88550021]] accuracy 0.7272727272727273\n","Iteration 48060 - loss value [[275.41100516]] accuracy 0.7272727272727273\n","Iteration 48061 - loss value [[276.80051435]] accuracy 0.7272727272727273\n","Iteration 48062 - loss value [[275.1412624]] accuracy 0.7272727272727273\n","Iteration 48063 - loss value [[277.07103492]] accuracy 0.7272727272727273\n","Iteration 48064 - loss value [[274.96495465]] accuracy 0.7272727272727273\n","Iteration 48065 - loss value [[275.94030336]] accuracy 0.7272727272727273\n","Iteration 48066 - loss value [[273.75604301]] accuracy 0.7272727272727273\n","Iteration 48067 - loss value [[274.52978662]] accuracy 0.7272727272727273\n","Iteration 48068 - loss value [[271.81609655]] accuracy 0.7272727272727273\n","Iteration 48069 - loss value [[272.07907658]] accuracy 0.7272727272727273\n","Iteration 48070 - loss value [[270.11815927]] accuracy 0.7272727272727273\n","Iteration 48071 - loss value [[270.80223072]] accuracy 0.7272727272727273\n","Iteration 48072 - loss value [[268.55161688]] accuracy 0.7272727272727273\n","Iteration 48073 - loss value [[268.80925416]] accuracy 0.7272727272727273\n","Iteration 48074 - loss value [[267.95279163]] accuracy 0.7272727272727273\n","Iteration 48075 - loss value [[269.72532957]] accuracy 0.7272727272727273\n","Iteration 48076 - loss value [[267.54502194]] accuracy 0.7272727272727273\n","Iteration 48077 - loss value [[267.6307841]] accuracy 0.7272727272727273\n","Iteration 48078 - loss value [[266.78970169]] accuracy 0.7272727272727273\n","Iteration 48079 - loss value [[267.33074367]] accuracy 0.7272727272727273\n","Iteration 48080 - loss value [[266.90456291]] accuracy 0.7272727272727273\n","Iteration 48081 - loss value [[269.28388245]] accuracy 0.7272727272727273\n","Iteration 48082 - loss value [[267.53760034]] accuracy 0.7272727272727273\n","Iteration 48083 - loss value [[267.57078602]] accuracy 0.7272727272727273\n","Iteration 48084 - loss value [[267.50098798]] accuracy 0.7272727272727273\n","Iteration 48085 - loss value [[270.67105401]] accuracy 0.7272727272727273\n","Iteration 48086 - loss value [[268.45186777]] accuracy 0.7272727272727273\n","Iteration 48087 - loss value [[270.98580087]] accuracy 0.7272727272727273\n","Iteration 48088 - loss value [[268.46315989]] accuracy 0.7272727272727273\n","Iteration 48089 - loss value [[269.33841533]] accuracy 0.7272727272727273\n","Iteration 48090 - loss value [[268.18876822]] accuracy 0.7272727272727273\n","Iteration 48091 - loss value [[270.73410287]] accuracy 0.7272727272727273\n","Iteration 48092 - loss value [[268.25787841]] accuracy 0.7272727272727273\n","Iteration 48093 - loss value [[269.30816451]] accuracy 0.7272727272727273\n","Iteration 48094 - loss value [[268.38966383]] accuracy 0.7272727272727273\n","Iteration 48095 - loss value [[271.14264394]] accuracy 0.7272727272727273\n","Iteration 48096 - loss value [[268.8787648]] accuracy 0.7272727272727273\n","Iteration 48097 - loss value [[270.82248636]] accuracy 0.7272727272727273\n","Iteration 48098 - loss value [[268.6925715]] accuracy 0.7272727272727273\n","Iteration 48099 - loss value [[270.54048269]] accuracy 0.7272727272727273\n","Iteration 48100 - loss value [[268.37994047]] accuracy 0.7272727272727273\n","Iteration 48101 - loss value [[269.53646934]] accuracy 0.7272727272727273\n","Iteration 48102 - loss value [[268.75274171]] accuracy 0.7272727272727273\n","Iteration 48103 - loss value [[271.66808066]] accuracy 0.7272727272727273\n","Iteration 48104 - loss value [[269.50558855]] accuracy 0.7272727272727273\n","Iteration 48105 - loss value [[272.00916342]] accuracy 0.7272727272727273\n","Iteration 48106 - loss value [[269.95995435]] accuracy 0.7272727272727273\n","Iteration 48107 - loss value [[273.53159499]] accuracy 0.7272727272727273\n","Iteration 48108 - loss value [[271.00506622]] accuracy 0.7272727272727273\n","Iteration 48109 - loss value [[273.02994954]] accuracy 0.7272727272727273\n","Iteration 48110 - loss value [[271.28609661]] accuracy 0.7272727272727273\n","Iteration 48111 - loss value [[273.48552569]] accuracy 0.7272727272727273\n","Iteration 48112 - loss value [[271.04230525]] accuracy 0.7272727272727273\n","Iteration 48113 - loss value [[273.75705042]] accuracy 0.7272727272727273\n","Iteration 48114 - loss value [[271.12696184]] accuracy 0.7272727272727273\n","Iteration 48115 - loss value [[273.06624894]] accuracy 0.7272727272727273\n","Iteration 48116 - loss value [[270.55122645]] accuracy 0.7272727272727273\n","Iteration 48117 - loss value [[273.30022987]] accuracy 0.7272727272727273\n","Iteration 48118 - loss value [[270.69141736]] accuracy 0.7272727272727273\n","Iteration 48119 - loss value [[271.66074347]] accuracy 0.7272727272727273\n","Iteration 48120 - loss value [[269.13797554]] accuracy 0.7272727272727273\n","Iteration 48121 - loss value [[269.92314977]] accuracy 0.7272727272727273\n","Iteration 48122 - loss value [[268.73207122]] accuracy 0.7272727272727273\n","Iteration 48123 - loss value [[271.15575121]] accuracy 0.7272727272727273\n","Iteration 48124 - loss value [[268.61787672]] accuracy 0.7272727272727273\n","Iteration 48125 - loss value [[269.90990261]] accuracy 0.7272727272727273\n","Iteration 48126 - loss value [[269.10073258]] accuracy 0.7272727272727273\n","Iteration 48127 - loss value [[271.68841716]] accuracy 0.7272727272727273\n","Iteration 48128 - loss value [[269.38078656]] accuracy 0.7272727272727273\n","Iteration 48129 - loss value [[271.1321232]] accuracy 0.7272727272727273\n","Iteration 48130 - loss value [[269.30970433]] accuracy 0.7272727272727273\n","Iteration 48131 - loss value [[272.05679491]] accuracy 0.7272727272727273\n","Iteration 48132 - loss value [[269.47444892]] accuracy 0.7272727272727273\n","Iteration 48133 - loss value [[270.65446764]] accuracy 0.7272727272727273\n","Iteration 48134 - loss value [[269.6855036]] accuracy 0.7272727272727273\n","Iteration 48135 - loss value [[272.63518536]] accuracy 0.7272727272727273\n","Iteration 48136 - loss value [[270.17300266]] accuracy 0.7272727272727273\n","Iteration 48137 - loss value [[271.73168002]] accuracy 0.7272727272727273\n","Iteration 48138 - loss value [[269.53523264]] accuracy 0.7272727272727273\n","Iteration 48139 - loss value [[272.39181466]] accuracy 0.7272727272727273\n","Iteration 48140 - loss value [[269.88802574]] accuracy 0.7272727272727273\n","Iteration 48141 - loss value [[271.66656053]] accuracy 0.7272727272727273\n","Iteration 48142 - loss value [[268.71163112]] accuracy 0.7272727272727273\n","Iteration 48143 - loss value [[269.08068224]] accuracy 0.7272727272727273\n","Iteration 48144 - loss value [[267.71888166]] accuracy 0.7272727272727273\n","Iteration 48145 - loss value [[268.8569501]] accuracy 0.7272727272727273\n","Iteration 48146 - loss value [[267.27832062]] accuracy 0.7272727272727273\n","Iteration 48147 - loss value [[267.51162871]] accuracy 0.7272727272727273\n","Iteration 48148 - loss value [[267.78811776]] accuracy 0.7272727272727273\n","Iteration 48149 - loss value [[270.77963398]] accuracy 0.7272727272727273\n","Iteration 48150 - loss value [[268.86612417]] accuracy 0.7272727272727273\n","Iteration 48151 - loss value [[272.43721695]] accuracy 0.7272727272727273\n","Iteration 48152 - loss value [[269.83158945]] accuracy 0.7272727272727273\n","Iteration 48153 - loss value [[271.68995368]] accuracy 0.7272727272727273\n","Iteration 48154 - loss value [[270.10710316]] accuracy 0.7272727272727273\n","Iteration 48155 - loss value [[273.23420374]] accuracy 0.7272727272727273\n","Iteration 48156 - loss value [[270.72831722]] accuracy 0.7272727272727273\n","Iteration 48157 - loss value [[273.03559821]] accuracy 0.7272727272727273\n","Iteration 48158 - loss value [[270.683327]] accuracy 0.7272727272727273\n","Iteration 48159 - loss value [[274.18552601]] accuracy 0.7272727272727273\n","Iteration 48160 - loss value [[272.17137712]] accuracy 0.7272727272727273\n","Iteration 48161 - loss value [[274.82363211]] accuracy 0.7272727272727273\n","Iteration 48162 - loss value [[273.4230057]] accuracy 0.7272727272727273\n","Iteration 48163 - loss value [[276.671554]] accuracy 0.7272727272727273\n","Iteration 48164 - loss value [[276.35798583]] accuracy 0.7272727272727273\n","Iteration 48165 - loss value [[279.16050388]] accuracy 0.7272727272727273\n","Iteration 48166 - loss value [[278.86356144]] accuracy 0.7272727272727273\n","Iteration 48167 - loss value [[281.86774987]] accuracy 0.7272727272727273\n","Iteration 48168 - loss value [[278.87593758]] accuracy 0.7272727272727273\n","Iteration 48169 - loss value [[278.94435363]] accuracy 0.7272727272727273\n","Iteration 48170 - loss value [[278.8993367]] accuracy 0.7272727272727273\n","Iteration 48171 - loss value [[280.69186143]] accuracy 0.7272727272727273\n","Iteration 48172 - loss value [[278.07926674]] accuracy 0.7272727272727273\n","Iteration 48173 - loss value [[279.88131275]] accuracy 0.7272727272727273\n","Iteration 48174 - loss value [[277.23717595]] accuracy 0.7272727272727273\n","Iteration 48175 - loss value [[277.83891954]] accuracy 0.7272727272727273\n","Iteration 48176 - loss value [[276.54285155]] accuracy 0.7272727272727273\n","Iteration 48177 - loss value [[278.01043104]] accuracy 0.7272727272727273\n","Iteration 48178 - loss value [[276.19382609]] accuracy 0.7272727272727273\n","Iteration 48179 - loss value [[276.70942691]] accuracy 0.7272727272727273\n","Iteration 48180 - loss value [[274.81100412]] accuracy 0.7272727272727273\n","Iteration 48181 - loss value [[275.64567392]] accuracy 0.7272727272727273\n","Iteration 48182 - loss value [[273.90532746]] accuracy 0.7272727272727273\n","Iteration 48183 - loss value [[275.34232853]] accuracy 0.7272727272727273\n","Iteration 48184 - loss value [[273.14899836]] accuracy 0.7272727272727273\n","Iteration 48185 - loss value [[274.05529647]] accuracy 0.7272727272727273\n","Iteration 48186 - loss value [[271.55704616]] accuracy 0.7272727272727273\n","Iteration 48187 - loss value [[272.21027106]] accuracy 0.7272727272727273\n","Iteration 48188 - loss value [[269.42176182]] accuracy 0.7272727272727273\n","Iteration 48189 - loss value [[270.01120183]] accuracy 0.7272727272727273\n","Iteration 48190 - loss value [[267.73373043]] accuracy 0.7272727272727273\n","Iteration 48191 - loss value [[267.93920848]] accuracy 0.7272727272727273\n","Iteration 48192 - loss value [[267.12840659]] accuracy 0.7272727272727273\n","Iteration 48193 - loss value [[268.30554671]] accuracy 0.7272727272727273\n","Iteration 48194 - loss value [[267.49837133]] accuracy 0.7272727272727273\n","Iteration 48195 - loss value [[268.21307975]] accuracy 0.7272727272727273\n","Iteration 48196 - loss value [[267.41586837]] accuracy 0.7272727272727273\n","Iteration 48197 - loss value [[268.89060122]] accuracy 0.7272727272727273\n","Iteration 48198 - loss value [[268.40097559]] accuracy 0.7272727272727273\n","Iteration 48199 - loss value [[270.18843249]] accuracy 0.7272727272727273\n","Iteration 48200 - loss value [[269.37803361]] accuracy 0.7272727272727273\n","Iteration 48201 - loss value [[272.3878734]] accuracy 0.7272727272727273\n","Iteration 48202 - loss value [[269.75584811]] accuracy 0.7272727272727273\n","Iteration 48203 - loss value [[271.15486327]] accuracy 0.7272727272727273\n","Iteration 48204 - loss value [[270.05010433]] accuracy 0.7272727272727273\n","Iteration 48205 - loss value [[273.31471516]] accuracy 0.7272727272727273\n","Iteration 48206 - loss value [[270.62608015]] accuracy 0.7272727272727273\n","Iteration 48207 - loss value [[272.35767051]] accuracy 0.7272727272727273\n","Iteration 48208 - loss value [[270.33298621]] accuracy 0.7272727272727273\n","Iteration 48209 - loss value [[273.36835338]] accuracy 0.7272727272727273\n","Iteration 48210 - loss value [[270.51117907]] accuracy 0.7272727272727273\n","Iteration 48211 - loss value [[271.6730332]] accuracy 0.7272727272727273\n","Iteration 48212 - loss value [[268.89831093]] accuracy 0.7272727272727273\n","Iteration 48213 - loss value [[270.06095677]] accuracy 0.7272727272727273\n","Iteration 48214 - loss value [[268.43712399]] accuracy 0.7272727272727273\n","Iteration 48215 - loss value [[270.45058278]] accuracy 0.7272727272727273\n","Iteration 48216 - loss value [[268.03727087]] accuracy 0.7272727272727273\n","Iteration 48217 - loss value [[268.89113377]] accuracy 0.7272727272727273\n","Iteration 48218 - loss value [[268.02050166]] accuracy 0.7272727272727273\n","Iteration 48219 - loss value [[270.36476591]] accuracy 0.7272727272727273\n","Iteration 48220 - loss value [[268.39035686]] accuracy 0.7272727272727273\n","Iteration 48221 - loss value [[270.72248523]] accuracy 0.7272727272727273\n","Iteration 48222 - loss value [[268.60428603]] accuracy 0.7272727272727273\n","Iteration 48223 - loss value [[271.31444479]] accuracy 0.7272727272727273\n","Iteration 48224 - loss value [[268.86011812]] accuracy 0.7272727272727273\n","Iteration 48225 - loss value [[270.02654252]] accuracy 0.7272727272727273\n","Iteration 48226 - loss value [[269.23474385]] accuracy 0.7272727272727273\n","Iteration 48227 - loss value [[271.79920191]] accuracy 0.7272727272727273\n","Iteration 48228 - loss value [[269.40077339]] accuracy 0.7272727272727273\n","Iteration 48229 - loss value [[271.10507714]] accuracy 0.7272727272727273\n","Iteration 48230 - loss value [[268.64804785]] accuracy 0.7272727272727273\n","Iteration 48231 - loss value [[269.45501841]] accuracy 0.7272727272727273\n","Iteration 48232 - loss value [[268.22763255]] accuracy 0.7272727272727273\n","Iteration 48233 - loss value [[271.01575049]] accuracy 0.7272727272727273\n","Iteration 48234 - loss value [[268.59717919]] accuracy 0.7272727272727273\n","Iteration 48235 - loss value [[269.8542398]] accuracy 0.7272727272727273\n","Iteration 48236 - loss value [[269.01700758]] accuracy 0.7272727272727273\n","Iteration 48237 - loss value [[272.00312265]] accuracy 0.7272727272727273\n","Iteration 48238 - loss value [[269.69082741]] accuracy 0.7272727272727273\n","Iteration 48239 - loss value [[272.4291766]] accuracy 0.7272727272727273\n","Iteration 48240 - loss value [[269.92653042]] accuracy 0.7272727272727273\n","Iteration 48241 - loss value [[272.07450073]] accuracy 0.7272727272727273\n","Iteration 48242 - loss value [[270.06854774]] accuracy 0.7272727272727273\n","Iteration 48243 - loss value [[273.05972657]] accuracy 0.7272727272727273\n","Iteration 48244 - loss value [[270.4147667]] accuracy 0.7272727272727273\n","Iteration 48245 - loss value [[271.52857881]] accuracy 0.7272727272727273\n","Iteration 48246 - loss value [[269.05711605]] accuracy 0.7272727272727273\n","Iteration 48247 - loss value [[269.93495609]] accuracy 0.7272727272727273\n","Iteration 48248 - loss value [[269.07875622]] accuracy 0.7272727272727273\n","Iteration 48249 - loss value [[271.80776674]] accuracy 0.7272727272727273\n","Iteration 48250 - loss value [[269.41740468]] accuracy 0.7272727272727273\n","Iteration 48251 - loss value [[270.9064804]] accuracy 0.7272727272727273\n","Iteration 48252 - loss value [[268.71241441]] accuracy 0.7272727272727273\n","Iteration 48253 - loss value [[269.63470901]] accuracy 0.7272727272727273\n","Iteration 48254 - loss value [[268.46760316]] accuracy 0.7272727272727273\n","Iteration 48255 - loss value [[271.55952279]] accuracy 0.7272727272727273\n","Iteration 48256 - loss value [[269.14986605]] accuracy 0.7272727272727273\n","Iteration 48257 - loss value [[271.29800929]] accuracy 0.7272727272727273\n","Iteration 48258 - loss value [[268.46947023]] accuracy 0.7272727272727273\n","Iteration 48259 - loss value [[269.2477449]] accuracy 0.7272727272727273\n","Iteration 48260 - loss value [[268.09579802]] accuracy 0.7272727272727273\n","Iteration 48261 - loss value [[270.73892064]] accuracy 0.7272727272727273\n","Iteration 48262 - loss value [[268.23637067]] accuracy 0.7272727272727273\n","Iteration 48263 - loss value [[269.06892821]] accuracy 0.7272727272727273\n","Iteration 48264 - loss value [[267.76368204]] accuracy 0.7272727272727273\n","Iteration 48265 - loss value [[269.98435928]] accuracy 0.7272727272727273\n","Iteration 48266 - loss value [[269.44873335]] accuracy 0.7272727272727273\n","Iteration 48267 - loss value [[272.43840441]] accuracy 0.7272727272727273\n","Iteration 48268 - loss value [[269.93834476]] accuracy 0.7272727272727273\n","Iteration 48269 - loss value [[272.78488555]] accuracy 0.7272727272727273\n","Iteration 48270 - loss value [[270.4062257]] accuracy 0.7272727272727273\n","Iteration 48271 - loss value [[273.2555522]] accuracy 0.7272727272727273\n","Iteration 48272 - loss value [[270.89900825]] accuracy 0.7272727272727273\n","Iteration 48273 - loss value [[274.42506627]] accuracy 0.7272727272727273\n","Iteration 48274 - loss value [[272.67000642]] accuracy 0.7272727272727273\n","Iteration 48275 - loss value [[274.89286584]] accuracy 0.7272727272727273\n","Iteration 48276 - loss value [[273.77029141]] accuracy 0.7272727272727273\n","Iteration 48277 - loss value [[277.08950952]] accuracy 0.7272727272727273\n","Iteration 48278 - loss value [[277.06143819]] accuracy 0.7272727272727273\n","Iteration 48279 - loss value [[279.97969153]] accuracy 0.7272727272727273\n","Iteration 48280 - loss value [[277.97079506]] accuracy 0.7272727272727273\n","Iteration 48281 - loss value [[280.47342677]] accuracy 0.7272727272727273\n","Iteration 48282 - loss value [[278.11363678]] accuracy 0.7272727272727273\n","Iteration 48283 - loss value [[279.87915407]] accuracy 0.7272727272727273\n","Iteration 48284 - loss value [[277.10218776]] accuracy 0.7272727272727273\n","Iteration 48285 - loss value [[278.03244011]] accuracy 0.7272727272727273\n","Iteration 48286 - loss value [[277.61722884]] accuracy 0.7272727272727273\n","Iteration 48287 - loss value [[279.13491115]] accuracy 0.7272727272727273\n","Iteration 48288 - loss value [[276.58162538]] accuracy 0.7272727272727273\n","Iteration 48289 - loss value [[278.36052103]] accuracy 0.7272727272727273\n","Iteration 48290 - loss value [[277.17243668]] accuracy 0.7272727272727273\n","Iteration 48291 - loss value [[278.75719203]] accuracy 0.7272727272727273\n","Iteration 48292 - loss value [[275.57760801]] accuracy 0.7272727272727273\n","Iteration 48293 - loss value [[276.06899463]] accuracy 0.7272727272727273\n","Iteration 48294 - loss value [[274.06694689]] accuracy 0.7272727272727273\n","Iteration 48295 - loss value [[275.85995797]] accuracy 0.7272727272727273\n","Iteration 48296 - loss value [[273.84458142]] accuracy 0.7272727272727273\n","Iteration 48297 - loss value [[275.39264248]] accuracy 0.7272727272727273\n","Iteration 48298 - loss value [[273.42916271]] accuracy 0.7272727272727273\n","Iteration 48299 - loss value [[275.0033293]] accuracy 0.7272727272727273\n","Iteration 48300 - loss value [[272.81789882]] accuracy 0.7272727272727273\n","Iteration 48301 - loss value [[273.62888392]] accuracy 0.7272727272727273\n","Iteration 48302 - loss value [[271.11897506]] accuracy 0.7272727272727273\n","Iteration 48303 - loss value [[272.42517443]] accuracy 0.7272727272727273\n","Iteration 48304 - loss value [[269.60688644]] accuracy 0.7272727272727273\n","Iteration 48305 - loss value [[270.12799464]] accuracy 0.7272727272727273\n","Iteration 48306 - loss value [[267.99658818]] accuracy 0.7272727272727273\n","Iteration 48307 - loss value [[267.97042278]] accuracy 0.7272727272727273\n","Iteration 48308 - loss value [[267.21533718]] accuracy 0.7272727272727273\n","Iteration 48309 - loss value [[267.94113846]] accuracy 0.7272727272727273\n","Iteration 48310 - loss value [[267.26181442]] accuracy 0.7272727272727273\n","Iteration 48311 - loss value [[268.01354908]] accuracy 0.7272727272727273\n","Iteration 48312 - loss value [[267.1138979]] accuracy 0.7272727272727273\n","Iteration 48313 - loss value [[268.10926385]] accuracy 0.7272727272727273\n","Iteration 48314 - loss value [[267.42245534]] accuracy 0.7272727272727273\n","Iteration 48315 - loss value [[267.63368152]] accuracy 0.7272727272727273\n","Iteration 48316 - loss value [[267.73143429]] accuracy 0.7272727272727273\n","Iteration 48317 - loss value [[270.08478002]] accuracy 0.7272727272727273\n","Iteration 48318 - loss value [[268.93751166]] accuracy 0.7272727272727273\n","Iteration 48319 - loss value [[271.50238046]] accuracy 0.7272727272727273\n","Iteration 48320 - loss value [[269.2931486]] accuracy 0.7272727272727273\n","Iteration 48321 - loss value [[271.08174263]] accuracy 0.7272727272727273\n","Iteration 48322 - loss value [[268.89244463]] accuracy 0.7272727272727273\n","Iteration 48323 - loss value [[270.46534054]] accuracy 0.7272727272727273\n","Iteration 48324 - loss value [[269.40960037]] accuracy 0.7272727272727273\n","Iteration 48325 - loss value [[272.13675421]] accuracy 0.7272727272727273\n","Iteration 48326 - loss value [[269.87775456]] accuracy 0.7272727272727273\n","Iteration 48327 - loss value [[271.87481214]] accuracy 0.7272727272727273\n","Iteration 48328 - loss value [[269.62705056]] accuracy 0.7272727272727273\n","Iteration 48329 - loss value [[272.05586704]] accuracy 0.7272727272727273\n","Iteration 48330 - loss value [[269.72930397]] accuracy 0.7272727272727273\n","Iteration 48331 - loss value [[271.52941215]] accuracy 0.7272727272727273\n","Iteration 48332 - loss value [[269.03879585]] accuracy 0.7272727272727273\n","Iteration 48333 - loss value [[269.59987774]] accuracy 0.7272727272727273\n","Iteration 48334 - loss value [[268.72241719]] accuracy 0.7272727272727273\n","Iteration 48335 - loss value [[271.29664767]] accuracy 0.7272727272727273\n","Iteration 48336 - loss value [[268.9378968]] accuracy 0.7272727272727273\n","Iteration 48337 - loss value [[270.10343812]] accuracy 0.7272727272727273\n","Iteration 48338 - loss value [[268.95490552]] accuracy 0.7272727272727273\n","Iteration 48339 - loss value [[271.57589475]] accuracy 0.7272727272727273\n","Iteration 48340 - loss value [[269.14246894]] accuracy 0.7272727272727273\n","Iteration 48341 - loss value [[270.52822855]] accuracy 0.7272727272727273\n","Iteration 48342 - loss value [[269.30616355]] accuracy 0.7272727272727273\n","Iteration 48343 - loss value [[272.50654615]] accuracy 0.7272727272727273\n","Iteration 48344 - loss value [[269.9002114]] accuracy 0.7272727272727273\n","Iteration 48345 - loss value [[271.4781734]] accuracy 0.7272727272727273\n","Iteration 48346 - loss value [[268.77081123]] accuracy 0.7272727272727273\n","Iteration 48347 - loss value [[269.41459472]] accuracy 0.7272727272727273\n","Iteration 48348 - loss value [[268.19025897]] accuracy 0.7272727272727273\n","Iteration 48349 - loss value [[269.91418377]] accuracy 0.7272727272727273\n","Iteration 48350 - loss value [[269.1090671]] accuracy 0.7272727272727273\n","Iteration 48351 - loss value [[271.94058813]] accuracy 0.7272727272727273\n","Iteration 48352 - loss value [[269.81592062]] accuracy 0.7272727272727273\n","Iteration 48353 - loss value [[272.33930542]] accuracy 0.7272727272727273\n","Iteration 48354 - loss value [[269.91821886]] accuracy 0.7272727272727273\n","Iteration 48355 - loss value [[271.34165373]] accuracy 0.7272727272727273\n","Iteration 48356 - loss value [[269.04126083]] accuracy 0.7272727272727273\n","Iteration 48357 - loss value [[271.34259309]] accuracy 0.7272727272727273\n","Iteration 48358 - loss value [[268.84206993]] accuracy 0.7272727272727273\n","Iteration 48359 - loss value [[269.95470133]] accuracy 0.7272727272727273\n","Iteration 48360 - loss value [[268.99328229]] accuracy 0.7272727272727273\n","Iteration 48361 - loss value [[272.07863676]] accuracy 0.7272727272727273\n","Iteration 48362 - loss value [[269.35469494]] accuracy 0.7272727272727273\n","Iteration 48363 - loss value [[270.83467275]] accuracy 0.7272727272727273\n","Iteration 48364 - loss value [[269.36164954]] accuracy 0.7272727272727273\n","Iteration 48365 - loss value [[271.83240431]] accuracy 0.7272727272727273\n","Iteration 48366 - loss value [[269.37190993]] accuracy 0.7272727272727273\n","Iteration 48367 - loss value [[270.85988402]] accuracy 0.7272727272727273\n","Iteration 48368 - loss value [[269.42178249]] accuracy 0.7272727272727273\n","Iteration 48369 - loss value [[272.61487245]] accuracy 0.7272727272727273\n","Iteration 48370 - loss value [[269.83770815]] accuracy 0.7272727272727273\n","Iteration 48371 - loss value [[271.31291532]] accuracy 0.7272727272727273\n","Iteration 48372 - loss value [[268.75505851]] accuracy 0.7272727272727273\n","Iteration 48373 - loss value [[269.30288707]] accuracy 0.7272727272727273\n","Iteration 48374 - loss value [[268.05721702]] accuracy 0.7272727272727273\n","Iteration 48375 - loss value [[269.96345639]] accuracy 0.7272727272727273\n","Iteration 48376 - loss value [[267.84113464]] accuracy 0.7272727272727273\n","Iteration 48377 - loss value [[268.94705285]] accuracy 0.7272727272727273\n","Iteration 48378 - loss value [[267.71438967]] accuracy 0.7272727272727273\n","Iteration 48379 - loss value [[270.33025838]] accuracy 0.7272727272727273\n","Iteration 48380 - loss value [[267.90948148]] accuracy 0.7272727272727273\n","Iteration 48381 - loss value [[268.43425906]] accuracy 0.7272727272727273\n","Iteration 48382 - loss value [[267.67662518]] accuracy 0.7272727272727273\n","Iteration 48383 - loss value [[270.56854184]] accuracy 0.7272727272727273\n","Iteration 48384 - loss value [[268.29063117]] accuracy 0.7272727272727273\n","Iteration 48385 - loss value [[269.10772285]] accuracy 0.7272727272727273\n","Iteration 48386 - loss value [[268.60855795]] accuracy 0.7272727272727273\n","Iteration 48387 - loss value [[271.46274572]] accuracy 0.7272727272727273\n","Iteration 48388 - loss value [[269.49396084]] accuracy 0.7272727272727273\n","Iteration 48389 - loss value [[272.05045085]] accuracy 0.7272727272727273\n","Iteration 48390 - loss value [[270.19477862]] accuracy 0.7272727272727273\n","Iteration 48391 - loss value [[273.64362817]] accuracy 0.7272727272727273\n","Iteration 48392 - loss value [[271.45982808]] accuracy 0.7272727272727273\n","Iteration 48393 - loss value [[274.42699665]] accuracy 0.7272727272727273\n","Iteration 48394 - loss value [[272.95755327]] accuracy 0.7272727272727273\n","Iteration 48395 - loss value [[276.31276268]] accuracy 0.7272727272727273\n","Iteration 48396 - loss value [[276.4526464]] accuracy 0.7272727272727273\n","Iteration 48397 - loss value [[279.21883225]] accuracy 0.7272727272727273\n","Iteration 48398 - loss value [[279.11261056]] accuracy 0.7272727272727273\n","Iteration 48399 - loss value [[281.41073145]] accuracy 0.7272727272727273\n","Iteration 48400 - loss value [[278.55829754]] accuracy 0.7272727272727273\n","Iteration 48401 - loss value [[279.83625754]] accuracy 0.7272727272727273\n","Iteration 48402 - loss value [[277.3213739]] accuracy 0.7272727272727273\n","Iteration 48403 - loss value [[279.69472904]] accuracy 0.7272727272727273\n","Iteration 48404 - loss value [[277.07870538]] accuracy 0.7272727272727273\n","Iteration 48405 - loss value [[278.59451625]] accuracy 0.7272727272727273\n","Iteration 48406 - loss value [[277.26199182]] accuracy 0.7272727272727273\n","Iteration 48407 - loss value [[278.67618536]] accuracy 0.7272727272727273\n","Iteration 48408 - loss value [[277.51355935]] accuracy 0.7272727272727273\n","Iteration 48409 - loss value [[278.51168503]] accuracy 0.7272727272727273\n","Iteration 48410 - loss value [[276.85704718]] accuracy 0.7272727272727273\n","Iteration 48411 - loss value [[277.46491972]] accuracy 0.7272727272727273\n","Iteration 48412 - loss value [[275.44706306]] accuracy 0.7272727272727273\n","Iteration 48413 - loss value [[276.64821856]] accuracy 0.7272727272727273\n","Iteration 48414 - loss value [[274.88780785]] accuracy 0.7272727272727273\n","Iteration 48415 - loss value [[276.42289712]] accuracy 0.7272727272727273\n","Iteration 48416 - loss value [[274.73237561]] accuracy 0.7272727272727273\n","Iteration 48417 - loss value [[276.24921284]] accuracy 0.7272727272727273\n","Iteration 48418 - loss value [[274.43772765]] accuracy 0.7272727272727273\n","Iteration 48419 - loss value [[275.780706]] accuracy 0.7272727272727273\n","Iteration 48420 - loss value [[273.77600199]] accuracy 0.7272727272727273\n","Iteration 48421 - loss value [[275.18894811]] accuracy 0.7272727272727273\n","Iteration 48422 - loss value [[272.85807138]] accuracy 0.7272727272727273\n","Iteration 48423 - loss value [[273.21811973]] accuracy 0.7272727272727273\n","Iteration 48424 - loss value [[270.36264283]] accuracy 0.7272727272727273\n","Iteration 48425 - loss value [[271.23375273]] accuracy 0.7272727272727273\n","Iteration 48426 - loss value [[268.96288949]] accuracy 0.7272727272727273\n","Iteration 48427 - loss value [[269.00691035]] accuracy 0.7272727272727273\n","Iteration 48428 - loss value [[267.31680341]] accuracy 0.7272727272727273\n","Iteration 48429 - loss value [[267.18546262]] accuracy 0.7272727272727273\n","Iteration 48430 - loss value [[267.01653437]] accuracy 0.7272727272727273\n","Iteration 48431 - loss value [[268.60664868]] accuracy 0.7272727272727273\n","Iteration 48432 - loss value [[267.76860958]] accuracy 0.7272727272727273\n","Iteration 48433 - loss value [[268.07655405]] accuracy 0.7272727272727273\n","Iteration 48434 - loss value [[267.15552419]] accuracy 0.7272727272727273\n","Iteration 48435 - loss value [[267.32572931]] accuracy 0.7272727272727273\n","Iteration 48436 - loss value [[266.89285855]] accuracy 0.7272727272727273\n","Iteration 48437 - loss value [[268.79194114]] accuracy 0.7272727272727273\n","Iteration 48438 - loss value [[268.2225199]] accuracy 0.7272727272727273\n","Iteration 48439 - loss value [[271.10885125]] accuracy 0.7272727272727273\n","Iteration 48440 - loss value [[268.51478946]] accuracy 0.7272727272727273\n","Iteration 48441 - loss value [[269.75497404]] accuracy 0.7272727272727273\n","Iteration 48442 - loss value [[269.00500966]] accuracy 0.7272727272727273\n","Iteration 48443 - loss value [[271.42565005]] accuracy 0.7272727272727273\n","Iteration 48444 - loss value [[269.04490417]] accuracy 0.7272727272727273\n","Iteration 48445 - loss value [[270.55866057]] accuracy 0.7272727272727273\n","Iteration 48446 - loss value [[269.6916973]] accuracy 0.7272727272727273\n","Iteration 48447 - loss value [[272.40528519]] accuracy 0.7272727272727273\n","Iteration 48448 - loss value [[270.10204556]] accuracy 0.7272727272727273\n","Iteration 48449 - loss value [[272.4573432]] accuracy 0.7272727272727273\n","Iteration 48450 - loss value [[270.18896391]] accuracy 0.7272727272727273\n","Iteration 48451 - loss value [[272.22884519]] accuracy 0.7272727272727273\n","Iteration 48452 - loss value [[269.73945965]] accuracy 0.7272727272727273\n","Iteration 48453 - loss value [[271.04993427]] accuracy 0.7272727272727273\n","Iteration 48454 - loss value [[269.78765589]] accuracy 0.7272727272727273\n","Iteration 48455 - loss value [[272.85763774]] accuracy 0.7272727272727273\n","Iteration 48456 - loss value [[270.14328752]] accuracy 0.7272727272727273\n","Iteration 48457 - loss value [[271.99529368]] accuracy 0.7272727272727273\n","Iteration 48458 - loss value [[269.78873177]] accuracy 0.7272727272727273\n","Iteration 48459 - loss value [[272.8704408]] accuracy 0.7272727272727273\n","Iteration 48460 - loss value [[270.03417421]] accuracy 0.7272727272727273\n","Iteration 48461 - loss value [[271.21491896]] accuracy 0.7272727272727273\n","Iteration 48462 - loss value [[268.24260561]] accuracy 0.7272727272727273\n","Iteration 48463 - loss value [[268.00490979]] accuracy 0.7272727272727273\n","Iteration 48464 - loss value [[267.31038876]] accuracy 0.7272727272727273\n","Iteration 48465 - loss value [[268.17435807]] accuracy 0.7272727272727273\n","Iteration 48466 - loss value [[267.60169438]] accuracy 0.7272727272727273\n","Iteration 48467 - loss value [[268.43566688]] accuracy 0.7272727272727273\n","Iteration 48468 - loss value [[268.12691036]] accuracy 0.7272727272727273\n","Iteration 48469 - loss value [[270.37484663]] accuracy 0.7272727272727273\n","Iteration 48470 - loss value [[268.51939691]] accuracy 0.7272727272727273\n","Iteration 48471 - loss value [[271.74028159]] accuracy 0.7272727272727273\n","Iteration 48472 - loss value [[269.17792229]] accuracy 0.7272727272727273\n","Iteration 48473 - loss value [[270.62028704]] accuracy 0.7272727272727273\n","Iteration 48474 - loss value [[268.39647782]] accuracy 0.7272727272727273\n","Iteration 48475 - loss value [[269.27885025]] accuracy 0.7272727272727273\n","Iteration 48476 - loss value [[268.74480392]] accuracy 0.7272727272727273\n","Iteration 48477 - loss value [[271.74711619]] accuracy 0.7272727272727273\n","Iteration 48478 - loss value [[269.22714926]] accuracy 0.7272727272727273\n","Iteration 48479 - loss value [[271.28119534]] accuracy 0.7272727272727273\n","Iteration 48480 - loss value [[269.37619894]] accuracy 0.7272727272727273\n","Iteration 48481 - loss value [[272.02755434]] accuracy 0.7272727272727273\n","Iteration 48482 - loss value [[269.77589034]] accuracy 0.7272727272727273\n","Iteration 48483 - loss value [[272.05062279]] accuracy 0.7272727272727273\n","Iteration 48484 - loss value [[269.93706202]] accuracy 0.7272727272727273\n","Iteration 48485 - loss value [[272.92486407]] accuracy 0.7272727272727273\n","Iteration 48486 - loss value [[270.59159992]] accuracy 0.7272727272727273\n","Iteration 48487 - loss value [[272.63854328]] accuracy 0.7272727272727273\n","Iteration 48488 - loss value [[270.45546162]] accuracy 0.7272727272727273\n","Iteration 48489 - loss value [[273.13828226]] accuracy 0.7272727272727273\n","Iteration 48490 - loss value [[270.75772261]] accuracy 0.7272727272727273\n","Iteration 48491 - loss value [[272.38765678]] accuracy 0.7272727272727273\n","Iteration 48492 - loss value [[269.9023411]] accuracy 0.7272727272727273\n","Iteration 48493 - loss value [[272.72275917]] accuracy 0.7272727272727273\n","Iteration 48494 - loss value [[270.185501]] accuracy 0.7272727272727273\n","Iteration 48495 - loss value [[271.64204039]] accuracy 0.7272727272727273\n","Iteration 48496 - loss value [[269.51409501]] accuracy 0.7272727272727273\n","Iteration 48497 - loss value [[271.86444568]] accuracy 0.7272727272727273\n","Iteration 48498 - loss value [[269.3947086]] accuracy 0.7272727272727273\n","Iteration 48499 - loss value [[270.80725588]] accuracy 0.7272727272727273\n","Iteration 48500 - loss value [[268.26401805]] accuracy 0.7272727272727273\n","Iteration 48501 - loss value [[268.64704066]] accuracy 0.7272727272727273\n","Iteration 48502 - loss value [[267.7721095]] accuracy 0.7272727272727273\n","Iteration 48503 - loss value [[269.75032595]] accuracy 0.7272727272727273\n","Iteration 48504 - loss value [[267.70192098]] accuracy 0.7272727272727273\n","Iteration 48505 - loss value [[269.58392008]] accuracy 0.7272727272727273\n","Iteration 48506 - loss value [[268.1349698]] accuracy 0.7272727272727273\n","Iteration 48507 - loss value [[271.08292103]] accuracy 0.7272727272727273\n","Iteration 48508 - loss value [[268.69788442]] accuracy 0.7272727272727273\n","Iteration 48509 - loss value [[269.84582323]] accuracy 0.7272727272727273\n","Iteration 48510 - loss value [[269.34557223]] accuracy 0.7272727272727273\n","Iteration 48511 - loss value [[272.40497214]] accuracy 0.7272727272727273\n","Iteration 48512 - loss value [[270.35982317]] accuracy 0.7272727272727273\n","Iteration 48513 - loss value [[272.91204702]] accuracy 0.7272727272727273\n","Iteration 48514 - loss value [[270.7220075]] accuracy 0.7272727272727273\n","Iteration 48515 - loss value [[273.76237717]] accuracy 0.7272727272727273\n","Iteration 48516 - loss value [[271.44157817]] accuracy 0.7272727272727273\n","Iteration 48517 - loss value [[274.73690745]] accuracy 0.7272727272727273\n","Iteration 48518 - loss value [[273.81713125]] accuracy 0.7272727272727273\n","Iteration 48519 - loss value [[277.06069709]] accuracy 0.7272727272727273\n","Iteration 48520 - loss value [[277.18374563]] accuracy 0.7272727272727273\n","Iteration 48521 - loss value [[279.39891975]] accuracy 0.7272727272727273\n","Iteration 48522 - loss value [[277.30498145]] accuracy 0.7272727272727273\n","Iteration 48523 - loss value [[279.20187407]] accuracy 0.7272727272727273\n","Iteration 48524 - loss value [[278.28054145]] accuracy 0.7272727272727273\n","Iteration 48525 - loss value [[280.30394826]] accuracy 0.7272727272727273\n","Iteration 48526 - loss value [[277.61242228]] accuracy 0.7272727272727273\n","Iteration 48527 - loss value [[278.97843979]] accuracy 0.7272727272727273\n","Iteration 48528 - loss value [[277.38196133]] accuracy 0.7272727272727273\n","Iteration 48529 - loss value [[278.58935899]] accuracy 0.7272727272727273\n","Iteration 48530 - loss value [[276.84639363]] accuracy 0.7272727272727273\n","Iteration 48531 - loss value [[277.70938653]] accuracy 0.7272727272727273\n","Iteration 48532 - loss value [[276.86567834]] accuracy 0.7272727272727273\n","Iteration 48533 - loss value [[278.00263139]] accuracy 0.7272727272727273\n","Iteration 48534 - loss value [[276.40627465]] accuracy 0.7272727272727273\n","Iteration 48535 - loss value [[276.75813923]] accuracy 0.7272727272727273\n","Iteration 48536 - loss value [[275.09841796]] accuracy 0.7272727272727273\n","Iteration 48537 - loss value [[275.65712082]] accuracy 0.7272727272727273\n","Iteration 48538 - loss value [[273.63934518]] accuracy 0.7272727272727273\n","Iteration 48539 - loss value [[274.51357562]] accuracy 0.7272727272727273\n","Iteration 48540 - loss value [[271.58116519]] accuracy 0.7272727272727273\n","Iteration 48541 - loss value [[272.23380846]] accuracy 0.7272727272727273\n","Iteration 48542 - loss value [[269.05479401]] accuracy 0.7272727272727273\n","Iteration 48543 - loss value [[268.62837583]] accuracy 0.7272727272727273\n","Iteration 48544 - loss value [[267.29394334]] accuracy 0.7272727272727273\n","Iteration 48545 - loss value [[266.97935222]] accuracy 0.7272727272727273\n","Iteration 48546 - loss value [[266.36805865]] accuracy 0.7272727272727273\n","Iteration 48547 - loss value [[267.56988201]] accuracy 0.7272727272727273\n","Iteration 48548 - loss value [[266.9953675]] accuracy 0.7272727272727273\n","Iteration 48549 - loss value [[267.15652963]] accuracy 0.7272727272727273\n","Iteration 48550 - loss value [[266.83524895]] accuracy 0.7272727272727273\n","Iteration 48551 - loss value [[267.82227794]] accuracy 0.7272727272727273\n","Iteration 48552 - loss value [[266.77180005]] accuracy 0.7272727272727273\n","Iteration 48553 - loss value [[266.79000091]] accuracy 0.7272727272727273\n","Iteration 48554 - loss value [[265.96467333]] accuracy 0.7272727272727273\n","Iteration 48555 - loss value [[266.91927809]] accuracy 0.7272727272727273\n","Iteration 48556 - loss value [[266.39689766]] accuracy 0.7272727272727273\n","Iteration 48557 - loss value [[267.67813217]] accuracy 0.7272727272727273\n","Iteration 48558 - loss value [[267.32055454]] accuracy 0.7272727272727273\n","Iteration 48559 - loss value [[268.58909285]] accuracy 0.7272727272727273\n","Iteration 48560 - loss value [[268.05057215]] accuracy 0.7272727272727273\n","Iteration 48561 - loss value [[270.80716075]] accuracy 0.7272727272727273\n","Iteration 48562 - loss value [[268.79354157]] accuracy 0.7272727272727273\n","Iteration 48563 - loss value [[270.97327609]] accuracy 0.7272727272727273\n","Iteration 48564 - loss value [[269.03492306]] accuracy 0.7272727272727273\n","Iteration 48565 - loss value [[272.14150388]] accuracy 0.7272727272727273\n","Iteration 48566 - loss value [[269.67881797]] accuracy 0.7272727272727273\n","Iteration 48567 - loss value [[271.41281876]] accuracy 0.7272727272727273\n","Iteration 48568 - loss value [[269.57467052]] accuracy 0.7272727272727273\n","Iteration 48569 - loss value [[272.81947405]] accuracy 0.7272727272727273\n","Iteration 48570 - loss value [[269.94096304]] accuracy 0.7272727272727273\n","Iteration 48571 - loss value [[271.34676428]] accuracy 0.7272727272727273\n","Iteration 48572 - loss value [[268.50866074]] accuracy 0.7272727272727273\n","Iteration 48573 - loss value [[268.97259387]] accuracy 0.7272727272727273\n","Iteration 48574 - loss value [[268.1163357]] accuracy 0.7272727272727273\n","Iteration 48575 - loss value [[270.48915425]] accuracy 0.7272727272727273\n","Iteration 48576 - loss value [[268.22594001]] accuracy 0.7272727272727273\n","Iteration 48577 - loss value [[269.4591784]] accuracy 0.7272727272727273\n","Iteration 48578 - loss value [[268.80505895]] accuracy 0.7272727272727273\n","Iteration 48579 - loss value [[272.05189244]] accuracy 0.7272727272727273\n","Iteration 48580 - loss value [[269.55959523]] accuracy 0.7272727272727273\n","Iteration 48581 - loss value [[271.87770699]] accuracy 0.7272727272727273\n","Iteration 48582 - loss value [[269.73601788]] accuracy 0.7272727272727273\n","Iteration 48583 - loss value [[273.04396045]] accuracy 0.7272727272727273\n","Iteration 48584 - loss value [[270.61179245]] accuracy 0.7272727272727273\n","Iteration 48585 - loss value [[272.50729278]] accuracy 0.7272727272727273\n","Iteration 48586 - loss value [[270.29297132]] accuracy 0.7272727272727273\n","Iteration 48587 - loss value [[273.53232069]] accuracy 0.7272727272727273\n","Iteration 48588 - loss value [[271.06428105]] accuracy 0.7272727272727273\n","Iteration 48589 - loss value [[272.99021183]] accuracy 0.7272727272727273\n","Iteration 48590 - loss value [[270.59736762]] accuracy 0.7272727272727273\n","Iteration 48591 - loss value [[273.41390751]] accuracy 0.7272727272727273\n","Iteration 48592 - loss value [[270.67101623]] accuracy 0.7272727272727273\n","Iteration 48593 - loss value [[271.94716098]] accuracy 0.7272727272727273\n","Iteration 48594 - loss value [[269.95438436]] accuracy 0.7272727272727273\n","Iteration 48595 - loss value [[272.89895597]] accuracy 0.7272727272727273\n","Iteration 48596 - loss value [[270.01905197]] accuracy 0.7272727272727273\n","Iteration 48597 - loss value [[271.31586432]] accuracy 0.7272727272727273\n","Iteration 48598 - loss value [[269.21631604]] accuracy 0.7272727272727273\n","Iteration 48599 - loss value [[272.16264105]] accuracy 0.7272727272727273\n","Iteration 48600 - loss value [[269.58931412]] accuracy 0.7272727272727273\n","Iteration 48601 - loss value [[271.51261825]] accuracy 0.7272727272727273\n","Iteration 48602 - loss value [[269.17441916]] accuracy 0.7272727272727273\n","Iteration 48603 - loss value [[272.19481668]] accuracy 0.7272727272727273\n","Iteration 48604 - loss value [[269.51527017]] accuracy 0.7272727272727273\n","Iteration 48605 - loss value [[270.42862]] accuracy 0.7272727272727273\n","Iteration 48606 - loss value [[269.46505436]] accuracy 0.7272727272727273\n","Iteration 48607 - loss value [[271.82417113]] accuracy 0.7272727272727273\n","Iteration 48608 - loss value [[269.56999526]] accuracy 0.7272727272727273\n","Iteration 48609 - loss value [[271.9335113]] accuracy 0.7272727272727273\n","Iteration 48610 - loss value [[269.58645344]] accuracy 0.7272727272727273\n","Iteration 48611 - loss value [[272.42495078]] accuracy 0.7272727272727273\n","Iteration 48612 - loss value [[269.93368214]] accuracy 0.7272727272727273\n","Iteration 48613 - loss value [[272.29128456]] accuracy 0.7272727272727273\n","Iteration 48614 - loss value [[269.72682929]] accuracy 0.7272727272727273\n","Iteration 48615 - loss value [[271.67923614]] accuracy 0.7272727272727273\n","Iteration 48616 - loss value [[269.33590706]] accuracy 0.7272727272727273\n","Iteration 48617 - loss value [[270.08548963]] accuracy 0.7272727272727273\n","Iteration 48618 - loss value [[269.25212937]] accuracy 0.7272727272727273\n","Iteration 48619 - loss value [[272.22814907]] accuracy 0.7272727272727273\n","Iteration 48620 - loss value [[269.62236039]] accuracy 0.7272727272727273\n","Iteration 48621 - loss value [[271.39361173]] accuracy 0.7272727272727273\n","Iteration 48622 - loss value [[269.05939376]] accuracy 0.7272727272727273\n","Iteration 48623 - loss value [[271.26391503]] accuracy 0.7272727272727273\n","Iteration 48624 - loss value [[268.97688707]] accuracy 0.7272727272727273\n","Iteration 48625 - loss value [[269.82407321]] accuracy 0.7272727272727273\n","Iteration 48626 - loss value [[268.87878149]] accuracy 0.7272727272727273\n","Iteration 48627 - loss value [[271.3632539]] accuracy 0.7272727272727273\n","Iteration 48628 - loss value [[268.9040318]] accuracy 0.7272727272727273\n","Iteration 48629 - loss value [[270.00421226]] accuracy 0.7272727272727273\n","Iteration 48630 - loss value [[269.09290825]] accuracy 0.7272727272727273\n","Iteration 48631 - loss value [[272.08153843]] accuracy 0.7272727272727273\n","Iteration 48632 - loss value [[269.67892006]] accuracy 0.7272727272727273\n","Iteration 48633 - loss value [[272.10776321]] accuracy 0.7272727272727273\n","Iteration 48634 - loss value [[269.62170242]] accuracy 0.7272727272727273\n","Iteration 48635 - loss value [[272.32032551]] accuracy 0.7272727272727273\n","Iteration 48636 - loss value [[269.79637805]] accuracy 0.7272727272727273\n","Iteration 48637 - loss value [[271.89476204]] accuracy 0.7272727272727273\n","Iteration 48638 - loss value [[270.02002998]] accuracy 0.7272727272727273\n","Iteration 48639 - loss value [[273.11842859]] accuracy 0.7272727272727273\n","Iteration 48640 - loss value [[270.36901635]] accuracy 0.7272727272727273\n","Iteration 48641 - loss value [[271.7315325]] accuracy 0.7272727272727273\n","Iteration 48642 - loss value [[269.5600443]] accuracy 0.7272727272727273\n","Iteration 48643 - loss value [[272.08978833]] accuracy 0.7272727272727273\n","Iteration 48644 - loss value [[269.60370072]] accuracy 0.7272727272727273\n","Iteration 48645 - loss value [[271.1694223]] accuracy 0.7272727272727273\n","Iteration 48646 - loss value [[268.43720315]] accuracy 0.7272727272727273\n","Iteration 48647 - loss value [[268.80262183]] accuracy 0.7272727272727273\n","Iteration 48648 - loss value [[267.47886978]] accuracy 0.7272727272727273\n","Iteration 48649 - loss value [[268.29020005]] accuracy 0.7272727272727273\n","Iteration 48650 - loss value [[266.91302937]] accuracy 0.7272727272727273\n","Iteration 48651 - loss value [[267.16320914]] accuracy 0.7272727272727273\n","Iteration 48652 - loss value [[266.80250431]] accuracy 0.7272727272727273\n","Iteration 48653 - loss value [[268.13582354]] accuracy 0.7272727272727273\n","Iteration 48654 - loss value [[267.62483932]] accuracy 0.7272727272727273\n","Iteration 48655 - loss value [[269.77930508]] accuracy 0.7272727272727273\n","Iteration 48656 - loss value [[268.9294388]] accuracy 0.7272727272727273\n","Iteration 48657 - loss value [[272.4477357]] accuracy 0.7272727272727273\n","Iteration 48658 - loss value [[270.29619133]] accuracy 0.7272727272727273\n","Iteration 48659 - loss value [[273.34299498]] accuracy 0.7272727272727273\n","Iteration 48660 - loss value [[270.87634422]] accuracy 0.7272727272727273\n","Iteration 48661 - loss value [[273.09804024]] accuracy 0.7272727272727273\n","Iteration 48662 - loss value [[271.14447267]] accuracy 0.7272727272727273\n","Iteration 48663 - loss value [[274.81854023]] accuracy 0.7272727272727273\n","Iteration 48664 - loss value [[273.6395299]] accuracy 0.7272727272727273\n","Iteration 48665 - loss value [[277.12569893]] accuracy 0.7272727272727273\n","Iteration 48666 - loss value [[277.19523586]] accuracy 0.7272727272727273\n","Iteration 48667 - loss value [[279.50640876]] accuracy 0.7272727272727273\n","Iteration 48668 - loss value [[278.06857226]] accuracy 0.7272727272727273\n","Iteration 48669 - loss value [[280.61418312]] accuracy 0.7272727272727273\n","Iteration 48670 - loss value [[278.26008032]] accuracy 0.7272727272727273\n","Iteration 48671 - loss value [[280.50691885]] accuracy 0.7272727272727273\n","Iteration 48672 - loss value [[277.61609594]] accuracy 0.7272727272727273\n","Iteration 48673 - loss value [[278.79487662]] accuracy 0.7272727272727273\n","Iteration 48674 - loss value [[276.89663643]] accuracy 0.7272727272727273\n","Iteration 48675 - loss value [[279.58327449]] accuracy 0.7272727272727273\n","Iteration 48676 - loss value [[276.38215192]] accuracy 0.7272727272727273\n","Iteration 48677 - loss value [[276.76262238]] accuracy 0.7272727272727273\n","Iteration 48678 - loss value [[275.92209567]] accuracy 0.7272727272727273\n","Iteration 48679 - loss value [[277.4573271]] accuracy 0.7272727272727273\n","Iteration 48680 - loss value [[276.1209719]] accuracy 0.7272727272727273\n","Iteration 48681 - loss value [[278.13008417]] accuracy 0.7272727272727273\n","Iteration 48682 - loss value [[276.18055515]] accuracy 0.7272727272727273\n","Iteration 48683 - loss value [[277.02187585]] accuracy 0.7272727272727273\n","Iteration 48684 - loss value [[274.95888607]] accuracy 0.7272727272727273\n","Iteration 48685 - loss value [[276.3763758]] accuracy 0.7272727272727273\n","Iteration 48686 - loss value [[275.07454807]] accuracy 0.7272727272727273\n","Iteration 48687 - loss value [[276.43126662]] accuracy 0.7272727272727273\n","Iteration 48688 - loss value [[275.13791479]] accuracy 0.7272727272727273\n","Iteration 48689 - loss value [[276.44077784]] accuracy 0.7272727272727273\n","Iteration 48690 - loss value [[274.6044996]] accuracy 0.7272727272727273\n","Iteration 48691 - loss value [[275.27218281]] accuracy 0.7272727272727273\n","Iteration 48692 - loss value [[272.93832648]] accuracy 0.7272727272727273\n","Iteration 48693 - loss value [[273.45068161]] accuracy 0.7272727272727273\n","Iteration 48694 - loss value [[270.21785601]] accuracy 0.7272727272727273\n","Iteration 48695 - loss value [[271.04656231]] accuracy 0.7272727272727273\n","Iteration 48696 - loss value [[269.19126494]] accuracy 0.7272727272727273\n","Iteration 48697 - loss value [[269.89496815]] accuracy 0.7272727272727273\n","Iteration 48698 - loss value [[268.21474229]] accuracy 0.7272727272727273\n","Iteration 48699 - loss value [[268.16726589]] accuracy 0.7272727272727273\n","Iteration 48700 - loss value [[267.4102703]] accuracy 0.7272727272727273\n","Iteration 48701 - loss value [[268.08017136]] accuracy 0.7272727272727273\n","Iteration 48702 - loss value [[267.35656274]] accuracy 0.7272727272727273\n","Iteration 48703 - loss value [[267.48479798]] accuracy 0.7272727272727273\n","Iteration 48704 - loss value [[266.95451084]] accuracy 0.7272727272727273\n","Iteration 48705 - loss value [[267.78354481]] accuracy 0.7272727272727273\n","Iteration 48706 - loss value [[267.51572354]] accuracy 0.7272727272727273\n","Iteration 48707 - loss value [[269.31635218]] accuracy 0.7272727272727273\n","Iteration 48708 - loss value [[267.94146341]] accuracy 0.7272727272727273\n","Iteration 48709 - loss value [[268.80994994]] accuracy 0.7272727272727273\n","Iteration 48710 - loss value [[267.88684347]] accuracy 0.7272727272727273\n","Iteration 48711 - loss value [[270.2823304]] accuracy 0.7272727272727273\n","Iteration 48712 - loss value [[268.16083981]] accuracy 0.7272727272727273\n","Iteration 48713 - loss value [[269.15666459]] accuracy 0.7272727272727273\n","Iteration 48714 - loss value [[268.60086283]] accuracy 0.7272727272727273\n","Iteration 48715 - loss value [[271.44121888]] accuracy 0.7272727272727273\n","Iteration 48716 - loss value [[268.7873146]] accuracy 0.7272727272727273\n","Iteration 48717 - loss value [[270.13311852]] accuracy 0.7272727272727273\n","Iteration 48718 - loss value [[269.43330998]] accuracy 0.7272727272727273\n","Iteration 48719 - loss value [[272.55228912]] accuracy 0.7272727272727273\n","Iteration 48720 - loss value [[270.12857536]] accuracy 0.7272727272727273\n","Iteration 48721 - loss value [[271.72309293]] accuracy 0.7272727272727273\n","Iteration 48722 - loss value [[269.62180028]] accuracy 0.7272727272727273\n","Iteration 48723 - loss value [[272.63368984]] accuracy 0.7272727272727273\n","Iteration 48724 - loss value [[269.92081269]] accuracy 0.7272727272727273\n","Iteration 48725 - loss value [[271.38966326]] accuracy 0.7272727272727273\n","Iteration 48726 - loss value [[268.73869834]] accuracy 0.7272727272727273\n","Iteration 48727 - loss value [[269.46405543]] accuracy 0.7272727272727273\n","Iteration 48728 - loss value [[268.28421956]] accuracy 0.7272727272727273\n","Iteration 48729 - loss value [[270.9707127]] accuracy 0.7272727272727273\n","Iteration 48730 - loss value [[268.24188608]] accuracy 0.7272727272727273\n","Iteration 48731 - loss value [[268.91566576]] accuracy 0.7272727272727273\n","Iteration 48732 - loss value [[267.85529435]] accuracy 0.7272727272727273\n","Iteration 48733 - loss value [[270.07727281]] accuracy 0.7272727272727273\n","Iteration 48734 - loss value [[268.38104977]] accuracy 0.7272727272727273\n","Iteration 48735 - loss value [[271.21136684]] accuracy 0.7272727272727273\n","Iteration 48736 - loss value [[268.56327411]] accuracy 0.7272727272727273\n","Iteration 48737 - loss value [[268.98198325]] accuracy 0.7272727272727273\n","Iteration 48738 - loss value [[268.23279083]] accuracy 0.7272727272727273\n","Iteration 48739 - loss value [[271.02424875]] accuracy 0.7272727272727273\n","Iteration 48740 - loss value [[268.33028932]] accuracy 0.7272727272727273\n","Iteration 48741 - loss value [[268.68197038]] accuracy 0.7272727272727273\n","Iteration 48742 - loss value [[267.89926529]] accuracy 0.7272727272727273\n","Iteration 48743 - loss value [[270.66099677]] accuracy 0.7272727272727273\n","Iteration 48744 - loss value [[268.37143762]] accuracy 0.7272727272727273\n","Iteration 48745 - loss value [[269.5230117]] accuracy 0.7272727272727273\n","Iteration 48746 - loss value [[269.05060255]] accuracy 0.7272727272727273\n","Iteration 48747 - loss value [[272.33445047]] accuracy 0.7272727272727273\n","Iteration 48748 - loss value [[270.10064974]] accuracy 0.7272727272727273\n","Iteration 48749 - loss value [[272.24704573]] accuracy 0.7272727272727273\n","Iteration 48750 - loss value [[270.16117601]] accuracy 0.7272727272727273\n","Iteration 48751 - loss value [[273.45278595]] accuracy 0.7272727272727273\n","Iteration 48752 - loss value [[271.0107564]] accuracy 0.7272727272727273\n","Iteration 48753 - loss value [[273.05416671]] accuracy 0.7272727272727273\n","Iteration 48754 - loss value [[270.79068035]] accuracy 0.7272727272727273\n","Iteration 48755 - loss value [[273.2455761]] accuracy 0.7272727272727273\n","Iteration 48756 - loss value [[270.69227174]] accuracy 0.7272727272727273\n","Iteration 48757 - loss value [[272.61249714]] accuracy 0.7272727272727273\n","Iteration 48758 - loss value [[270.29883836]] accuracy 0.7272727272727273\n","Iteration 48759 - loss value [[273.35819181]] accuracy 0.7272727272727273\n","Iteration 48760 - loss value [[270.46355575]] accuracy 0.7272727272727273\n","Iteration 48761 - loss value [[271.81580964]] accuracy 0.7272727272727273\n","Iteration 48762 - loss value [[269.31900501]] accuracy 0.7272727272727273\n","Iteration 48763 - loss value [[271.93261128]] accuracy 0.7272727272727273\n","Iteration 48764 - loss value [[269.38202184]] accuracy 0.7272727272727273\n","Iteration 48765 - loss value [[270.52099891]] accuracy 0.7272727272727273\n","Iteration 48766 - loss value [[268.5209129]] accuracy 0.7272727272727273\n","Iteration 48767 - loss value [[269.28535844]] accuracy 0.7272727272727273\n","Iteration 48768 - loss value [[268.19909946]] accuracy 0.7272727272727273\n","Iteration 48769 - loss value [[271.28962478]] accuracy 0.7272727272727273\n","Iteration 48770 - loss value [[268.63683628]] accuracy 0.7272727272727273\n","Iteration 48771 - loss value [[269.00951935]] accuracy 0.7272727272727273\n","Iteration 48772 - loss value [[267.90166555]] accuracy 0.7272727272727273\n","Iteration 48773 - loss value [[270.79297086]] accuracy 0.7272727272727273\n","Iteration 48774 - loss value [[268.15043069]] accuracy 0.7272727272727273\n","Iteration 48775 - loss value [[268.26145874]] accuracy 0.7272727272727273\n","Iteration 48776 - loss value [[267.66942059]] accuracy 0.7272727272727273\n","Iteration 48777 - loss value [[270.25981502]] accuracy 0.7272727272727273\n","Iteration 48778 - loss value [[267.92475733]] accuracy 0.7272727272727273\n","Iteration 48779 - loss value [[268.0917844]] accuracy 0.7272727272727273\n","Iteration 48780 - loss value [[267.03420965]] accuracy 0.7272727272727273\n","Iteration 48781 - loss value [[268.07335166]] accuracy 0.7272727272727273\n","Iteration 48782 - loss value [[267.2937586]] accuracy 0.7272727272727273\n","Iteration 48783 - loss value [[269.67809537]] accuracy 0.7272727272727273\n","Iteration 48784 - loss value [[268.88528109]] accuracy 0.7272727272727273\n","Iteration 48785 - loss value [[272.31240128]] accuracy 0.7272727272727273\n","Iteration 48786 - loss value [[270.2325846]] accuracy 0.7272727272727273\n","Iteration 48787 - loss value [[273.42273396]] accuracy 0.7272727272727273\n","Iteration 48788 - loss value [[271.28945852]] accuracy 0.7272727272727273\n","Iteration 48789 - loss value [[275.37114092]] accuracy 0.7272727272727273\n","Iteration 48790 - loss value [[274.38247716]] accuracy 0.7272727272727273\n","Iteration 48791 - loss value [[277.62474666]] accuracy 0.7272727272727273\n","Iteration 48792 - loss value [[277.96673798]] accuracy 0.7272727272727273\n","Iteration 48793 - loss value [[280.01834105]] accuracy 0.7272727272727273\n","Iteration 48794 - loss value [[279.36596486]] accuracy 0.7272727272727273\n","Iteration 48795 - loss value [[281.15799418]] accuracy 0.7272727272727273\n","Iteration 48796 - loss value [[278.1244396]] accuracy 0.7272727272727273\n","Iteration 48797 - loss value [[279.18562854]] accuracy 0.7272727272727273\n","Iteration 48798 - loss value [[278.37580785]] accuracy 0.7272727272727273\n","Iteration 48799 - loss value [[280.20594845]] accuracy 0.7272727272727273\n","Iteration 48800 - loss value [[278.16730957]] accuracy 0.7272727272727273\n","Iteration 48801 - loss value [[279.67506711]] accuracy 0.7272727272727273\n","Iteration 48802 - loss value [[276.75011311]] accuracy 0.7272727272727273\n","Iteration 48803 - loss value [[277.02238654]] accuracy 0.7272727272727273\n","Iteration 48804 - loss value [[276.04141734]] accuracy 0.7272727272727273\n","Iteration 48805 - loss value [[278.01724243]] accuracy 0.7272727272727273\n","Iteration 48806 - loss value [[276.73398906]] accuracy 0.7272727272727273\n","Iteration 48807 - loss value [[278.39458181]] accuracy 0.7272727272727273\n","Iteration 48808 - loss value [[276.30936675]] accuracy 0.7272727272727273\n","Iteration 48809 - loss value [[277.34954597]] accuracy 0.7272727272727273\n","Iteration 48810 - loss value [[275.04761541]] accuracy 0.7272727272727273\n","Iteration 48811 - loss value [[276.29411382]] accuracy 0.7272727272727273\n","Iteration 48812 - loss value [[274.79834808]] accuracy 0.7272727272727273\n","Iteration 48813 - loss value [[275.96586358]] accuracy 0.7272727272727273\n","Iteration 48814 - loss value [[273.66815606]] accuracy 0.7272727272727273\n","Iteration 48815 - loss value [[275.17031162]] accuracy 0.7272727272727273\n","Iteration 48816 - loss value [[272.66140308]] accuracy 0.7272727272727273\n","Iteration 48817 - loss value [[273.24794632]] accuracy 0.7272727272727273\n","Iteration 48818 - loss value [[269.98888421]] accuracy 0.7272727272727273\n","Iteration 48819 - loss value [[271.38132173]] accuracy 0.7272727272727273\n","Iteration 48820 - loss value [[268.82173139]] accuracy 0.7272727272727273\n","Iteration 48821 - loss value [[269.55930712]] accuracy 0.7272727272727273\n","Iteration 48822 - loss value [[267.90559694]] accuracy 0.7272727272727273\n","Iteration 48823 - loss value [[268.04374453]] accuracy 0.7272727272727273\n","Iteration 48824 - loss value [[267.20777434]] accuracy 0.7272727272727273\n","Iteration 48825 - loss value [[267.85228309]] accuracy 0.7272727272727273\n","Iteration 48826 - loss value [[267.42823017]] accuracy 0.7272727272727273\n","Iteration 48827 - loss value [[268.17344692]] accuracy 0.7272727272727273\n","Iteration 48828 - loss value [[267.73518182]] accuracy 0.7272727272727273\n","Iteration 48829 - loss value [[270.27603551]] accuracy 0.7272727272727273\n","Iteration 48830 - loss value [[268.21312145]] accuracy 0.7272727272727273\n","Iteration 48831 - loss value [[268.81499003]] accuracy 0.7272727272727273\n","Iteration 48832 - loss value [[267.60992914]] accuracy 0.7272727272727273\n","Iteration 48833 - loss value [[269.10745197]] accuracy 0.7272727272727273\n","Iteration 48834 - loss value [[268.23261551]] accuracy 0.7272727272727273\n","Iteration 48835 - loss value [[270.85749432]] accuracy 0.7272727272727273\n","Iteration 48836 - loss value [[268.3089389]] accuracy 0.7272727272727273\n","Iteration 48837 - loss value [[269.55679666]] accuracy 0.7272727272727273\n","Iteration 48838 - loss value [[269.13132394]] accuracy 0.7272727272727273\n","Iteration 48839 - loss value [[272.16729395]] accuracy 0.7272727272727273\n","Iteration 48840 - loss value [[269.60177308]] accuracy 0.7272727272727273\n","Iteration 48841 - loss value [[270.89839793]] accuracy 0.7272727272727273\n","Iteration 48842 - loss value [[269.47031411]] accuracy 0.7272727272727273\n","Iteration 48843 - loss value [[272.64137815]] accuracy 0.7272727272727273\n","Iteration 48844 - loss value [[270.10808148]] accuracy 0.7272727272727273\n","Iteration 48845 - loss value [[271.96500834]] accuracy 0.7272727272727273\n","Iteration 48846 - loss value [[269.69223938]] accuracy 0.7272727272727273\n","Iteration 48847 - loss value [[272.76727943]] accuracy 0.7272727272727273\n","Iteration 48848 - loss value [[269.78815121]] accuracy 0.7272727272727273\n","Iteration 48849 - loss value [[271.02456503]] accuracy 0.7272727272727273\n","Iteration 48850 - loss value [[268.91322217]] accuracy 0.7272727272727273\n","Iteration 48851 - loss value [[270.94924272]] accuracy 0.7272727272727273\n","Iteration 48852 - loss value [[268.30777575]] accuracy 0.7272727272727273\n","Iteration 48853 - loss value [[268.19400564]] accuracy 0.7272727272727273\n","Iteration 48854 - loss value [[266.99769637]] accuracy 0.7272727272727273\n","Iteration 48855 - loss value [[267.6239643]] accuracy 0.7272727272727273\n","Iteration 48856 - loss value [[266.36463543]] accuracy 0.7272727272727273\n","Iteration 48857 - loss value [[266.68009088]] accuracy 0.7272727272727273\n","Iteration 48858 - loss value [[266.13377901]] accuracy 0.7272727272727273\n","Iteration 48859 - loss value [[266.92189684]] accuracy 0.7272727272727273\n","Iteration 48860 - loss value [[266.52671641]] accuracy 0.7272727272727273\n","Iteration 48861 - loss value [[267.8416229]] accuracy 0.7272727272727273\n","Iteration 48862 - loss value [[267.751932]] accuracy 0.7272727272727273\n","Iteration 48863 - loss value [[270.98197082]] accuracy 0.7272727272727273\n","Iteration 48864 - loss value [[268.76460701]] accuracy 0.7272727272727273\n","Iteration 48865 - loss value [[270.58471413]] accuracy 0.7272727272727273\n","Iteration 48866 - loss value [[269.37059661]] accuracy 0.7272727272727273\n","Iteration 48867 - loss value [[272.6825733]] accuracy 0.7272727272727273\n","Iteration 48868 - loss value [[270.40496872]] accuracy 0.7272727272727273\n","Iteration 48869 - loss value [[273.99998725]] accuracy 0.7272727272727273\n","Iteration 48870 - loss value [[271.67407973]] accuracy 0.7272727272727273\n","Iteration 48871 - loss value [[275.57778941]] accuracy 0.7272727272727273\n","Iteration 48872 - loss value [[274.52615347]] accuracy 0.7272727272727273\n","Iteration 48873 - loss value [[276.92457413]] accuracy 0.7272727272727273\n","Iteration 48874 - loss value [[276.78960008]] accuracy 0.7272727272727273\n","Iteration 48875 - loss value [[278.20702002]] accuracy 0.7272727272727273\n","Iteration 48876 - loss value [[278.1008343]] accuracy 0.7272727272727273\n","Iteration 48877 - loss value [[280.06289002]] accuracy 0.7272727272727273\n","Iteration 48878 - loss value [[278.94479213]] accuracy 0.7272727272727273\n","Iteration 48879 - loss value [[280.43098608]] accuracy 0.7272727272727273\n","Iteration 48880 - loss value [[278.65870447]] accuracy 0.7272727272727273\n","Iteration 48881 - loss value [[280.0287342]] accuracy 0.7272727272727273\n","Iteration 48882 - loss value [[278.31546467]] accuracy 0.7272727272727273\n","Iteration 48883 - loss value [[279.61746263]] accuracy 0.7272727272727273\n","Iteration 48884 - loss value [[277.46504119]] accuracy 0.7272727272727273\n","Iteration 48885 - loss value [[278.07030806]] accuracy 0.7272727272727273\n","Iteration 48886 - loss value [[277.16993918]] accuracy 0.7272727272727273\n","Iteration 48887 - loss value [[277.6816984]] accuracy 0.7272727272727273\n","Iteration 48888 - loss value [[275.97655708]] accuracy 0.7272727272727273\n","Iteration 48889 - loss value [[276.78154859]] accuracy 0.7272727272727273\n","Iteration 48890 - loss value [[274.95103579]] accuracy 0.7272727272727273\n","Iteration 48891 - loss value [[275.63397807]] accuracy 0.7272727272727273\n","Iteration 48892 - loss value [[273.32505867]] accuracy 0.7272727272727273\n","Iteration 48893 - loss value [[274.77160131]] accuracy 0.7272727272727273\n","Iteration 48894 - loss value [[272.01889912]] accuracy 0.7272727272727273\n","Iteration 48895 - loss value [[271.97779317]] accuracy 0.7272727272727273\n","Iteration 48896 - loss value [[269.05357135]] accuracy 0.7272727272727273\n","Iteration 48897 - loss value [[269.19818614]] accuracy 0.7272727272727273\n","Iteration 48898 - loss value [[268.15803533]] accuracy 0.7272727272727273\n","Iteration 48899 - loss value [[270.46831123]] accuracy 0.7272727272727273\n","Iteration 48900 - loss value [[267.82589418]] accuracy 0.7272727272727273\n","Iteration 48901 - loss value [[267.46229098]] accuracy 0.7272727272727273\n","Iteration 48902 - loss value [[266.66836759]] accuracy 0.7272727272727273\n","Iteration 48903 - loss value [[267.28028196]] accuracy 0.7272727272727273\n","Iteration 48904 - loss value [[266.62647719]] accuracy 0.7272727272727273\n","Iteration 48905 - loss value [[266.87177219]] accuracy 0.7272727272727273\n","Iteration 48906 - loss value [[266.36161313]] accuracy 0.7272727272727273\n","Iteration 48907 - loss value [[267.03198982]] accuracy 0.7272727272727273\n","Iteration 48908 - loss value [[266.64068071]] accuracy 0.7272727272727273\n","Iteration 48909 - loss value [[267.26707368]] accuracy 0.7272727272727273\n","Iteration 48910 - loss value [[266.75353798]] accuracy 0.7272727272727273\n","Iteration 48911 - loss value [[267.13362902]] accuracy 0.7272727272727273\n","Iteration 48912 - loss value [[266.85624642]] accuracy 0.7272727272727273\n","Iteration 48913 - loss value [[268.93181798]] accuracy 0.7272727272727273\n","Iteration 48914 - loss value [[268.39585229]] accuracy 0.7272727272727273\n","Iteration 48915 - loss value [[271.25208311]] accuracy 0.7272727272727273\n","Iteration 48916 - loss value [[268.8764687]] accuracy 0.7272727272727273\n","Iteration 48917 - loss value [[272.32018826]] accuracy 0.7272727272727273\n","Iteration 48918 - loss value [[269.5233239]] accuracy 0.7272727272727273\n","Iteration 48919 - loss value [[270.85510814]] accuracy 0.7272727272727273\n","Iteration 48920 - loss value [[269.34605587]] accuracy 0.7272727272727273\n","Iteration 48921 - loss value [[272.45923273]] accuracy 0.7272727272727273\n","Iteration 48922 - loss value [[269.800271]] accuracy 0.7272727272727273\n","Iteration 48923 - loss value [[271.38507811]] accuracy 0.7272727272727273\n","Iteration 48924 - loss value [[269.16919061]] accuracy 0.7272727272727273\n","Iteration 48925 - loss value [[272.17343616]] accuracy 0.7272727272727273\n","Iteration 48926 - loss value [[269.49319958]] accuracy 0.7272727272727273\n","Iteration 48927 - loss value [[271.55164638]] accuracy 0.7272727272727273\n","Iteration 48928 - loss value [[268.94262656]] accuracy 0.7272727272727273\n","Iteration 48929 - loss value [[270.94533741]] accuracy 0.7272727272727273\n","Iteration 48930 - loss value [[268.26735362]] accuracy 0.7272727272727273\n","Iteration 48931 - loss value [[268.63317776]] accuracy 0.7272727272727273\n","Iteration 48932 - loss value [[267.89735929]] accuracy 0.7272727272727273\n","Iteration 48933 - loss value [[270.7121451]] accuracy 0.7272727272727273\n","Iteration 48934 - loss value [[268.28513558]] accuracy 0.7272727272727273\n","Iteration 48935 - loss value [[269.03438247]] accuracy 0.7272727272727273\n","Iteration 48936 - loss value [[268.4792637]] accuracy 0.7272727272727273\n","Iteration 48937 - loss value [[271.35507386]] accuracy 0.7272727272727273\n","Iteration 48938 - loss value [[268.61415046]] accuracy 0.7272727272727273\n","Iteration 48939 - loss value [[269.60924031]] accuracy 0.7272727272727273\n","Iteration 48940 - loss value [[269.57307636]] accuracy 0.7272727272727273\n","Iteration 48941 - loss value [[271.78077665]] accuracy 0.7272727272727273\n","Iteration 48942 - loss value [[269.58215239]] accuracy 0.7272727272727273\n","Iteration 48943 - loss value [[272.85587678]] accuracy 0.7272727272727273\n","Iteration 48944 - loss value [[270.49009274]] accuracy 0.7272727272727273\n","Iteration 48945 - loss value [[273.57080858]] accuracy 0.7272727272727273\n","Iteration 48946 - loss value [[271.24785916]] accuracy 0.7272727272727273\n","Iteration 48947 - loss value [[274.93473287]] accuracy 0.7272727272727273\n","Iteration 48948 - loss value [[273.67807778]] accuracy 0.7272727272727273\n","Iteration 48949 - loss value [[276.63716997]] accuracy 0.7272727272727273\n","Iteration 48950 - loss value [[276.04654235]] accuracy 0.7272727272727273\n","Iteration 48951 - loss value [[278.51755697]] accuracy 0.7272727272727273\n","Iteration 48952 - loss value [[277.79485157]] accuracy 0.7272727272727273\n","Iteration 48953 - loss value [[279.7647882]] accuracy 0.7272727272727273\n","Iteration 48954 - loss value [[278.18791206]] accuracy 0.7272727272727273\n","Iteration 48955 - loss value [[279.86796313]] accuracy 0.7272727272727273\n","Iteration 48956 - loss value [[278.4690849]] accuracy 0.7272727272727273\n","Iteration 48957 - loss value [[279.95253803]] accuracy 0.7272727272727273\n","Iteration 48958 - loss value [[277.97007444]] accuracy 0.7272727272727273\n","Iteration 48959 - loss value [[278.82392111]] accuracy 0.7272727272727273\n","Iteration 48960 - loss value [[276.54127096]] accuracy 0.7272727272727273\n","Iteration 48961 - loss value [[277.85957808]] accuracy 0.7272727272727273\n","Iteration 48962 - loss value [[276.01549068]] accuracy 0.7272727272727273\n","Iteration 48963 - loss value [[276.9282851]] accuracy 0.7272727272727273\n","Iteration 48964 - loss value [[275.35229093]] accuracy 0.7272727272727273\n","Iteration 48965 - loss value [[275.98267352]] accuracy 0.7272727272727273\n","Iteration 48966 - loss value [[273.73140932]] accuracy 0.7272727272727273\n","Iteration 48967 - loss value [[274.72943682]] accuracy 0.7272727272727273\n","Iteration 48968 - loss value [[271.96940647]] accuracy 0.7272727272727273\n","Iteration 48969 - loss value [[272.43977425]] accuracy 0.7272727272727273\n","Iteration 48970 - loss value [[269.4296726]] accuracy 0.7272727272727273\n","Iteration 48971 - loss value [[270.07638182]] accuracy 0.7272727272727273\n","Iteration 48972 - loss value [[268.47233059]] accuracy 0.7272727272727273\n","Iteration 48973 - loss value [[269.52025676]] accuracy 0.7272727272727273\n","Iteration 48974 - loss value [[267.98043469]] accuracy 0.7272727272727273\n","Iteration 48975 - loss value [[268.28869528]] accuracy 0.7272727272727273\n","Iteration 48976 - loss value [[266.88184794]] accuracy 0.7272727272727273\n","Iteration 48977 - loss value [[266.95639177]] accuracy 0.7272727272727273\n","Iteration 48978 - loss value [[266.27260379]] accuracy 0.7272727272727273\n","Iteration 48979 - loss value [[266.8326791]] accuracy 0.7272727272727273\n","Iteration 48980 - loss value [[266.21225152]] accuracy 0.7272727272727273\n","Iteration 48981 - loss value [[266.09918532]] accuracy 0.7272727272727273\n","Iteration 48982 - loss value [[265.29824902]] accuracy 0.7272727272727273\n","Iteration 48983 - loss value [[266.17521929]] accuracy 0.7272727272727273\n","Iteration 48984 - loss value [[265.4677073]] accuracy 0.7272727272727273\n","Iteration 48985 - loss value [[266.28252375]] accuracy 0.7272727272727273\n","Iteration 48986 - loss value [[265.41361012]] accuracy 0.7272727272727273\n","Iteration 48987 - loss value [[265.8126903]] accuracy 0.7272727272727273\n","Iteration 48988 - loss value [[264.97400987]] accuracy 0.7272727272727273\n","Iteration 48989 - loss value [[265.76316174]] accuracy 0.7272727272727273\n","Iteration 48990 - loss value [[265.2814481]] accuracy 0.7272727272727273\n","Iteration 48991 - loss value [[266.48527019]] accuracy 0.7272727272727273\n","Iteration 48992 - loss value [[266.48485476]] accuracy 0.7272727272727273\n","Iteration 48993 - loss value [[268.58568553]] accuracy 0.7272727272727273\n","Iteration 48994 - loss value [[269.21074261]] accuracy 0.7272727272727273\n","Iteration 48995 - loss value [[272.14894736]] accuracy 0.7272727272727273\n","Iteration 48996 - loss value [[269.67489558]] accuracy 0.7272727272727273\n","Iteration 48997 - loss value [[273.3073464]] accuracy 0.7272727272727273\n","Iteration 48998 - loss value [[270.49964174]] accuracy 0.7272727272727273\n","Iteration 48999 - loss value [[273.07384457]] accuracy 0.7272727272727273\n","Iteration 49000 - loss value [[270.56762788]] accuracy 0.7272727272727273\n","Iteration 49001 - loss value [[273.53263541]] accuracy 0.7272727272727273\n","Iteration 49002 - loss value [[271.06427057]] accuracy 0.7272727272727273\n","Iteration 49003 - loss value [[273.92841873]] accuracy 0.7272727272727273\n","Iteration 49004 - loss value [[271.45629316]] accuracy 0.7272727272727273\n","Iteration 49005 - loss value [[273.66859454]] accuracy 0.7272727272727273\n","Iteration 49006 - loss value [[271.61586599]] accuracy 0.7272727272727273\n","Iteration 49007 - loss value [[272.72601117]] accuracy 0.7272727272727273\n","Iteration 49008 - loss value [[270.6618008]] accuracy 0.7272727272727273\n","Iteration 49009 - loss value [[273.19068789]] accuracy 0.7272727272727273\n","Iteration 49010 - loss value [[270.26395207]] accuracy 0.7272727272727273\n","Iteration 49011 - loss value [[272.33416491]] accuracy 0.7272727272727273\n","Iteration 49012 - loss value [[269.81792724]] accuracy 0.7272727272727273\n","Iteration 49013 - loss value [[272.83079833]] accuracy 0.7272727272727273\n","Iteration 49014 - loss value [[269.99488198]] accuracy 0.7272727272727273\n","Iteration 49015 - loss value [[270.79429861]] accuracy 0.7272727272727273\n","Iteration 49016 - loss value [[268.60284853]] accuracy 0.7272727272727273\n","Iteration 49017 - loss value [[269.78074096]] accuracy 0.7272727272727273\n","Iteration 49018 - loss value [[268.87652964]] accuracy 0.7272727272727273\n","Iteration 49019 - loss value [[271.80534636]] accuracy 0.7272727272727273\n","Iteration 49020 - loss value [[268.96126397]] accuracy 0.7272727272727273\n","Iteration 49021 - loss value [[269.70923647]] accuracy 0.7272727272727273\n","Iteration 49022 - loss value [[268.99202152]] accuracy 0.7272727272727273\n","Iteration 49023 - loss value [[271.40691056]] accuracy 0.7272727272727273\n","Iteration 49024 - loss value [[268.70727131]] accuracy 0.7272727272727273\n","Iteration 49025 - loss value [[270.28971243]] accuracy 0.7272727272727273\n","Iteration 49026 - loss value [[268.47847413]] accuracy 0.7272727272727273\n","Iteration 49027 - loss value [[270.96946564]] accuracy 0.7272727272727273\n","Iteration 49028 - loss value [[268.20796194]] accuracy 0.7272727272727273\n","Iteration 49029 - loss value [[268.68371675]] accuracy 0.7272727272727273\n","Iteration 49030 - loss value [[267.91716618]] accuracy 0.7272727272727273\n","Iteration 49031 - loss value [[270.68002066]] accuracy 0.7272727272727273\n","Iteration 49032 - loss value [[268.37725368]] accuracy 0.7272727272727273\n","Iteration 49033 - loss value [[269.24682742]] accuracy 0.7272727272727273\n","Iteration 49034 - loss value [[268.28726407]] accuracy 0.7272727272727273\n","Iteration 49035 - loss value [[271.37755128]] accuracy 0.7272727272727273\n","Iteration 49036 - loss value [[268.71324154]] accuracy 0.7272727272727273\n","Iteration 49037 - loss value [[269.56883625]] accuracy 0.7272727272727273\n","Iteration 49038 - loss value [[268.96970511]] accuracy 0.7272727272727273\n","Iteration 49039 - loss value [[271.97727152]] accuracy 0.7272727272727273\n","Iteration 49040 - loss value [[269.546345]] accuracy 0.7272727272727273\n","Iteration 49041 - loss value [[272.49061715]] accuracy 0.7272727272727273\n","Iteration 49042 - loss value [[269.79459163]] accuracy 0.7272727272727273\n","Iteration 49043 - loss value [[271.20070175]] accuracy 0.7272727272727273\n","Iteration 49044 - loss value [[269.45957591]] accuracy 0.7272727272727273\n","Iteration 49045 - loss value [[272.46417465]] accuracy 0.7272727272727273\n","Iteration 49046 - loss value [[269.68729267]] accuracy 0.7272727272727273\n","Iteration 49047 - loss value [[271.73078722]] accuracy 0.7272727272727273\n","Iteration 49048 - loss value [[269.35076401]] accuracy 0.7272727272727273\n","Iteration 49049 - loss value [[272.3608768]] accuracy 0.7272727272727273\n","Iteration 49050 - loss value [[269.44542023]] accuracy 0.7272727272727273\n","Iteration 49051 - loss value [[269.48178674]] accuracy 0.7272727272727273\n","Iteration 49052 - loss value [[268.29082693]] accuracy 0.7272727272727273\n","Iteration 49053 - loss value [[271.38583916]] accuracy 0.7272727272727273\n","Iteration 49054 - loss value [[268.71423598]] accuracy 0.7272727272727273\n","Iteration 49055 - loss value [[269.5212917]] accuracy 0.7272727272727273\n","Iteration 49056 - loss value [[268.82997835]] accuracy 0.7272727272727273\n","Iteration 49057 - loss value [[271.86432728]] accuracy 0.7272727272727273\n","Iteration 49058 - loss value [[269.36218684]] accuracy 0.7272727272727273\n","Iteration 49059 - loss value [[271.91993238]] accuracy 0.7272727272727273\n","Iteration 49060 - loss value [[269.35267165]] accuracy 0.7272727272727273\n","Iteration 49061 - loss value [[271.67247024]] accuracy 0.7272727272727273\n","Iteration 49062 - loss value [[269.26631795]] accuracy 0.7272727272727273\n","Iteration 49063 - loss value [[270.87059921]] accuracy 0.7272727272727273\n","Iteration 49064 - loss value [[269.24437415]] accuracy 0.7272727272727273\n","Iteration 49065 - loss value [[272.25722187]] accuracy 0.7272727272727273\n","Iteration 49066 - loss value [[269.65979008]] accuracy 0.7272727272727273\n","Iteration 49067 - loss value [[271.75278544]] accuracy 0.7272727272727273\n","Iteration 49068 - loss value [[269.3488497]] accuracy 0.7272727272727273\n","Iteration 49069 - loss value [[272.44856962]] accuracy 0.7272727272727273\n","Iteration 49070 - loss value [[269.56139876]] accuracy 0.7272727272727273\n","Iteration 49071 - loss value [[270.16507323]] accuracy 0.7272727272727273\n","Iteration 49072 - loss value [[269.25885138]] accuracy 0.7272727272727273\n","Iteration 49073 - loss value [[272.32526236]] accuracy 0.7272727272727273\n","Iteration 49074 - loss value [[269.70714364]] accuracy 0.7272727272727273\n","Iteration 49075 - loss value [[271.81539084]] accuracy 0.7272727272727273\n","Iteration 49076 - loss value [[269.42882916]] accuracy 0.7272727272727273\n","Iteration 49077 - loss value [[272.01525833]] accuracy 0.7272727272727273\n","Iteration 49078 - loss value [[269.32514356]] accuracy 0.7272727272727273\n","Iteration 49079 - loss value [[270.5705563]] accuracy 0.7272727272727273\n","Iteration 49080 - loss value [[268.50325518]] accuracy 0.7272727272727273\n","Iteration 49081 - loss value [[269.78524809]] accuracy 0.7272727272727273\n","Iteration 49082 - loss value [[268.31265264]] accuracy 0.7272727272727273\n","Iteration 49083 - loss value [[271.40098177]] accuracy 0.7272727272727273\n","Iteration 49084 - loss value [[268.73221531]] accuracy 0.7272727272727273\n","Iteration 49085 - loss value [[269.24357221]] accuracy 0.7272727272727273\n","Iteration 49086 - loss value [[268.36760725]] accuracy 0.7272727272727273\n","Iteration 49087 - loss value [[271.49121476]] accuracy 0.7272727272727273\n","Iteration 49088 - loss value [[268.95279518]] accuracy 0.7272727272727273\n","Iteration 49089 - loss value [[271.29295933]] accuracy 0.7272727272727273\n","Iteration 49090 - loss value [[268.88920689]] accuracy 0.7272727272727273\n","Iteration 49091 - loss value [[271.04385564]] accuracy 0.7272727272727273\n","Iteration 49092 - loss value [[268.68444169]] accuracy 0.7272727272727273\n","Iteration 49093 - loss value [[270.46305648]] accuracy 0.7272727272727273\n","Iteration 49094 - loss value [[268.408615]] accuracy 0.7272727272727273\n","Iteration 49095 - loss value [[270.3400013]] accuracy 0.7272727272727273\n","Iteration 49096 - loss value [[268.58661326]] accuracy 0.7272727272727273\n","Iteration 49097 - loss value [[271.29082955]] accuracy 0.7272727272727273\n","Iteration 49098 - loss value [[268.73706625]] accuracy 0.7272727272727273\n","Iteration 49099 - loss value [[269.28620555]] accuracy 0.7272727272727273\n","Iteration 49100 - loss value [[268.77372343]] accuracy 0.7272727272727273\n","Iteration 49101 - loss value [[271.9853359]] accuracy 0.7272727272727273\n","Iteration 49102 - loss value [[269.64874825]] accuracy 0.7272727272727273\n","Iteration 49103 - loss value [[272.09396991]] accuracy 0.7272727272727273\n","Iteration 49104 - loss value [[269.89095648]] accuracy 0.7272727272727273\n","Iteration 49105 - loss value [[272.43445176]] accuracy 0.7272727272727273\n","Iteration 49106 - loss value [[270.3255728]] accuracy 0.7272727272727273\n","Iteration 49107 - loss value [[273.92148218]] accuracy 0.7272727272727273\n","Iteration 49108 - loss value [[272.58248009]] accuracy 0.7272727272727273\n","Iteration 49109 - loss value [[276.50925269]] accuracy 0.7272727272727273\n","Iteration 49110 - loss value [[276.54404079]] accuracy 0.7272727272727273\n","Iteration 49111 - loss value [[279.69592687]] accuracy 0.7272727272727273\n","Iteration 49112 - loss value [[277.87623258]] accuracy 0.7272727272727273\n","Iteration 49113 - loss value [[280.71568057]] accuracy 0.7272727272727273\n","Iteration 49114 - loss value [[278.00929629]] accuracy 0.7272727272727273\n","Iteration 49115 - loss value [[281.32113003]] accuracy 0.7272727272727273\n","Iteration 49116 - loss value [[278.11171053]] accuracy 0.7272727272727273\n","Iteration 49117 - loss value [[278.87518206]] accuracy 0.7272727272727273\n","Iteration 49118 - loss value [[276.8387857]] accuracy 0.7272727272727273\n","Iteration 49119 - loss value [[278.43108858]] accuracy 0.7272727272727273\n","Iteration 49120 - loss value [[276.81869834]] accuracy 0.7272727272727273\n","Iteration 49121 - loss value [[278.61030908]] accuracy 0.7272727272727273\n","Iteration 49122 - loss value [[277.02142906]] accuracy 0.7272727272727273\n","Iteration 49123 - loss value [[278.36643982]] accuracy 0.7272727272727273\n","Iteration 49124 - loss value [[276.22027164]] accuracy 0.7272727272727273\n","Iteration 49125 - loss value [[277.66887153]] accuracy 0.7272727272727273\n","Iteration 49126 - loss value [[275.14539858]] accuracy 0.7272727272727273\n","Iteration 49127 - loss value [[276.95381615]] accuracy 0.7272727272727273\n","Iteration 49128 - loss value [[274.67368631]] accuracy 0.7272727272727273\n","Iteration 49129 - loss value [[275.96011613]] accuracy 0.7272727272727273\n","Iteration 49130 - loss value [[273.72443402]] accuracy 0.7272727272727273\n","Iteration 49131 - loss value [[275.17688403]] accuracy 0.7272727272727273\n","Iteration 49132 - loss value [[272.43268107]] accuracy 0.7272727272727273\n","Iteration 49133 - loss value [[273.39679436]] accuracy 0.7272727272727273\n","Iteration 49134 - loss value [[270.30348789]] accuracy 0.7272727272727273\n","Iteration 49135 - loss value [[270.81701538]] accuracy 0.7272727272727273\n","Iteration 49136 - loss value [[269.03940742]] accuracy 0.7272727272727273\n","Iteration 49137 - loss value [[270.33345002]] accuracy 0.7272727272727273\n","Iteration 49138 - loss value [[268.45976682]] accuracy 0.7272727272727273\n","Iteration 49139 - loss value [[269.30321737]] accuracy 0.7272727272727273\n","Iteration 49140 - loss value [[267.621393]] accuracy 0.7272727272727273\n","Iteration 49141 - loss value [[267.4591486]] accuracy 0.7272727272727273\n","Iteration 49142 - loss value [[267.24085379]] accuracy 0.7272727272727273\n","Iteration 49143 - loss value [[267.63014875]] accuracy 0.7272727272727273\n","Iteration 49144 - loss value [[267.36219408]] accuracy 0.7272727272727273\n","Iteration 49145 - loss value [[268.42857082]] accuracy 0.7272727272727273\n","Iteration 49146 - loss value [[267.35006727]] accuracy 0.7272727272727273\n","Iteration 49147 - loss value [[267.89606922]] accuracy 0.7272727272727273\n","Iteration 49148 - loss value [[267.53775019]] accuracy 0.7272727272727273\n","Iteration 49149 - loss value [[269.08492637]] accuracy 0.7272727272727273\n","Iteration 49150 - loss value [[268.85321135]] accuracy 0.7272727272727273\n","Iteration 49151 - loss value [[271.62230056]] accuracy 0.7272727272727273\n","Iteration 49152 - loss value [[268.80538821]] accuracy 0.7272727272727273\n","Iteration 49153 - loss value [[269.74892245]] accuracy 0.7272727272727273\n","Iteration 49154 - loss value [[269.14924086]] accuracy 0.7272727272727273\n","Iteration 49155 - loss value [[272.18901124]] accuracy 0.7272727272727273\n","Iteration 49156 - loss value [[269.65733339]] accuracy 0.7272727272727273\n","Iteration 49157 - loss value [[271.19159137]] accuracy 0.7272727272727273\n","Iteration 49158 - loss value [[269.06258828]] accuracy 0.7272727272727273\n","Iteration 49159 - loss value [[272.22446251]] accuracy 0.7272727272727273\n","Iteration 49160 - loss value [[269.26493585]] accuracy 0.7272727272727273\n","Iteration 49161 - loss value [[270.22130445]] accuracy 0.7272727272727273\n","Iteration 49162 - loss value [[269.59673498]] accuracy 0.7272727272727273\n","Iteration 49163 - loss value [[271.7555852]] accuracy 0.7272727272727273\n","Iteration 49164 - loss value [[269.09791716]] accuracy 0.7272727272727273\n","Iteration 49165 - loss value [[270.89955449]] accuracy 0.7272727272727273\n","Iteration 49166 - loss value [[268.69529225]] accuracy 0.7272727272727273\n","Iteration 49167 - loss value [[270.52233451]] accuracy 0.7272727272727273\n","Iteration 49168 - loss value [[268.54595041]] accuracy 0.7272727272727273\n","Iteration 49169 - loss value [[270.32361141]] accuracy 0.7272727272727273\n","Iteration 49170 - loss value [[268.64046819]] accuracy 0.7272727272727273\n","Iteration 49171 - loss value [[271.49627072]] accuracy 0.7272727272727273\n","Iteration 49172 - loss value [[268.70039711]] accuracy 0.7272727272727273\n","Iteration 49173 - loss value [[269.07170231]] accuracy 0.7272727272727273\n","Iteration 49174 - loss value [[267.92843549]] accuracy 0.7272727272727273\n","Iteration 49175 - loss value [[270.390562]] accuracy 0.7272727272727273\n","Iteration 49176 - loss value [[268.68212528]] accuracy 0.7272727272727273\n","Iteration 49177 - loss value [[271.65555308]] accuracy 0.7272727272727273\n","Iteration 49178 - loss value [[268.78817212]] accuracy 0.7272727272727273\n","Iteration 49179 - loss value [[269.41565306]] accuracy 0.7272727272727273\n","Iteration 49180 - loss value [[268.620755]] accuracy 0.7272727272727273\n","Iteration 49181 - loss value [[271.67132714]] accuracy 0.7272727272727273\n","Iteration 49182 - loss value [[268.82940234]] accuracy 0.7272727272727273\n","Iteration 49183 - loss value [[269.87195039]] accuracy 0.7272727272727273\n","Iteration 49184 - loss value [[269.26809074]] accuracy 0.7272727272727273\n","Iteration 49185 - loss value [[271.53387407]] accuracy 0.7272727272727273\n","Iteration 49186 - loss value [[268.80114133]] accuracy 0.7272727272727273\n","Iteration 49187 - loss value [[270.63797204]] accuracy 0.7272727272727273\n","Iteration 49188 - loss value [[269.24036511]] accuracy 0.7272727272727273\n","Iteration 49189 - loss value [[272.30656531]] accuracy 0.7272727272727273\n","Iteration 49190 - loss value [[269.79915249]] accuracy 0.7272727272727273\n","Iteration 49191 - loss value [[272.02396166]] accuracy 0.7272727272727273\n","Iteration 49192 - loss value [[269.55194516]] accuracy 0.7272727272727273\n","Iteration 49193 - loss value [[271.59344659]] accuracy 0.7272727272727273\n","Iteration 49194 - loss value [[269.14123615]] accuracy 0.7272727272727273\n","Iteration 49195 - loss value [[271.46484773]] accuracy 0.7272727272727273\n","Iteration 49196 - loss value [[268.92277777]] accuracy 0.7272727272727273\n","Iteration 49197 - loss value [[269.75482388]] accuracy 0.7272727272727273\n","Iteration 49198 - loss value [[269.38889104]] accuracy 0.7272727272727273\n","Iteration 49199 - loss value [[271.7774694]] accuracy 0.7272727272727273\n","Iteration 49200 - loss value [[269.26000485]] accuracy 0.7272727272727273\n","Iteration 49201 - loss value [[271.28616187]] accuracy 0.7272727272727273\n","Iteration 49202 - loss value [[268.83263628]] accuracy 0.7272727272727273\n","Iteration 49203 - loss value [[270.28166098]] accuracy 0.7272727272727273\n","Iteration 49204 - loss value [[268.13766933]] accuracy 0.7272727272727273\n","Iteration 49205 - loss value [[269.39990774]] accuracy 0.7272727272727273\n","Iteration 49206 - loss value [[269.52451966]] accuracy 0.7272727272727273\n","Iteration 49207 - loss value [[272.73112645]] accuracy 0.7272727272727273\n","Iteration 49208 - loss value [[269.91889024]] accuracy 0.7272727272727273\n","Iteration 49209 - loss value [[272.60688386]] accuracy 0.7272727272727273\n","Iteration 49210 - loss value [[270.23996992]] accuracy 0.7272727272727273\n","Iteration 49211 - loss value [[273.5312952]] accuracy 0.7272727272727273\n","Iteration 49212 - loss value [[271.53002675]] accuracy 0.7272727272727273\n","Iteration 49213 - loss value [[274.51534378]] accuracy 0.7272727272727273\n","Iteration 49214 - loss value [[273.29841531]] accuracy 0.7272727272727273\n","Iteration 49215 - loss value [[276.27073083]] accuracy 0.7272727272727273\n","Iteration 49216 - loss value [[276.55164978]] accuracy 0.7272727272727273\n","Iteration 49217 - loss value [[279.20657394]] accuracy 0.7272727272727273\n","Iteration 49218 - loss value [[277.39835448]] accuracy 0.7272727272727273\n","Iteration 49219 - loss value [[279.95815707]] accuracy 0.7272727272727273\n","Iteration 49220 - loss value [[277.83347146]] accuracy 0.7272727272727273\n","Iteration 49221 - loss value [[280.65916075]] accuracy 0.7272727272727273\n","Iteration 49222 - loss value [[278.31310684]] accuracy 0.7272727272727273\n","Iteration 49223 - loss value [[280.86920247]] accuracy 0.7272727272727273\n","Iteration 49224 - loss value [[277.78856122]] accuracy 0.7272727272727273\n","Iteration 49225 - loss value [[279.38107699]] accuracy 0.7272727272727273\n","Iteration 49226 - loss value [[276.92986347]] accuracy 0.7272727272727273\n","Iteration 49227 - loss value [[278.27865397]] accuracy 0.7272727272727273\n","Iteration 49228 - loss value [[275.31203896]] accuracy 0.7272727272727273\n","Iteration 49229 - loss value [[276.14067652]] accuracy 0.7272727272727273\n","Iteration 49230 - loss value [[273.84680786]] accuracy 0.7272727272727273\n","Iteration 49231 - loss value [[275.50216545]] accuracy 0.7272727272727273\n","Iteration 49232 - loss value [[272.65014078]] accuracy 0.7272727272727273\n","Iteration 49233 - loss value [[272.94571753]] accuracy 0.7272727272727273\n","Iteration 49234 - loss value [[269.83783707]] accuracy 0.7272727272727273\n","Iteration 49235 - loss value [[270.71472223]] accuracy 0.7272727272727273\n","Iteration 49236 - loss value [[269.49928676]] accuracy 0.7272727272727273\n","Iteration 49237 - loss value [[271.21622934]] accuracy 0.7272727272727273\n","Iteration 49238 - loss value [[268.39775905]] accuracy 0.7272727272727273\n","Iteration 49239 - loss value [[268.24430762]] accuracy 0.7272727272727273\n","Iteration 49240 - loss value [[267.55319228]] accuracy 0.7272727272727273\n","Iteration 49241 - loss value [[267.97915175]] accuracy 0.7272727272727273\n","Iteration 49242 - loss value [[267.49138438]] accuracy 0.7272727272727273\n","Iteration 49243 - loss value [[268.28210792]] accuracy 0.7272727272727273\n","Iteration 49244 - loss value [[267.07977096]] accuracy 0.7272727272727273\n","Iteration 49245 - loss value [[266.84609057]] accuracy 0.7272727272727273\n","Iteration 49246 - loss value [[266.82934472]] accuracy 0.7272727272727273\n","Iteration 49247 - loss value [[268.40425753]] accuracy 0.7272727272727273\n","Iteration 49248 - loss value [[267.312417]] accuracy 0.7272727272727273\n","Iteration 49249 - loss value [[268.99776918]] accuracy 0.7272727272727273\n","Iteration 49250 - loss value [[268.85149138]] accuracy 0.7272727272727273\n","Iteration 49251 - loss value [[270.98257505]] accuracy 0.7272727272727273\n","Iteration 49252 - loss value [[268.93426834]] accuracy 0.7272727272727273\n","Iteration 49253 - loss value [[271.10469431]] accuracy 0.7272727272727273\n","Iteration 49254 - loss value [[268.99788137]] accuracy 0.7272727272727273\n","Iteration 49255 - loss value [[271.98817483]] accuracy 0.7272727272727273\n","Iteration 49256 - loss value [[269.30601851]] accuracy 0.7272727272727273\n","Iteration 49257 - loss value [[270.19689823]] accuracy 0.7272727272727273\n","Iteration 49258 - loss value [[269.92236441]] accuracy 0.7272727272727273\n","Iteration 49259 - loss value [[272.40363103]] accuracy 0.7272727272727273\n","Iteration 49260 - loss value [[269.78869689]] accuracy 0.7272727272727273\n","Iteration 49261 - loss value [[272.83026901]] accuracy 0.7272727272727273\n","Iteration 49262 - loss value [[269.73677189]] accuracy 0.7272727272727273\n","Iteration 49263 - loss value [[270.22094263]] accuracy 0.7272727272727273\n","Iteration 49264 - loss value [[269.57915458]] accuracy 0.7272727272727273\n","Iteration 49265 - loss value [[271.97508991]] accuracy 0.7272727272727273\n","Iteration 49266 - loss value [[269.2884373]] accuracy 0.7272727272727273\n","Iteration 49267 - loss value [[271.00713585]] accuracy 0.7272727272727273\n","Iteration 49268 - loss value [[268.48541896]] accuracy 0.7272727272727273\n","Iteration 49269 - loss value [[270.21096416]] accuracy 0.7272727272727273\n","Iteration 49270 - loss value [[268.61940005]] accuracy 0.7272727272727273\n","Iteration 49271 - loss value [[270.90683568]] accuracy 0.7272727272727273\n","Iteration 49272 - loss value [[268.30663137]] accuracy 0.7272727272727273\n","Iteration 49273 - loss value [[268.87683574]] accuracy 0.7272727272727273\n","Iteration 49274 - loss value [[267.9575209]] accuracy 0.7272727272727273\n","Iteration 49275 - loss value [[270.25277084]] accuracy 0.7272727272727273\n","Iteration 49276 - loss value [[268.32411352]] accuracy 0.7272727272727273\n","Iteration 49277 - loss value [[269.92044042]] accuracy 0.7272727272727273\n","Iteration 49278 - loss value [[268.67927243]] accuracy 0.7272727272727273\n","Iteration 49279 - loss value [[271.53374007]] accuracy 0.7272727272727273\n","Iteration 49280 - loss value [[268.81819649]] accuracy 0.7272727272727273\n","Iteration 49281 - loss value [[269.85228966]] accuracy 0.7272727272727273\n","Iteration 49282 - loss value [[269.13348002]] accuracy 0.7272727272727273\n","Iteration 49283 - loss value [[271.9193624]] accuracy 0.7272727272727273\n","Iteration 49284 - loss value [[269.29807031]] accuracy 0.7272727272727273\n","Iteration 49285 - loss value [[271.13178457]] accuracy 0.7272727272727273\n","Iteration 49286 - loss value [[268.97191434]] accuracy 0.7272727272727273\n","Iteration 49287 - loss value [[271.86327737]] accuracy 0.7272727272727273\n","Iteration 49288 - loss value [[269.25832036]] accuracy 0.7272727272727273\n","Iteration 49289 - loss value [[270.27652077]] accuracy 0.7272727272727273\n","Iteration 49290 - loss value [[269.98859424]] accuracy 0.7272727272727273\n","Iteration 49291 - loss value [[272.770341]] accuracy 0.7272727272727273\n","Iteration 49292 - loss value [[270.12528266]] accuracy 0.7272727272727273\n","Iteration 49293 - loss value [[273.22039828]] accuracy 0.7272727272727273\n","Iteration 49294 - loss value [[270.46047176]] accuracy 0.7272727272727273\n","Iteration 49295 - loss value [[273.07100215]] accuracy 0.7272727272727273\n","Iteration 49296 - loss value [[270.39219311]] accuracy 0.7272727272727273\n","Iteration 49297 - loss value [[272.83988272]] accuracy 0.7272727272727273\n","Iteration 49298 - loss value [[270.20959481]] accuracy 0.7272727272727273\n","Iteration 49299 - loss value [[272.06742833]] accuracy 0.7272727272727273\n","Iteration 49300 - loss value [[269.58944446]] accuracy 0.7272727272727273\n","Iteration 49301 - loss value [[272.51261598]] accuracy 0.7272727272727273\n","Iteration 49302 - loss value [[269.50821171]] accuracy 0.7272727272727273\n","Iteration 49303 - loss value [[269.45665588]] accuracy 0.7272727272727273\n","Iteration 49304 - loss value [[268.29881801]] accuracy 0.7272727272727273\n","Iteration 49305 - loss value [[271.06158304]] accuracy 0.7272727272727273\n","Iteration 49306 - loss value [[268.18051389]] accuracy 0.7272727272727273\n","Iteration 49307 - loss value [[268.10878928]] accuracy 0.7272727272727273\n","Iteration 49308 - loss value [[267.61256358]] accuracy 0.7272727272727273\n","Iteration 49309 - loss value [[269.25492793]] accuracy 0.7272727272727273\n","Iteration 49310 - loss value [[268.52661773]] accuracy 0.7272727272727273\n","Iteration 49311 - loss value [[271.41960114]] accuracy 0.7272727272727273\n","Iteration 49312 - loss value [[268.70203435]] accuracy 0.7272727272727273\n","Iteration 49313 - loss value [[269.5437002]] accuracy 0.7272727272727273\n","Iteration 49314 - loss value [[269.01784285]] accuracy 0.7272727272727273\n","Iteration 49315 - loss value [[271.89527682]] accuracy 0.7272727272727273\n","Iteration 49316 - loss value [[269.58794759]] accuracy 0.7272727272727273\n","Iteration 49317 - loss value [[272.4918201]] accuracy 0.7272727272727273\n","Iteration 49318 - loss value [[269.79963992]] accuracy 0.7272727272727273\n","Iteration 49319 - loss value [[271.86835626]] accuracy 0.7272727272727273\n","Iteration 49320 - loss value [[269.47312268]] accuracy 0.7272727272727273\n","Iteration 49321 - loss value [[271.7747862]] accuracy 0.7272727272727273\n","Iteration 49322 - loss value [[269.37031115]] accuracy 0.7272727272727273\n","Iteration 49323 - loss value [[271.42651703]] accuracy 0.7272727272727273\n","Iteration 49324 - loss value [[269.0184419]] accuracy 0.7272727272727273\n","Iteration 49325 - loss value [[270.95267227]] accuracy 0.7272727272727273\n","Iteration 49326 - loss value [[268.42082691]] accuracy 0.7272727272727273\n","Iteration 49327 - loss value [[269.50637568]] accuracy 0.7272727272727273\n","Iteration 49328 - loss value [[268.64087116]] accuracy 0.7272727272727273\n","Iteration 49329 - loss value [[271.7925138]] accuracy 0.7272727272727273\n","Iteration 49330 - loss value [[269.23305181]] accuracy 0.7272727272727273\n","Iteration 49331 - loss value [[270.8238277]] accuracy 0.7272727272727273\n","Iteration 49332 - loss value [[268.7704434]] accuracy 0.7272727272727273\n","Iteration 49333 - loss value [[271.50087341]] accuracy 0.7272727272727273\n","Iteration 49334 - loss value [[268.96881762]] accuracy 0.7272727272727273\n","Iteration 49335 - loss value [[270.31114758]] accuracy 0.7272727272727273\n","Iteration 49336 - loss value [[268.87256537]] accuracy 0.7272727272727273\n","Iteration 49337 - loss value [[271.8849198]] accuracy 0.7272727272727273\n","Iteration 49338 - loss value [[269.2385499]] accuracy 0.7272727272727273\n","Iteration 49339 - loss value [[270.35107167]] accuracy 0.7272727272727273\n","Iteration 49340 - loss value [[268.69120497]] accuracy 0.7272727272727273\n","Iteration 49341 - loss value [[271.24590388]] accuracy 0.7272727272727273\n","Iteration 49342 - loss value [[268.853762]] accuracy 0.7272727272727273\n","Iteration 49343 - loss value [[269.83446881]] accuracy 0.7272727272727273\n","Iteration 49344 - loss value [[269.19593623]] accuracy 0.7272727272727273\n","Iteration 49345 - loss value [[272.26600116]] accuracy 0.7272727272727273\n","Iteration 49346 - loss value [[269.59007041]] accuracy 0.7272727272727273\n","Iteration 49347 - loss value [[272.20370049]] accuracy 0.7272727272727273\n","Iteration 49348 - loss value [[269.80668525]] accuracy 0.7272727272727273\n","Iteration 49349 - loss value [[272.13740924]] accuracy 0.7272727272727273\n","Iteration 49350 - loss value [[269.64775468]] accuracy 0.7272727272727273\n","Iteration 49351 - loss value [[271.77105155]] accuracy 0.7272727272727273\n","Iteration 49352 - loss value [[269.49931277]] accuracy 0.7272727272727273\n","Iteration 49353 - loss value [[272.52691783]] accuracy 0.7272727272727273\n","Iteration 49354 - loss value [[269.82916623]] accuracy 0.7272727272727273\n","Iteration 49355 - loss value [[270.88422083]] accuracy 0.7272727272727273\n","Iteration 49356 - loss value [[269.59221015]] accuracy 0.7272727272727273\n","Iteration 49357 - loss value [[272.56259963]] accuracy 0.7272727272727273\n","Iteration 49358 - loss value [[269.97392721]] accuracy 0.7272727272727273\n","Iteration 49359 - loss value [[272.32287945]] accuracy 0.7272727272727273\n","Iteration 49360 - loss value [[269.65164204]] accuracy 0.7272727272727273\n","Iteration 49361 - loss value [[271.3794358]] accuracy 0.7272727272727273\n","Iteration 49362 - loss value [[269.03439518]] accuracy 0.7272727272727273\n","Iteration 49363 - loss value [[270.71005486]] accuracy 0.7272727272727273\n","Iteration 49364 - loss value [[268.54088134]] accuracy 0.7272727272727273\n","Iteration 49365 - loss value [[270.33769155]] accuracy 0.7272727272727273\n","Iteration 49366 - loss value [[268.49941172]] accuracy 0.7272727272727273\n","Iteration 49367 - loss value [[270.83091012]] accuracy 0.7272727272727273\n","Iteration 49368 - loss value [[268.31153811]] accuracy 0.7272727272727273\n","Iteration 49369 - loss value [[269.5699403]] accuracy 0.7272727272727273\n","Iteration 49370 - loss value [[268.47338598]] accuracy 0.7272727272727273\n","Iteration 49371 - loss value [[271.50377046]] accuracy 0.7272727272727273\n","Iteration 49372 - loss value [[268.94856708]] accuracy 0.7272727272727273\n","Iteration 49373 - loss value [[270.54637476]] accuracy 0.7272727272727273\n","Iteration 49374 - loss value [[268.75264097]] accuracy 0.7272727272727273\n","Iteration 49375 - loss value [[270.80664136]] accuracy 0.7272727272727273\n","Iteration 49376 - loss value [[268.48689531]] accuracy 0.7272727272727273\n","Iteration 49377 - loss value [[270.4496067]] accuracy 0.7272727272727273\n","Iteration 49378 - loss value [[268.61190508]] accuracy 0.7272727272727273\n","Iteration 49379 - loss value [[271.09076232]] accuracy 0.7272727272727273\n","Iteration 49380 - loss value [[268.68142225]] accuracy 0.7272727272727273\n","Iteration 49381 - loss value [[269.75322162]] accuracy 0.7272727272727273\n","Iteration 49382 - loss value [[269.11893938]] accuracy 0.7272727272727273\n","Iteration 49383 - loss value [[272.26429567]] accuracy 0.7272727272727273\n","Iteration 49384 - loss value [[269.84587458]] accuracy 0.7272727272727273\n","Iteration 49385 - loss value [[272.25964831]] accuracy 0.7272727272727273\n","Iteration 49386 - loss value [[269.81850821]] accuracy 0.7272727272727273\n","Iteration 49387 - loss value [[271.96308053]] accuracy 0.7272727272727273\n","Iteration 49388 - loss value [[269.85991998]] accuracy 0.7272727272727273\n","Iteration 49389 - loss value [[273.22136409]] accuracy 0.7272727272727273\n","Iteration 49390 - loss value [[271.26361166]] accuracy 0.7272727272727273\n","Iteration 49391 - loss value [[275.12477141]] accuracy 0.7272727272727273\n","Iteration 49392 - loss value [[274.32234527]] accuracy 0.7272727272727273\n","Iteration 49393 - loss value [[277.07365791]] accuracy 0.7272727272727273\n","Iteration 49394 - loss value [[276.57735825]] accuracy 0.7272727272727273\n","Iteration 49395 - loss value [[279.76490337]] accuracy 0.7272727272727273\n","Iteration 49396 - loss value [[278.16856624]] accuracy 0.7272727272727273\n","Iteration 49397 - loss value [[280.72936512]] accuracy 0.7272727272727273\n","Iteration 49398 - loss value [[278.38637551]] accuracy 0.7272727272727273\n","Iteration 49399 - loss value [[281.3706346]] accuracy 0.7272727272727273\n","Iteration 49400 - loss value [[279.01768727]] accuracy 0.7272727272727273\n","Iteration 49401 - loss value [[281.44159109]] accuracy 0.7272727272727273\n","Iteration 49402 - loss value [[278.87027494]] accuracy 0.7272727272727273\n","Iteration 49403 - loss value [[280.93858984]] accuracy 0.7272727272727273\n","Iteration 49404 - loss value [[277.7704629]] accuracy 0.7272727272727273\n","Iteration 49405 - loss value [[278.85379179]] accuracy 0.7272727272727273\n","Iteration 49406 - loss value [[275.77836222]] accuracy 0.7272727272727273\n","Iteration 49407 - loss value [[277.49978171]] accuracy 0.7272727272727273\n","Iteration 49408 - loss value [[274.77444362]] accuracy 0.7272727272727273\n","Iteration 49409 - loss value [[275.2271294]] accuracy 0.7272727272727273\n","Iteration 49410 - loss value [[272.86265769]] accuracy 0.7272727272727273\n","Iteration 49411 - loss value [[274.04621439]] accuracy 0.7272727272727273\n","Iteration 49412 - loss value [[271.61512336]] accuracy 0.7272727272727273\n","Iteration 49413 - loss value [[273.27303711]] accuracy 0.7272727272727273\n","Iteration 49414 - loss value [[270.14825069]] accuracy 0.7272727272727273\n","Iteration 49415 - loss value [[270.49279747]] accuracy 0.7272727272727273\n","Iteration 49416 - loss value [[268.95213866]] accuracy 0.7272727272727273\n","Iteration 49417 - loss value [[270.48843555]] accuracy 0.7272727272727273\n","Iteration 49418 - loss value [[268.71319151]] accuracy 0.7272727272727273\n","Iteration 49419 - loss value [[270.08605748]] accuracy 0.7272727272727273\n","Iteration 49420 - loss value [[268.26205106]] accuracy 0.7272727272727273\n","Iteration 49421 - loss value [[268.51510204]] accuracy 0.7272727272727273\n","Iteration 49422 - loss value [[267.38892406]] accuracy 0.7272727272727273\n","Iteration 49423 - loss value [[267.83008438]] accuracy 0.7272727272727273\n","Iteration 49424 - loss value [[267.20909634]] accuracy 0.7272727272727273\n","Iteration 49425 - loss value [[268.29087372]] accuracy 0.7272727272727273\n","Iteration 49426 - loss value [[267.08473139]] accuracy 0.7272727272727273\n","Iteration 49427 - loss value [[267.25964948]] accuracy 0.7272727272727273\n","Iteration 49428 - loss value [[266.78292796]] accuracy 0.7272727272727273\n","Iteration 49429 - loss value [[267.47179136]] accuracy 0.7272727272727273\n","Iteration 49430 - loss value [[267.58103278]] accuracy 0.7272727272727273\n","Iteration 49431 - loss value [[270.01220536]] accuracy 0.7272727272727273\n","Iteration 49432 - loss value [[268.19433745]] accuracy 0.7272727272727273\n","Iteration 49433 - loss value [[269.93573316]] accuracy 0.7272727272727273\n","Iteration 49434 - loss value [[268.98294]] accuracy 0.7272727272727273\n","Iteration 49435 - loss value [[271.98183502]] accuracy 0.7272727272727273\n","Iteration 49436 - loss value [[269.33290948]] accuracy 0.7272727272727273\n","Iteration 49437 - loss value [[271.43016171]] accuracy 0.7272727272727273\n","Iteration 49438 - loss value [[268.9843235]] accuracy 0.7272727272727273\n","Iteration 49439 - loss value [[272.16669876]] accuracy 0.7272727272727273\n","Iteration 49440 - loss value [[269.45357219]] accuracy 0.7272727272727273\n","Iteration 49441 - loss value [[269.82641255]] accuracy 0.7272727272727273\n","Iteration 49442 - loss value [[269.20742272]] accuracy 0.7272727272727273\n","Iteration 49443 - loss value [[271.63279786]] accuracy 0.7272727272727273\n","Iteration 49444 - loss value [[268.992862]] accuracy 0.7272727272727273\n","Iteration 49445 - loss value [[271.83663792]] accuracy 0.7272727272727273\n","Iteration 49446 - loss value [[269.13039594]] accuracy 0.7272727272727273\n","Iteration 49447 - loss value [[270.0995483]] accuracy 0.7272727272727273\n","Iteration 49448 - loss value [[269.85405006]] accuracy 0.7272727272727273\n","Iteration 49449 - loss value [[272.26742855]] accuracy 0.7272727272727273\n","Iteration 49450 - loss value [[269.63800837]] accuracy 0.7272727272727273\n","Iteration 49451 - loss value [[272.11787069]] accuracy 0.7272727272727273\n","Iteration 49452 - loss value [[269.30838899]] accuracy 0.7272727272727273\n","Iteration 49453 - loss value [[270.32532655]] accuracy 0.7272727272727273\n","Iteration 49454 - loss value [[269.54029121]] accuracy 0.7272727272727273\n","Iteration 49455 - loss value [[271.82786581]] accuracy 0.7272727272727273\n","Iteration 49456 - loss value [[269.21270363]] accuracy 0.7272727272727273\n","Iteration 49457 - loss value [[271.15292797]] accuracy 0.7272727272727273\n","Iteration 49458 - loss value [[268.59245448]] accuracy 0.7272727272727273\n","Iteration 49459 - loss value [[269.89480382]] accuracy 0.7272727272727273\n","Iteration 49460 - loss value [[268.96425689]] accuracy 0.7272727272727273\n","Iteration 49461 - loss value [[271.80120704]] accuracy 0.7272727272727273\n","Iteration 49462 - loss value [[269.14130095]] accuracy 0.7272727272727273\n","Iteration 49463 - loss value [[270.15333711]] accuracy 0.7272727272727273\n","Iteration 49464 - loss value [[269.41385921]] accuracy 0.7272727272727273\n","Iteration 49465 - loss value [[271.75383503]] accuracy 0.7272727272727273\n","Iteration 49466 - loss value [[269.15423386]] accuracy 0.7272727272727273\n","Iteration 49467 - loss value [[271.11919669]] accuracy 0.7272727272727273\n","Iteration 49468 - loss value [[268.65200792]] accuracy 0.7272727272727273\n","Iteration 49469 - loss value [[270.52237603]] accuracy 0.7272727272727273\n","Iteration 49470 - loss value [[268.53802462]] accuracy 0.7272727272727273\n","Iteration 49471 - loss value [[270.46424266]] accuracy 0.7272727272727273\n","Iteration 49472 - loss value [[268.47889275]] accuracy 0.7272727272727273\n","Iteration 49473 - loss value [[270.74479074]] accuracy 0.7272727272727273\n","Iteration 49474 - loss value [[268.3670097]] accuracy 0.7272727272727273\n","Iteration 49475 - loss value [[269.69950072]] accuracy 0.7272727272727273\n","Iteration 49476 - loss value [[269.25274382]] accuracy 0.7272727272727273\n","Iteration 49477 - loss value [[271.60777236]] accuracy 0.7272727272727273\n","Iteration 49478 - loss value [[269.02587298]] accuracy 0.7272727272727273\n","Iteration 49479 - loss value [[271.05940972]] accuracy 0.7272727272727273\n","Iteration 49480 - loss value [[268.66960422]] accuracy 0.7272727272727273\n","Iteration 49481 - loss value [[270.52924226]] accuracy 0.7272727272727273\n","Iteration 49482 - loss value [[268.77776457]] accuracy 0.7272727272727273\n","Iteration 49483 - loss value [[271.36670699]] accuracy 0.7272727272727273\n","Iteration 49484 - loss value [[268.78234925]] accuracy 0.7272727272727273\n","Iteration 49485 - loss value [[270.45837105]] accuracy 0.7272727272727273\n","Iteration 49486 - loss value [[268.69881369]] accuracy 0.7272727272727273\n","Iteration 49487 - loss value [[271.3026368]] accuracy 0.7272727272727273\n","Iteration 49488 - loss value [[268.5816151]] accuracy 0.7272727272727273\n","Iteration 49489 - loss value [[269.0729577]] accuracy 0.7272727272727273\n","Iteration 49490 - loss value [[268.39660821]] accuracy 0.7272727272727273\n","Iteration 49491 - loss value [[271.48319304]] accuracy 0.7272727272727273\n","Iteration 49492 - loss value [[268.79536704]] accuracy 0.7272727272727273\n","Iteration 49493 - loss value [[269.68629675]] accuracy 0.7272727272727273\n","Iteration 49494 - loss value [[269.48049825]] accuracy 0.7272727272727273\n","Iteration 49495 - loss value [[272.01440998]] accuracy 0.7272727272727273\n","Iteration 49496 - loss value [[269.53065285]] accuracy 0.7272727272727273\n","Iteration 49497 - loss value [[272.84018752]] accuracy 0.7272727272727273\n","Iteration 49498 - loss value [[270.31664382]] accuracy 0.7272727272727273\n","Iteration 49499 - loss value [[272.57966685]] accuracy 0.7272727272727273\n","Iteration 49500 - loss value [[270.7973352]] accuracy 0.7272727272727273\n","Iteration 49501 - loss value [[272.90892698]] accuracy 0.7272727272727273\n","Iteration 49502 - loss value [[270.51881533]] accuracy 0.7272727272727273\n","Iteration 49503 - loss value [[273.62609797]] accuracy 0.7272727272727273\n","Iteration 49504 - loss value [[271.46106888]] accuracy 0.7272727272727273\n","Iteration 49505 - loss value [[274.54911054]] accuracy 0.7272727272727273\n","Iteration 49506 - loss value [[272.90037466]] accuracy 0.7272727272727273\n","Iteration 49507 - loss value [[276.06272572]] accuracy 0.7272727272727273\n","Iteration 49508 - loss value [[275.44686517]] accuracy 0.7272727272727273\n","Iteration 49509 - loss value [[278.35243238]] accuracy 0.7272727272727273\n","Iteration 49510 - loss value [[276.93882801]] accuracy 0.7272727272727273\n","Iteration 49511 - loss value [[279.85541753]] accuracy 0.7272727272727273\n","Iteration 49512 - loss value [[277.09787891]] accuracy 0.7272727272727273\n","Iteration 49513 - loss value [[280.00061688]] accuracy 0.7272727272727273\n","Iteration 49514 - loss value [[277.19944432]] accuracy 0.7272727272727273\n","Iteration 49515 - loss value [[279.67669196]] accuracy 0.7272727272727273\n","Iteration 49516 - loss value [[276.09118612]] accuracy 0.7272727272727273\n","Iteration 49517 - loss value [[276.44237506]] accuracy 0.7272727272727273\n","Iteration 49518 - loss value [[274.84077614]] accuracy 0.7272727272727273\n","Iteration 49519 - loss value [[276.43744925]] accuracy 0.7272727272727273\n","Iteration 49520 - loss value [[274.75896685]] accuracy 0.7272727272727273\n","Iteration 49521 - loss value [[276.51427268]] accuracy 0.7272727272727273\n","Iteration 49522 - loss value [[274.72421557]] accuracy 0.7272727272727273\n","Iteration 49523 - loss value [[276.47946405]] accuracy 0.7272727272727273\n","Iteration 49524 - loss value [[274.64039923]] accuracy 0.7272727272727273\n","Iteration 49525 - loss value [[276.09294204]] accuracy 0.7272727272727273\n","Iteration 49526 - loss value [[274.09308122]] accuracy 0.7272727272727273\n","Iteration 49527 - loss value [[275.32531561]] accuracy 0.7272727272727273\n","Iteration 49528 - loss value [[272.63085325]] accuracy 0.7272727272727273\n","Iteration 49529 - loss value [[274.02717246]] accuracy 0.7272727272727273\n","Iteration 49530 - loss value [[271.54542451]] accuracy 0.7272727272727273\n","Iteration 49531 - loss value [[272.34228015]] accuracy 0.7272727272727273\n","Iteration 49532 - loss value [[269.07278249]] accuracy 0.7272727272727273\n","Iteration 49533 - loss value [[268.44200172]] accuracy 0.7272727272727273\n","Iteration 49534 - loss value [[267.68562895]] accuracy 0.7272727272727273\n","Iteration 49535 - loss value [[269.29675067]] accuracy 0.7272727272727273\n","Iteration 49536 - loss value [[267.6614875]] accuracy 0.7272727272727273\n","Iteration 49537 - loss value [[267.7613803]] accuracy 0.7272727272727273\n","Iteration 49538 - loss value [[267.09267495]] accuracy 0.7272727272727273\n","Iteration 49539 - loss value [[267.47448696]] accuracy 0.7272727272727273\n","Iteration 49540 - loss value [[267.12407202]] accuracy 0.7272727272727273\n","Iteration 49541 - loss value [[267.67641667]] accuracy 0.7272727272727273\n","Iteration 49542 - loss value [[267.22501854]] accuracy 0.7272727272727273\n","Iteration 49543 - loss value [[268.82470814]] accuracy 0.7272727272727273\n","Iteration 49544 - loss value [[268.23738414]] accuracy 0.7272727272727273\n","Iteration 49545 - loss value [[270.6483191]] accuracy 0.7272727272727273\n","Iteration 49546 - loss value [[268.56451105]] accuracy 0.7272727272727273\n","Iteration 49547 - loss value [[269.95258912]] accuracy 0.7272727272727273\n","Iteration 49548 - loss value [[268.59173316]] accuracy 0.7272727272727273\n","Iteration 49549 - loss value [[271.58462492]] accuracy 0.7272727272727273\n","Iteration 49550 - loss value [[268.75060371]] accuracy 0.7272727272727273\n","Iteration 49551 - loss value [[269.15108551]] accuracy 0.7272727272727273\n","Iteration 49552 - loss value [[268.33712406]] accuracy 0.7272727272727273\n","Iteration 49553 - loss value [[270.99760133]] accuracy 0.7272727272727273\n","Iteration 49554 - loss value [[268.57541224]] accuracy 0.7272727272727273\n","Iteration 49555 - loss value [[269.69426015]] accuracy 0.7272727272727273\n","Iteration 49556 - loss value [[268.96701606]] accuracy 0.7272727272727273\n","Iteration 49557 - loss value [[271.95587355]] accuracy 0.7272727272727273\n","Iteration 49558 - loss value [[269.24588652]] accuracy 0.7272727272727273\n","Iteration 49559 - loss value [[271.22946254]] accuracy 0.7272727272727273\n","Iteration 49560 - loss value [[269.00175616]] accuracy 0.7272727272727273\n","Iteration 49561 - loss value [[271.17533017]] accuracy 0.7272727272727273\n","Iteration 49562 - loss value [[268.53451939]] accuracy 0.7272727272727273\n","Iteration 49563 - loss value [[269.89705194]] accuracy 0.7272727272727273\n","Iteration 49564 - loss value [[268.95838439]] accuracy 0.7272727272727273\n","Iteration 49565 - loss value [[271.9162073]] accuracy 0.7272727272727273\n","Iteration 49566 - loss value [[269.2065676]] accuracy 0.7272727272727273\n","Iteration 49567 - loss value [[270.14608898]] accuracy 0.7272727272727273\n","Iteration 49568 - loss value [[269.3120024]] accuracy 0.7272727272727273\n","Iteration 49569 - loss value [[271.55797782]] accuracy 0.7272727272727273\n","Iteration 49570 - loss value [[268.89165875]] accuracy 0.7272727272727273\n","Iteration 49571 - loss value [[270.47073898]] accuracy 0.7272727272727273\n","Iteration 49572 - loss value [[268.7876207]] accuracy 0.7272727272727273\n","Iteration 49573 - loss value [[271.6193428]] accuracy 0.7272727272727273\n","Iteration 49574 - loss value [[268.84084606]] accuracy 0.7272727272727273\n","Iteration 49575 - loss value [[269.2926452]] accuracy 0.7272727272727273\n","Iteration 49576 - loss value [[268.17506485]] accuracy 0.7272727272727273\n","Iteration 49577 - loss value [[270.94521067]] accuracy 0.7272727272727273\n","Iteration 49578 - loss value [[268.44823172]] accuracy 0.7272727272727273\n","Iteration 49579 - loss value [[270.03118964]] accuracy 0.7272727272727273\n","Iteration 49580 - loss value [[268.41283848]] accuracy 0.7272727272727273\n","Iteration 49581 - loss value [[270.65986931]] accuracy 0.7272727272727273\n","Iteration 49582 - loss value [[268.63397151]] accuracy 0.7272727272727273\n","Iteration 49583 - loss value [[270.62216697]] accuracy 0.7272727272727273\n","Iteration 49584 - loss value [[269.19516701]] accuracy 0.7272727272727273\n","Iteration 49585 - loss value [[272.11888637]] accuracy 0.7272727272727273\n","Iteration 49586 - loss value [[269.37919444]] accuracy 0.7272727272727273\n","Iteration 49587 - loss value [[270.39027888]] accuracy 0.7272727272727273\n","Iteration 49588 - loss value [[268.88025478]] accuracy 0.7272727272727273\n","Iteration 49589 - loss value [[271.1564811]] accuracy 0.7272727272727273\n","Iteration 49590 - loss value [[269.04255546]] accuracy 0.7272727272727273\n","Iteration 49591 - loss value [[271.45567976]] accuracy 0.7272727272727273\n","Iteration 49592 - loss value [[268.98242677]] accuracy 0.7272727272727273\n","Iteration 49593 - loss value [[271.24732414]] accuracy 0.7272727272727273\n","Iteration 49594 - loss value [[268.51219073]] accuracy 0.7272727272727273\n","Iteration 49595 - loss value [[268.45287224]] accuracy 0.7272727272727273\n","Iteration 49596 - loss value [[268.51520184]] accuracy 0.7272727272727273\n","Iteration 49597 - loss value [[270.95224533]] accuracy 0.7272727272727273\n","Iteration 49598 - loss value [[269.09315117]] accuracy 0.7272727272727273\n","Iteration 49599 - loss value [[272.1398437]] accuracy 0.7272727272727273\n","Iteration 49600 - loss value [[269.46579957]] accuracy 0.7272727272727273\n","Iteration 49601 - loss value [[270.49942684]] accuracy 0.7272727272727273\n","Iteration 49602 - loss value [[268.68972531]] accuracy 0.7272727272727273\n","Iteration 49603 - loss value [[271.28118171]] accuracy 0.7272727272727273\n","Iteration 49604 - loss value [[268.51625535]] accuracy 0.7272727272727273\n","Iteration 49605 - loss value [[269.44521124]] accuracy 0.7272727272727273\n","Iteration 49606 - loss value [[268.74441569]] accuracy 0.7272727272727273\n","Iteration 49607 - loss value [[271.6517465]] accuracy 0.7272727272727273\n","Iteration 49608 - loss value [[268.97819658]] accuracy 0.7272727272727273\n","Iteration 49609 - loss value [[270.05597038]] accuracy 0.7272727272727273\n","Iteration 49610 - loss value [[270.13708063]] accuracy 0.7272727272727273\n","Iteration 49611 - loss value [[273.24035834]] accuracy 0.7272727272727273\n","Iteration 49612 - loss value [[270.46391246]] accuracy 0.7272727272727273\n","Iteration 49613 - loss value [[273.35306826]] accuracy 0.7272727272727273\n","Iteration 49614 - loss value [[270.81745703]] accuracy 0.7272727272727273\n","Iteration 49615 - loss value [[274.11301986]] accuracy 0.7272727272727273\n","Iteration 49616 - loss value [[272.51597516]] accuracy 0.7272727272727273\n","Iteration 49617 - loss value [[275.51561677]] accuracy 0.7272727272727273\n","Iteration 49618 - loss value [[274.74689402]] accuracy 0.7272727272727273\n","Iteration 49619 - loss value [[277.41932348]] accuracy 0.7272727272727273\n","Iteration 49620 - loss value [[276.70951618]] accuracy 0.7272727272727273\n","Iteration 49621 - loss value [[279.52142174]] accuracy 0.7272727272727273\n","Iteration 49622 - loss value [[278.16198787]] accuracy 0.7272727272727273\n","Iteration 49623 - loss value [[280.34856028]] accuracy 0.7272727272727273\n","Iteration 49624 - loss value [[278.16467179]] accuracy 0.7272727272727273\n","Iteration 49625 - loss value [[280.38625431]] accuracy 0.7272727272727273\n","Iteration 49626 - loss value [[277.79206552]] accuracy 0.7272727272727273\n","Iteration 49627 - loss value [[279.36291056]] accuracy 0.7272727272727273\n","Iteration 49628 - loss value [[275.83506535]] accuracy 0.7272727272727273\n","Iteration 49629 - loss value [[277.04993851]] accuracy 0.7272727272727273\n","Iteration 49630 - loss value [[274.84189607]] accuracy 0.7272727272727273\n","Iteration 49631 - loss value [[276.65095175]] accuracy 0.7272727272727273\n","Iteration 49632 - loss value [[273.78174319]] accuracy 0.7272727272727273\n","Iteration 49633 - loss value [[274.89257329]] accuracy 0.7272727272727273\n","Iteration 49634 - loss value [[272.51419156]] accuracy 0.7272727272727273\n","Iteration 49635 - loss value [[273.04133986]] accuracy 0.7272727272727273\n","Iteration 49636 - loss value [[270.08348111]] accuracy 0.7272727272727273\n","Iteration 49637 - loss value [[270.13456781]] accuracy 0.7272727272727273\n","Iteration 49638 - loss value [[268.5175296]] accuracy 0.7272727272727273\n","Iteration 49639 - loss value [[269.16873932]] accuracy 0.7272727272727273\n","Iteration 49640 - loss value [[267.76443394]] accuracy 0.7272727272727273\n","Iteration 49641 - loss value [[267.79944965]] accuracy 0.7272727272727273\n","Iteration 49642 - loss value [[267.068152]] accuracy 0.7272727272727273\n","Iteration 49643 - loss value [[267.59579145]] accuracy 0.7272727272727273\n","Iteration 49644 - loss value [[267.20872903]] accuracy 0.7272727272727273\n","Iteration 49645 - loss value [[268.38111957]] accuracy 0.7272727272727273\n","Iteration 49646 - loss value [[267.94325637]] accuracy 0.7272727272727273\n","Iteration 49647 - loss value [[270.13038099]] accuracy 0.7272727272727273\n","Iteration 49648 - loss value [[268.6133188]] accuracy 0.7272727272727273\n","Iteration 49649 - loss value [[270.97770701]] accuracy 0.7272727272727273\n","Iteration 49650 - loss value [[268.47701177]] accuracy 0.7272727272727273\n","Iteration 49651 - loss value [[269.14045264]] accuracy 0.7272727272727273\n","Iteration 49652 - loss value [[268.71415484]] accuracy 0.7272727272727273\n","Iteration 49653 - loss value [[271.32125121]] accuracy 0.7272727272727273\n","Iteration 49654 - loss value [[268.65143995]] accuracy 0.7272727272727273\n","Iteration 49655 - loss value [[269.53884888]] accuracy 0.7272727272727273\n","Iteration 49656 - loss value [[268.78945734]] accuracy 0.7272727272727273\n","Iteration 49657 - loss value [[271.65418216]] accuracy 0.7272727272727273\n","Iteration 49658 - loss value [[268.62383299]] accuracy 0.7272727272727273\n","Iteration 49659 - loss value [[269.524049]] accuracy 0.7272727272727273\n","Iteration 49660 - loss value [[269.06388657]] accuracy 0.7272727272727273\n","Iteration 49661 - loss value [[271.24036078]] accuracy 0.7272727272727273\n","Iteration 49662 - loss value [[268.76091641]] accuracy 0.7272727272727273\n","Iteration 49663 - loss value [[271.24215324]] accuracy 0.7272727272727273\n","Iteration 49664 - loss value [[268.39813154]] accuracy 0.7272727272727273\n","Iteration 49665 - loss value [[269.12110178]] accuracy 0.7272727272727273\n","Iteration 49666 - loss value [[268.52982227]] accuracy 0.7272727272727273\n","Iteration 49667 - loss value [[271.2202969]] accuracy 0.7272727272727273\n","Iteration 49668 - loss value [[268.74303142]] accuracy 0.7272727272727273\n","Iteration 49669 - loss value [[270.55038013]] accuracy 0.7272727272727273\n","Iteration 49670 - loss value [[268.47740052]] accuracy 0.7272727272727273\n","Iteration 49671 - loss value [[270.30067645]] accuracy 0.7272727272727273\n","Iteration 49672 - loss value [[268.97455127]] accuracy 0.7272727272727273\n","Iteration 49673 - loss value [[271.956382]] accuracy 0.7272727272727273\n","Iteration 49674 - loss value [[269.25328963]] accuracy 0.7272727272727273\n","Iteration 49675 - loss value [[270.20947078]] accuracy 0.7272727272727273\n","Iteration 49676 - loss value [[269.66143511]] accuracy 0.7272727272727273\n","Iteration 49677 - loss value [[271.95158763]] accuracy 0.7272727272727273\n","Iteration 49678 - loss value [[269.28732564]] accuracy 0.7272727272727273\n","Iteration 49679 - loss value [[271.41381625]] accuracy 0.7272727272727273\n","Iteration 49680 - loss value [[269.03563943]] accuracy 0.7272727272727273\n","Iteration 49681 - loss value [[270.81106845]] accuracy 0.7272727272727273\n","Iteration 49682 - loss value [[268.47676595]] accuracy 0.7272727272727273\n","Iteration 49683 - loss value [[270.01529104]] accuracy 0.7272727272727273\n","Iteration 49684 - loss value [[268.26228435]] accuracy 0.7272727272727273\n","Iteration 49685 - loss value [[270.45120861]] accuracy 0.7272727272727273\n","Iteration 49686 - loss value [[268.44416123]] accuracy 0.7272727272727273\n","Iteration 49687 - loss value [[270.3121535]] accuracy 0.7272727272727273\n","Iteration 49688 - loss value [[268.92176531]] accuracy 0.7272727272727273\n","Iteration 49689 - loss value [[272.04689933]] accuracy 0.7272727272727273\n","Iteration 49690 - loss value [[269.28322964]] accuracy 0.7272727272727273\n","Iteration 49691 - loss value [[269.72885336]] accuracy 0.7272727272727273\n","Iteration 49692 - loss value [[269.16502317]] accuracy 0.7272727272727273\n","Iteration 49693 - loss value [[271.51136867]] accuracy 0.7272727272727273\n","Iteration 49694 - loss value [[268.96107389]] accuracy 0.7272727272727273\n","Iteration 49695 - loss value [[271.6061033]] accuracy 0.7272727272727273\n","Iteration 49696 - loss value [[268.96194349]] accuracy 0.7272727272727273\n","Iteration 49697 - loss value [[270.20754195]] accuracy 0.7272727272727273\n","Iteration 49698 - loss value [[268.47225266]] accuracy 0.7272727272727273\n","Iteration 49699 - loss value [[270.95884418]] accuracy 0.7272727272727273\n","Iteration 49700 - loss value [[268.22521938]] accuracy 0.7272727272727273\n","Iteration 49701 - loss value [[268.10698785]] accuracy 0.7272727272727273\n","Iteration 49702 - loss value [[267.97961073]] accuracy 0.7272727272727273\n","Iteration 49703 - loss value [[270.78627005]] accuracy 0.7272727272727273\n","Iteration 49704 - loss value [[269.07278394]] accuracy 0.7272727272727273\n","Iteration 49705 - loss value [[272.02077336]] accuracy 0.7272727272727273\n","Iteration 49706 - loss value [[269.44784841]] accuracy 0.7272727272727273\n","Iteration 49707 - loss value [[271.55289646]] accuracy 0.7272727272727273\n","Iteration 49708 - loss value [[269.17455176]] accuracy 0.7272727272727273\n","Iteration 49709 - loss value [[271.49060792]] accuracy 0.7272727272727273\n","Iteration 49710 - loss value [[269.06411744]] accuracy 0.7272727272727273\n","Iteration 49711 - loss value [[270.70227218]] accuracy 0.7272727272727273\n","Iteration 49712 - loss value [[268.944873]] accuracy 0.7272727272727273\n","Iteration 49713 - loss value [[272.15653139]] accuracy 0.7272727272727273\n","Iteration 49714 - loss value [[269.18676405]] accuracy 0.7272727272727273\n","Iteration 49715 - loss value [[269.67494679]] accuracy 0.7272727272727273\n","Iteration 49716 - loss value [[269.0691877]] accuracy 0.7272727272727273\n","Iteration 49717 - loss value [[271.94827837]] accuracy 0.7272727272727273\n","Iteration 49718 - loss value [[269.36433427]] accuracy 0.7272727272727273\n","Iteration 49719 - loss value [[271.00933146]] accuracy 0.7272727272727273\n","Iteration 49720 - loss value [[268.76068287]] accuracy 0.7272727272727273\n","Iteration 49721 - loss value [[270.38317366]] accuracy 0.7272727272727273\n","Iteration 49722 - loss value [[268.75773458]] accuracy 0.7272727272727273\n","Iteration 49723 - loss value [[271.4704011]] accuracy 0.7272727272727273\n","Iteration 49724 - loss value [[268.83550169]] accuracy 0.7272727272727273\n","Iteration 49725 - loss value [[270.13825883]] accuracy 0.7272727272727273\n","Iteration 49726 - loss value [[268.69145724]] accuracy 0.7272727272727273\n","Iteration 49727 - loss value [[271.63701308]] accuracy 0.7272727272727273\n","Iteration 49728 - loss value [[269.04046814]] accuracy 0.7272727272727273\n","Iteration 49729 - loss value [[270.33986465]] accuracy 0.7272727272727273\n","Iteration 49730 - loss value [[268.61214784]] accuracy 0.7272727272727273\n","Iteration 49731 - loss value [[271.66070579]] accuracy 0.7272727272727273\n","Iteration 49732 - loss value [[268.88718863]] accuracy 0.7272727272727273\n","Iteration 49733 - loss value [[269.24723017]] accuracy 0.7272727272727273\n","Iteration 49734 - loss value [[268.29846411]] accuracy 0.7272727272727273\n","Iteration 49735 - loss value [[271.32070641]] accuracy 0.7272727272727273\n","Iteration 49736 - loss value [[268.69713083]] accuracy 0.7272727272727273\n","Iteration 49737 - loss value [[270.40891738]] accuracy 0.7272727272727273\n","Iteration 49738 - loss value [[269.18731789]] accuracy 0.7272727272727273\n","Iteration 49739 - loss value [[272.10410213]] accuracy 0.7272727272727273\n","Iteration 49740 - loss value [[269.55079551]] accuracy 0.7272727272727273\n","Iteration 49741 - loss value [[271.74726384]] accuracy 0.7272727272727273\n","Iteration 49742 - loss value [[269.4222651]] accuracy 0.7272727272727273\n","Iteration 49743 - loss value [[271.87975174]] accuracy 0.7272727272727273\n","Iteration 49744 - loss value [[269.47400057]] accuracy 0.7272727272727273\n","Iteration 49745 - loss value [[271.16869208]] accuracy 0.7272727272727273\n","Iteration 49746 - loss value [[268.80473514]] accuracy 0.7272727272727273\n","Iteration 49747 - loss value [[270.49090866]] accuracy 0.7272727272727273\n","Iteration 49748 - loss value [[269.00580266]] accuracy 0.7272727272727273\n","Iteration 49749 - loss value [[272.22549745]] accuracy 0.7272727272727273\n","Iteration 49750 - loss value [[269.48026865]] accuracy 0.7272727272727273\n","Iteration 49751 - loss value [[270.57131214]] accuracy 0.7272727272727273\n","Iteration 49752 - loss value [[268.89751217]] accuracy 0.7272727272727273\n","Iteration 49753 - loss value [[271.91029109]] accuracy 0.7272727272727273\n","Iteration 49754 - loss value [[269.25634183]] accuracy 0.7272727272727273\n","Iteration 49755 - loss value [[270.12878983]] accuracy 0.7272727272727273\n","Iteration 49756 - loss value [[268.2730996]] accuracy 0.7272727272727273\n","Iteration 49757 - loss value [[270.51683816]] accuracy 0.7272727272727273\n","Iteration 49758 - loss value [[268.82587317]] accuracy 0.7272727272727273\n","Iteration 49759 - loss value [[271.91748289]] accuracy 0.7272727272727273\n","Iteration 49760 - loss value [[269.30384383]] accuracy 0.7272727272727273\n","Iteration 49761 - loss value [[270.45595183]] accuracy 0.7272727272727273\n","Iteration 49762 - loss value [[268.66452753]] accuracy 0.7272727272727273\n","Iteration 49763 - loss value [[271.50508102]] accuracy 0.7272727272727273\n","Iteration 49764 - loss value [[268.78086203]] accuracy 0.7272727272727273\n","Iteration 49765 - loss value [[268.66683952]] accuracy 0.7272727272727273\n","Iteration 49766 - loss value [[268.13060084]] accuracy 0.7272727272727273\n","Iteration 49767 - loss value [[271.21087606]] accuracy 0.7272727272727273\n","Iteration 49768 - loss value [[268.66303283]] accuracy 0.7272727272727273\n","Iteration 49769 - loss value [[270.42860797]] accuracy 0.7272727272727273\n","Iteration 49770 - loss value [[269.19002897]] accuracy 0.7272727272727273\n","Iteration 49771 - loss value [[272.39297575]] accuracy 0.7272727272727273\n","Iteration 49772 - loss value [[269.81689738]] accuracy 0.7272727272727273\n","Iteration 49773 - loss value [[272.88297868]] accuracy 0.7272727272727273\n","Iteration 49774 - loss value [[270.5462267]] accuracy 0.7272727272727273\n","Iteration 49775 - loss value [[273.65635145]] accuracy 0.7272727272727273\n","Iteration 49776 - loss value [[271.99368741]] accuracy 0.7272727272727273\n","Iteration 49777 - loss value [[275.01172312]] accuracy 0.7272727272727273\n","Iteration 49778 - loss value [[274.32643768]] accuracy 0.7272727272727273\n","Iteration 49779 - loss value [[277.24827694]] accuracy 0.7272727272727273\n","Iteration 49780 - loss value [[276.68493771]] accuracy 0.7272727272727273\n","Iteration 49781 - loss value [[279.78579355]] accuracy 0.7272727272727273\n","Iteration 49782 - loss value [[278.31814023]] accuracy 0.7272727272727273\n","Iteration 49783 - loss value [[280.51975185]] accuracy 0.7272727272727273\n","Iteration 49784 - loss value [[278.37549272]] accuracy 0.7272727272727273\n","Iteration 49785 - loss value [[280.35223133]] accuracy 0.7272727272727273\n","Iteration 49786 - loss value [[278.02187296]] accuracy 0.7272727272727273\n","Iteration 49787 - loss value [[279.81268888]] accuracy 0.7272727272727273\n","Iteration 49788 - loss value [[276.73543258]] accuracy 0.7272727272727273\n","Iteration 49789 - loss value [[278.48118489]] accuracy 0.7272727272727273\n","Iteration 49790 - loss value [[275.89938204]] accuracy 0.7272727272727273\n","Iteration 49791 - loss value [[276.5180986]] accuracy 0.7272727272727273\n","Iteration 49792 - loss value [[274.13355187]] accuracy 0.7272727272727273\n","Iteration 49793 - loss value [[275.57035705]] accuracy 0.7272727272727273\n","Iteration 49794 - loss value [[273.04840141]] accuracy 0.7272727272727273\n","Iteration 49795 - loss value [[274.73286423]] accuracy 0.7272727272727273\n","Iteration 49796 - loss value [[271.84728709]] accuracy 0.7272727272727273\n","Iteration 49797 - loss value [[272.08191581]] accuracy 0.7272727272727273\n","Iteration 49798 - loss value [[269.75976553]] accuracy 0.7272727272727273\n","Iteration 49799 - loss value [[271.06857764]] accuracy 0.7272727272727273\n","Iteration 49800 - loss value [[268.20392867]] accuracy 0.7272727272727273\n","Iteration 49801 - loss value [[267.6270207]] accuracy 0.7272727272727273\n","Iteration 49802 - loss value [[267.11874391]] accuracy 0.7272727272727273\n","Iteration 49803 - loss value [[267.92618036]] accuracy 0.7272727272727273\n","Iteration 49804 - loss value [[267.17311472]] accuracy 0.7272727272727273\n","Iteration 49805 - loss value [[268.01474323]] accuracy 0.7272727272727273\n","Iteration 49806 - loss value [[267.2475249]] accuracy 0.7272727272727273\n","Iteration 49807 - loss value [[268.4125418]] accuracy 0.7272727272727273\n","Iteration 49808 - loss value [[267.83847898]] accuracy 0.7272727272727273\n","Iteration 49809 - loss value [[270.23549695]] accuracy 0.7272727272727273\n","Iteration 49810 - loss value [[268.65384577]] accuracy 0.7272727272727273\n","Iteration 49811 - loss value [[270.44797424]] accuracy 0.7272727272727273\n","Iteration 49812 - loss value [[268.90809407]] accuracy 0.7272727272727273\n","Iteration 49813 - loss value [[271.65131201]] accuracy 0.7272727272727273\n","Iteration 49814 - loss value [[268.94446554]] accuracy 0.7272727272727273\n","Iteration 49815 - loss value [[269.90086555]] accuracy 0.7272727272727273\n","Iteration 49816 - loss value [[269.17172647]] accuracy 0.7272727272727273\n","Iteration 49817 - loss value [[271.53531344]] accuracy 0.7272727272727273\n","Iteration 49818 - loss value [[268.88928954]] accuracy 0.7272727272727273\n","Iteration 49819 - loss value [[269.99680079]] accuracy 0.7272727272727273\n","Iteration 49820 - loss value [[268.3686051]] accuracy 0.7272727272727273\n","Iteration 49821 - loss value [[270.80566046]] accuracy 0.7272727272727273\n","Iteration 49822 - loss value [[268.2475793]] accuracy 0.7272727272727273\n","Iteration 49823 - loss value [[268.69970969]] accuracy 0.7272727272727273\n","Iteration 49824 - loss value [[268.45651439]] accuracy 0.7272727272727273\n","Iteration 49825 - loss value [[271.2877539]] accuracy 0.7272727272727273\n","Iteration 49826 - loss value [[268.837991]] accuracy 0.7272727272727273\n","Iteration 49827 - loss value [[270.62181245]] accuracy 0.7272727272727273\n","Iteration 49828 - loss value [[269.1463496]] accuracy 0.7272727272727273\n","Iteration 49829 - loss value [[271.95502822]] accuracy 0.7272727272727273\n","Iteration 49830 - loss value [[269.16824847]] accuracy 0.7272727272727273\n","Iteration 49831 - loss value [[270.11680762]] accuracy 0.7272727272727273\n","Iteration 49832 - loss value [[268.87742947]] accuracy 0.7272727272727273\n","Iteration 49833 - loss value [[271.72454172]] accuracy 0.7272727272727273\n","Iteration 49834 - loss value [[268.99758703]] accuracy 0.7272727272727273\n","Iteration 49835 - loss value [[269.99114396]] accuracy 0.7272727272727273\n","Iteration 49836 - loss value [[269.55286371]] accuracy 0.7272727272727273\n","Iteration 49837 - loss value [[271.85224293]] accuracy 0.7272727272727273\n","Iteration 49838 - loss value [[269.28896359]] accuracy 0.7272727272727273\n","Iteration 49839 - loss value [[272.43463751]] accuracy 0.7272727272727273\n","Iteration 49840 - loss value [[269.56945146]] accuracy 0.7272727272727273\n","Iteration 49841 - loss value [[269.94551194]] accuracy 0.7272727272727273\n","Iteration 49842 - loss value [[269.28829407]] accuracy 0.7272727272727273\n","Iteration 49843 - loss value [[271.48641202]] accuracy 0.7272727272727273\n","Iteration 49844 - loss value [[268.93382187]] accuracy 0.7272727272727273\n","Iteration 49845 - loss value [[270.85298908]] accuracy 0.7272727272727273\n","Iteration 49846 - loss value [[268.9280984]] accuracy 0.7272727272727273\n","Iteration 49847 - loss value [[271.49837288]] accuracy 0.7272727272727273\n","Iteration 49848 - loss value [[268.78400538]] accuracy 0.7272727272727273\n","Iteration 49849 - loss value [[269.53674512]] accuracy 0.7272727272727273\n","Iteration 49850 - loss value [[268.93688328]] accuracy 0.7272727272727273\n","Iteration 49851 - loss value [[271.189516]] accuracy 0.7272727272727273\n","Iteration 49852 - loss value [[268.90675742]] accuracy 0.7272727272727273\n","Iteration 49853 - loss value [[271.78433189]] accuracy 0.7272727272727273\n","Iteration 49854 - loss value [[269.00177108]] accuracy 0.7272727272727273\n","Iteration 49855 - loss value [[269.26766703]] accuracy 0.7272727272727273\n","Iteration 49856 - loss value [[268.19564941]] accuracy 0.7272727272727273\n","Iteration 49857 - loss value [[270.88031903]] accuracy 0.7272727272727273\n","Iteration 49858 - loss value [[268.56953716]] accuracy 0.7272727272727273\n","Iteration 49859 - loss value [[270.17756234]] accuracy 0.7272727272727273\n","Iteration 49860 - loss value [[268.68834335]] accuracy 0.7272727272727273\n","Iteration 49861 - loss value [[271.63072609]] accuracy 0.7272727272727273\n","Iteration 49862 - loss value [[268.82432134]] accuracy 0.7272727272727273\n","Iteration 49863 - loss value [[269.64959339]] accuracy 0.7272727272727273\n","Iteration 49864 - loss value [[269.17728285]] accuracy 0.7272727272727273\n","Iteration 49865 - loss value [[271.52063785]] accuracy 0.7272727272727273\n","Iteration 49866 - loss value [[268.91807343]] accuracy 0.7272727272727273\n","Iteration 49867 - loss value [[271.5733788]] accuracy 0.7272727272727273\n","Iteration 49868 - loss value [[268.89624491]] accuracy 0.7272727272727273\n","Iteration 49869 - loss value [[269.70404171]] accuracy 0.7272727272727273\n","Iteration 49870 - loss value [[269.38859745]] accuracy 0.7272727272727273\n","Iteration 49871 - loss value [[271.67106424]] accuracy 0.7272727272727273\n","Iteration 49872 - loss value [[269.20460698]] accuracy 0.7272727272727273\n","Iteration 49873 - loss value [[272.20304863]] accuracy 0.7272727272727273\n","Iteration 49874 - loss value [[269.3557473]] accuracy 0.7272727272727273\n","Iteration 49875 - loss value [[269.81109881]] accuracy 0.7272727272727273\n","Iteration 49876 - loss value [[269.36782537]] accuracy 0.7272727272727273\n","Iteration 49877 - loss value [[271.61439985]] accuracy 0.7272727272727273\n","Iteration 49878 - loss value [[269.12610751]] accuracy 0.7272727272727273\n","Iteration 49879 - loss value [[271.84139931]] accuracy 0.7272727272727273\n","Iteration 49880 - loss value [[269.06669977]] accuracy 0.7272727272727273\n","Iteration 49881 - loss value [[269.77452901]] accuracy 0.7272727272727273\n","Iteration 49882 - loss value [[269.53378806]] accuracy 0.7272727272727273\n","Iteration 49883 - loss value [[271.94948872]] accuracy 0.7272727272727273\n","Iteration 49884 - loss value [[269.40651778]] accuracy 0.7272727272727273\n","Iteration 49885 - loss value [[272.34231448]] accuracy 0.7272727272727273\n","Iteration 49886 - loss value [[269.70451591]] accuracy 0.7272727272727273\n","Iteration 49887 - loss value [[271.80075217]] accuracy 0.7272727272727273\n","Iteration 49888 - loss value [[269.352735]] accuracy 0.7272727272727273\n","Iteration 49889 - loss value [[271.24553188]] accuracy 0.7272727272727273\n","Iteration 49890 - loss value [[268.81455657]] accuracy 0.7272727272727273\n","Iteration 49891 - loss value [[270.56680649]] accuracy 0.7272727272727273\n","Iteration 49892 - loss value [[268.42014361]] accuracy 0.7272727272727273\n","Iteration 49893 - loss value [[270.58341329]] accuracy 0.7272727272727273\n","Iteration 49894 - loss value [[268.27745464]] accuracy 0.7272727272727273\n","Iteration 49895 - loss value [[269.4900743]] accuracy 0.7272727272727273\n","Iteration 49896 - loss value [[269.11683531]] accuracy 0.7272727272727273\n","Iteration 49897 - loss value [[271.50077143]] accuracy 0.7272727272727273\n","Iteration 49898 - loss value [[269.03594617]] accuracy 0.7272727272727273\n","Iteration 49899 - loss value [[270.98966972]] accuracy 0.7272727272727273\n","Iteration 49900 - loss value [[268.49864104]] accuracy 0.7272727272727273\n","Iteration 49901 - loss value [[269.73243197]] accuracy 0.7272727272727273\n","Iteration 49902 - loss value [[268.99305461]] accuracy 0.7272727272727273\n","Iteration 49903 - loss value [[271.36825356]] accuracy 0.7272727272727273\n","Iteration 49904 - loss value [[268.90227666]] accuracy 0.7272727272727273\n","Iteration 49905 - loss value [[271.23458746]] accuracy 0.7272727272727273\n","Iteration 49906 - loss value [[268.57798026]] accuracy 0.7272727272727273\n","Iteration 49907 - loss value [[269.92409502]] accuracy 0.7272727272727273\n","Iteration 49908 - loss value [[268.68322275]] accuracy 0.7272727272727273\n","Iteration 49909 - loss value [[271.63865362]] accuracy 0.7272727272727273\n","Iteration 49910 - loss value [[269.05633168]] accuracy 0.7272727272727273\n","Iteration 49911 - loss value [[270.36545289]] accuracy 0.7272727272727273\n","Iteration 49912 - loss value [[269.19711854]] accuracy 0.7272727272727273\n","Iteration 49913 - loss value [[272.09937996]] accuracy 0.7272727272727273\n","Iteration 49914 - loss value [[269.48433307]] accuracy 0.7272727272727273\n","Iteration 49915 - loss value [[272.58081912]] accuracy 0.7272727272727273\n","Iteration 49916 - loss value [[269.81918185]] accuracy 0.7272727272727273\n","Iteration 49917 - loss value [[270.33496259]] accuracy 0.7272727272727273\n","Iteration 49918 - loss value [[270.09116048]] accuracy 0.7272727272727273\n","Iteration 49919 - loss value [[273.34001835]] accuracy 0.7272727272727273\n","Iteration 49920 - loss value [[271.0555322]] accuracy 0.7272727272727273\n","Iteration 49921 - loss value [[274.16316352]] accuracy 0.7272727272727273\n","Iteration 49922 - loss value [[272.27173686]] accuracy 0.7272727272727273\n","Iteration 49923 - loss value [[275.61847851]] accuracy 0.7272727272727273\n","Iteration 49924 - loss value [[275.063046]] accuracy 0.7272727272727273\n","Iteration 49925 - loss value [[277.85323194]] accuracy 0.7272727272727273\n","Iteration 49926 - loss value [[277.17162345]] accuracy 0.7272727272727273\n","Iteration 49927 - loss value [[279.96543693]] accuracy 0.7272727272727273\n","Iteration 49928 - loss value [[277.31514278]] accuracy 0.7272727272727273\n","Iteration 49929 - loss value [[279.70032248]] accuracy 0.7272727272727273\n","Iteration 49930 - loss value [[276.99894808]] accuracy 0.7272727272727273\n","Iteration 49931 - loss value [[278.9912668]] accuracy 0.7272727272727273\n","Iteration 49932 - loss value [[276.0374247]] accuracy 0.7272727272727273\n","Iteration 49933 - loss value [[277.21393952]] accuracy 0.7272727272727273\n","Iteration 49934 - loss value [[275.32674676]] accuracy 0.7272727272727273\n","Iteration 49935 - loss value [[277.40403656]] accuracy 0.7272727272727273\n","Iteration 49936 - loss value [[275.28810506]] accuracy 0.7272727272727273\n","Iteration 49937 - loss value [[277.04472979]] accuracy 0.7272727272727273\n","Iteration 49938 - loss value [[275.01903782]] accuracy 0.7272727272727273\n","Iteration 49939 - loss value [[276.75561936]] accuracy 0.7272727272727273\n","Iteration 49940 - loss value [[274.03897951]] accuracy 0.7272727272727273\n","Iteration 49941 - loss value [[275.15426174]] accuracy 0.7272727272727273\n","Iteration 49942 - loss value [[272.47180525]] accuracy 0.7272727272727273\n","Iteration 49943 - loss value [[273.32233369]] accuracy 0.7272727272727273\n","Iteration 49944 - loss value [[270.67310049]] accuracy 0.7272727272727273\n","Iteration 49945 - loss value [[270.99282056]] accuracy 0.7272727272727273\n","Iteration 49946 - loss value [[269.1483267]] accuracy 0.7272727272727273\n","Iteration 49947 - loss value [[269.32377575]] accuracy 0.7272727272727273\n","Iteration 49948 - loss value [[268.24939847]] accuracy 0.7272727272727273\n","Iteration 49949 - loss value [[268.42806907]] accuracy 0.7272727272727273\n","Iteration 49950 - loss value [[267.26472702]] accuracy 0.7272727272727273\n","Iteration 49951 - loss value [[267.79292274]] accuracy 0.7272727272727273\n","Iteration 49952 - loss value [[267.23977223]] accuracy 0.7272727272727273\n","Iteration 49953 - loss value [[267.68858656]] accuracy 0.7272727272727273\n","Iteration 49954 - loss value [[267.18933845]] accuracy 0.7272727272727273\n","Iteration 49955 - loss value [[267.31620925]] accuracy 0.7272727272727273\n","Iteration 49956 - loss value [[266.9216752]] accuracy 0.7272727272727273\n","Iteration 49957 - loss value [[268.68325356]] accuracy 0.7272727272727273\n","Iteration 49958 - loss value [[268.33095697]] accuracy 0.7272727272727273\n","Iteration 49959 - loss value [[269.91724599]] accuracy 0.7272727272727273\n","Iteration 49960 - loss value [[268.99350052]] accuracy 0.7272727272727273\n","Iteration 49961 - loss value [[272.02784899]] accuracy 0.7272727272727273\n","Iteration 49962 - loss value [[269.1266912]] accuracy 0.7272727272727273\n","Iteration 49963 - loss value [[269.45371694]] accuracy 0.7272727272727273\n","Iteration 49964 - loss value [[268.1566072]] accuracy 0.7272727272727273\n","Iteration 49965 - loss value [[270.77401721]] accuracy 0.7272727272727273\n","Iteration 49966 - loss value [[268.63902742]] accuracy 0.7272727272727273\n","Iteration 49967 - loss value [[270.45539721]] accuracy 0.7272727272727273\n","Iteration 49968 - loss value [[269.02794945]] accuracy 0.7272727272727273\n","Iteration 49969 - loss value [[271.3543451]] accuracy 0.7272727272727273\n","Iteration 49970 - loss value [[269.06921595]] accuracy 0.7272727272727273\n","Iteration 49971 - loss value [[270.89481827]] accuracy 0.7272727272727273\n","Iteration 49972 - loss value [[268.57681486]] accuracy 0.7272727272727273\n","Iteration 49973 - loss value [[270.37246867]] accuracy 0.7272727272727273\n","Iteration 49974 - loss value [[268.69788636]] accuracy 0.7272727272727273\n","Iteration 49975 - loss value [[270.86902304]] accuracy 0.7272727272727273\n","Iteration 49976 - loss value [[268.95913074]] accuracy 0.7272727272727273\n","Iteration 49977 - loss value [[271.46492287]] accuracy 0.7272727272727273\n","Iteration 49978 - loss value [[268.80413663]] accuracy 0.7272727272727273\n","Iteration 49979 - loss value [[269.98535909]] accuracy 0.7272727272727273\n","Iteration 49980 - loss value [[268.5423296]] accuracy 0.7272727272727273\n","Iteration 49981 - loss value [[271.43312451]] accuracy 0.7272727272727273\n","Iteration 49982 - loss value [[268.59998635]] accuracy 0.7272727272727273\n","Iteration 49983 - loss value [[269.26437777]] accuracy 0.7272727272727273\n","Iteration 49984 - loss value [[268.53218651]] accuracy 0.7272727272727273\n","Iteration 49985 - loss value [[271.26492533]] accuracy 0.7272727272727273\n","Iteration 49986 - loss value [[268.82577058]] accuracy 0.7272727272727273\n","Iteration 49987 - loss value [[270.35538137]] accuracy 0.7272727272727273\n","Iteration 49988 - loss value [[268.88358589]] accuracy 0.7272727272727273\n","Iteration 49989 - loss value [[271.70390621]] accuracy 0.7272727272727273\n","Iteration 49990 - loss value [[268.9229247]] accuracy 0.7272727272727273\n","Iteration 49991 - loss value [[270.02616779]] accuracy 0.7272727272727273\n","Iteration 49992 - loss value [[269.43305866]] accuracy 0.7272727272727273\n","Iteration 49993 - loss value [[271.96110109]] accuracy 0.7272727272727273\n","Iteration 49994 - loss value [[269.52640887]] accuracy 0.7272727272727273\n","Iteration 49995 - loss value [[272.44947669]] accuracy 0.7272727272727273\n","Iteration 49996 - loss value [[269.75412852]] accuracy 0.7272727272727273\n","Iteration 49997 - loss value [[271.55126214]] accuracy 0.7272727272727273\n","Iteration 49998 - loss value [[269.22470003]] accuracy 0.7272727272727273\n","Iteration 49999 - loss value [[271.39177955]] accuracy 0.7272727272727273\n"]}]},{"cell_type":"markdown","source":["### **VALIDACION DEL MODELO MLP**"],"metadata":{"id":"eMUEisiB3i7L"}},{"cell_type":"code","source":["Y_pred_linear2 = model.forward(X_test)\n","regr = LinearRegression()\n","regr.fit(Y_test,Y_pred_linear2)\n","coef = regr.coef_\n","print(coef)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cJizPfz2jED","executionInfo":{"status":"ok","timestamp":1664239263947,"user_tz":300,"elapsed":88,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"7c090770-a75a-43b8-99f5-26479fd52f66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.4266004]]\n"]}]},{"cell_type":"markdown","source":["## **MODELO A PARTIR DE MLP KERAS**"],"metadata":{"id":"8GFEixoA3cdQ"}},{"cell_type":"markdown","source":["### **DIVISION DEL DATASET Y PREPROCESAMIENTO DE LOS DATOS**"],"metadata":{"id":"-pFnM2mw3yAA"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","\n","X = datos.iloc[:,0:6]\n","Y = datos.iloc[:,6]\n","\n","scaler1 = MinMaxScaler(feature_range=(-1, 1))\n","scaler1.fit(X)\n","X_n=scaler1.transform(X)\n","X_n = np.array(X_n)\n","\n","scaler2 = MinMaxScaler(feature_range=(-1, 1))\n","Y_n = Y.values\n","Y_n = Y_n.reshape(-1,1)\n","\n","scaler2.fit(Y_n)\n","Y_n=scaler2.transform(Y_n)\n","Y_n = np.array(Y_n)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X_n, Y_n, test_size = 0.2, random_state=5)"],"metadata":{"id":"vOHob8c82_1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **CREACION DEL MODELO MLP**"],"metadata":{"id":"C9nEa8jj4FMv"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n","from tensorflow.keras.utils import plot_model\n","\n","model_RL_1 = Sequential()\n","\n","model_RL_1.add(Dense(6, input_dim=6, activation='relu'))\n","model_RL_1.add(Dense(4, activation='sigmoid'))\n","model_RL_1.add(Dense(1, activation='linear'))"],"metadata":{"id":"0Iny2TtiZIY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_model(model_RL_1, to_file = 'model.jpg', show_shapes = True)"],"metadata":{"id":"1r9K-iR-ZJqF","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"ok","timestamp":1664239432588,"user_tz":300,"elapsed":366,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"3072dcf0-dbab-4798-bdd8-0084914cd4c3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"image/jpeg":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["### **ENTRENAMIENTO DEL MODELO MLP**"],"metadata":{"id":"30REv-0nOyS8"}},{"cell_type":"code","source":["model_RL_1.compile(loss='mean_squared_error',\n","              optimizer = keras.optimizers.Adam(learning_rate=0.005),\n","              metrics=['accuracy'])\n","\n","model_RL_1.fit(X_train, Y_train, epochs=5000)"],"metadata":{"id":"adNXdsOrZMiP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239851008,"user_tz":300,"elapsed":417459,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"664f000c-720a-4661-f008-2ce57e11f6e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las ltimas 5000 lneas del flujo de salida.\u001b[0m\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2502/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2503/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2504/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2505/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2506/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2507/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2508/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2509/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2510/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2511/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2512/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2513/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2514/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2515/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2516/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2517/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2518/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2519/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2520/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2521/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2522/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2523/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 2524/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2525/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2526/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2527/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2528/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2529/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2530/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2531/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2532/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2533/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2534/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2535/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2536/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2537/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2538/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2539/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2540/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2541/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2542/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2543/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2544/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2545/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2546/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2547/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2548/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2549/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2550/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2551/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2552/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2553/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2554/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2555/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 2556/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2557/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2558/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2559/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2560/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2561/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2562/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2563/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2564/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2565/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2566/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2567/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2568/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2569/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2570/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2571/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2572/5000\n","23/23 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2573/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2574/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.0014\n","Epoch 2575/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2576/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2577/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2578/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2579/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2580/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2581/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2582/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2583/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2584/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2585/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2586/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2587/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2588/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2589/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 2590/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2591/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2592/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2593/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2594/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2595/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2596/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2597/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2598/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 2599/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2600/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2601/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2602/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2603/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2604/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2605/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2606/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2607/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2608/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2609/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2610/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2611/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2612/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2613/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2614/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2615/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2616/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2617/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2618/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2619/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2620/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2621/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2622/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2623/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2624/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2625/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2626/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2627/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2628/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2629/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2630/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2631/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2632/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2633/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2634/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2635/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2636/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2637/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2638/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2639/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2640/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2641/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2642/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2643/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2644/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2645/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2646/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2647/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2648/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2649/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2650/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2651/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2652/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2653/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2654/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2655/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 2656/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2657/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2658/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2659/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2660/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2661/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2662/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2663/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2664/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2665/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2666/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2667/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2668/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2669/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2670/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2671/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2672/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2673/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2674/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2675/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2676/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2677/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2678/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2679/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2680/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2681/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2682/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2683/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2684/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2685/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2686/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2687/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2688/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2689/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2690/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2691/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2692/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2693/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2694/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2695/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2696/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2697/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2698/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2699/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2700/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2701/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2702/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2703/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2704/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2705/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2706/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2707/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2708/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2709/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2710/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2711/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2712/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2713/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2714/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2715/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2716/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2717/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2718/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2719/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2720/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2721/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2722/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2723/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2724/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2725/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2726/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2727/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2728/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2729/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2730/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2731/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2732/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2733/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2734/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2735/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2736/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2737/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2738/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2739/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2740/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2741/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2742/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2743/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2744/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2745/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2746/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2747/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2748/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2749/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2750/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2751/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2752/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2753/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2754/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2755/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2756/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2757/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2758/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 2759/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2760/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2761/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2762/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2763/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2764/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2765/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2766/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2767/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2768/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2769/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2770/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0277 - accuracy: 0.0014\n","Epoch 2771/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2772/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2773/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2774/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2775/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2776/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 2777/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2778/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2779/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2780/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2781/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2782/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2783/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2784/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2785/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2786/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2787/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2788/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2789/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2790/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2791/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2792/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2793/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2794/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2795/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2796/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2797/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2798/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2799/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 2800/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2801/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2802/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2803/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2804/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2805/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2806/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2807/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2808/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2809/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2810/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2811/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2812/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2813/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2814/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2815/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2816/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2817/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2818/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2819/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2820/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2821/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2822/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2823/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2824/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2825/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2826/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2827/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2828/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2829/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2830/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2831/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2832/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2833/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2834/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2835/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2836/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2837/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2838/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2839/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2840/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2841/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2842/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2843/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2844/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2845/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2846/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2847/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2848/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2849/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2850/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2851/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2852/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2853/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2854/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2855/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2856/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2857/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2858/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2859/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2860/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2861/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2862/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2863/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2864/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2865/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2866/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2867/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 2868/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2869/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2870/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2871/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2872/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2873/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2874/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2875/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2876/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2877/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2878/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2879/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2880/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2881/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2882/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2883/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2884/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2885/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2886/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2887/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2888/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 2889/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2890/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2891/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2892/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2893/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2894/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2895/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2896/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2897/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2898/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2899/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2900/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2901/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2902/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2903/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2904/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2905/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2906/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2907/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2908/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2909/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2910/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2911/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2912/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2913/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2914/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2915/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2916/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2917/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2918/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2919/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2920/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2921/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2922/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2923/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2924/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2925/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2926/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2927/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2928/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2929/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2930/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2931/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2932/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2933/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2934/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2935/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2936/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2937/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2938/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2939/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2940/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2941/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2942/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2943/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2944/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2945/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 2946/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2947/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2948/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2949/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2950/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 2951/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2952/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2953/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2954/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2955/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2956/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2957/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2958/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2959/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2960/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2961/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2962/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2963/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2964/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2965/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2966/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.0014\n","Epoch 2967/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2968/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2969/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2970/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2971/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2972/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2973/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2974/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2975/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2976/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2977/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2978/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2979/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2980/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 2981/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2982/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 2983/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2984/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2985/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2986/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2987/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2988/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 2989/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2990/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2991/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2992/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 2993/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 2994/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 2995/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 2996/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 2997/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 2998/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 2999/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3000/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3001/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3002/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3003/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3004/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3005/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3006/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3007/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3008/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3009/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3010/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3011/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3012/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3013/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3014/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3015/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3016/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3017/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3018/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3019/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3020/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3021/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3022/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3023/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3024/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3025/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3026/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3027/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3028/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3029/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3030/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3031/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3032/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3033/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3034/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3035/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3036/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3037/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3038/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3039/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3040/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3041/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3042/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3043/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3044/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3045/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3046/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3047/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3048/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3049/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3050/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3051/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3052/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3053/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3054/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3055/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3056/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3057/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3058/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3059/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3060/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3061/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3062/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3063/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3064/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3065/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3066/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3067/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3068/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3069/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3070/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3071/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3072/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3073/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3074/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3075/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3076/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3077/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3078/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3079/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3080/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3081/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3082/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3083/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3084/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3085/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3086/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3087/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3088/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3089/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3090/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3091/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3092/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3093/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3094/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3095/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3096/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3097/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3098/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3099/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3100/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3101/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3102/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3103/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3104/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3105/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3106/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3107/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3108/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3109/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3110/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3111/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3112/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3113/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3114/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3115/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3116/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3117/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3118/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3119/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3120/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3121/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3122/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.0014\n","Epoch 3123/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3124/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3125/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3126/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3127/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3128/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3129/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3130/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3131/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3132/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 3133/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3134/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3135/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3136/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3137/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3138/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3139/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3140/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3141/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3142/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3143/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3144/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3145/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 3146/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3147/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3148/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3149/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3150/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3151/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3152/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3153/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3154/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3155/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3156/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3157/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3158/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3159/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 3160/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.0014\n","Epoch 3161/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3162/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3163/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3164/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3165/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3166/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3167/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3168/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3169/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3170/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3171/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3172/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3173/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3174/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3175/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3176/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3177/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3178/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3179/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3180/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3181/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3182/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3183/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3184/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3185/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3186/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3187/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3188/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3189/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3190/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3191/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3192/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3193/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3194/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3195/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3196/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3197/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3198/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3199/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3200/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3201/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3202/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3203/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3204/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3205/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.0014\n","Epoch 3206/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 0.0014\n","Epoch 3207/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3208/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3209/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3210/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3211/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3212/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3213/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3214/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3215/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3216/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3217/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3218/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3219/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3220/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3221/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3222/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3223/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3224/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3225/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3226/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3227/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3228/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3229/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3230/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3231/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3232/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3233/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3234/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3235/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3236/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3237/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3238/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3239/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3240/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3241/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3242/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3243/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3244/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3245/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3246/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3247/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3248/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3249/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3250/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3251/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3252/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3253/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3254/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3255/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3256/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3257/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3258/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3259/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3260/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3261/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3262/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3263/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3264/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3265/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3266/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3267/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3268/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3269/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3270/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3271/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3272/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3273/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3274/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3275/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3276/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3277/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3278/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3279/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3280/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3281/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3282/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3283/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3284/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3285/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3286/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3287/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3288/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3289/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3290/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3291/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3292/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3293/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3294/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3295/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3296/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3297/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3298/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3299/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3300/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3301/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3302/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 3303/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3304/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3305/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3306/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3307/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3308/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3309/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3310/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3311/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3312/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3313/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3314/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3315/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3316/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3317/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3318/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3319/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3320/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3321/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3322/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3323/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3324/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3325/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3326/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3327/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3328/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3329/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3330/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3331/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3332/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3333/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3334/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3335/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3336/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3337/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3338/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3339/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3340/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3341/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 3342/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3343/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3344/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3345/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3346/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3347/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3348/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3349/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3350/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3351/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3352/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3353/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3354/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3355/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3356/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3357/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3358/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3359/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3360/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3361/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3362/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3363/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3364/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3365/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3366/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3367/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3368/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3369/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3370/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 3371/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3372/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3373/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3374/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3375/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3376/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3377/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3378/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3379/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3380/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 3381/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3382/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3383/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3384/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3385/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3386/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3387/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3388/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3389/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3390/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3391/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3392/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3393/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3394/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3395/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3396/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3397/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3398/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3399/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3400/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3401/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3402/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3403/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3404/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3405/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3406/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3407/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3408/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3409/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3410/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3411/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3412/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3413/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3414/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3415/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3416/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3417/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3418/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3419/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3420/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3421/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3422/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3423/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3424/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3425/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3426/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3427/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 3428/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3429/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3430/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3431/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3432/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3433/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3434/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3435/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3436/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3437/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3438/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3439/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3440/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3441/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3442/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3443/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3444/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3445/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3446/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3447/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3448/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3449/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3450/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3451/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3452/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3453/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3454/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0277 - accuracy: 0.0014\n","Epoch 3455/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3456/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3457/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3458/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3459/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3460/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3461/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3462/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3463/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3464/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3465/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3466/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3467/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3468/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3469/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 3470/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3471/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3472/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3473/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3474/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3475/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3476/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3477/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3478/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3479/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3480/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3481/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3482/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3483/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3484/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3485/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3486/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3487/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3488/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3489/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3490/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3491/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3492/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3493/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3494/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3495/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3496/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3497/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3498/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3499/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3500/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3501/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3502/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3503/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 3504/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3505/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3506/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3507/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3508/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3509/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3510/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3511/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3512/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3513/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3514/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3515/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3516/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3517/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3518/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3519/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3520/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3521/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3522/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3523/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3524/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3525/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3526/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3527/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3528/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3529/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3530/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3531/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3532/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3533/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3534/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3535/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3536/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3537/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3538/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3539/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3540/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3541/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3542/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3543/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3544/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3545/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3546/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3547/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3548/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3549/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3550/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3551/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3552/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3553/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3554/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3555/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3556/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 3557/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3558/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3559/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3560/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3561/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3562/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3563/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3564/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3565/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3566/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3567/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3568/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3569/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3570/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3571/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3572/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3573/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3574/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3575/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3576/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3577/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3578/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3579/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3580/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3581/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3582/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3583/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3584/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3585/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3586/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3587/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3588/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3589/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 3590/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3591/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3592/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3593/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3594/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3595/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3596/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3597/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3598/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3599/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3600/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3601/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3602/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3603/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3604/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3605/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3606/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3607/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3608/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3609/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3610/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3611/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3612/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3613/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3614/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3615/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 3616/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3617/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3618/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3619/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3620/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3621/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3622/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3623/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3624/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3625/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3626/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3627/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3628/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3629/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3630/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3631/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3632/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3633/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3634/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3635/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3636/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3637/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3638/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3639/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3640/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3641/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3642/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3643/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3644/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3645/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3646/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3647/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3648/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3649/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3650/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3651/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3652/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3653/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3654/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3655/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3656/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3657/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3658/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3659/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3660/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3661/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3662/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3663/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3664/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3665/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3666/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3667/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3668/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3669/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3670/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3671/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3672/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3673/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3674/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3675/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3676/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3677/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3678/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3679/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3680/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3681/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3682/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3683/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3684/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3685/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3686/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3687/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3688/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3689/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3690/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3691/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3692/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3693/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3694/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3695/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3696/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3697/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3698/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3699/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3700/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3701/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3702/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3703/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3704/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.0014\n","Epoch 3705/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3706/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3707/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.0014\n","Epoch 3708/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3709/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3710/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3711/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3712/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3713/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3714/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3715/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3716/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3717/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3718/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3719/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3720/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3721/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3722/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3723/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3724/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3725/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3726/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3727/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3728/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3729/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3730/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3731/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3732/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3733/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3734/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3735/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3736/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3737/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3738/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3739/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3740/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3741/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3742/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3743/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3744/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3745/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3746/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3747/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3748/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3749/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 3750/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3751/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3752/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3753/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3754/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3755/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3756/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3757/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3758/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3759/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3760/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3761/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3762/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3763/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3764/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3765/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3766/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3767/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3768/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3769/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3770/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3771/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3772/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3773/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3774/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3775/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3776/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3777/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3778/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3779/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3780/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3781/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3782/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 3783/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3784/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3785/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3786/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3787/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3788/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3789/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3790/5000\n","23/23 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3791/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3792/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3793/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3794/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3795/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3796/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3797/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3798/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.0014\n","Epoch 3799/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3800/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3801/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3802/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3803/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3804/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3805/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3806/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3807/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3808/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3809/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3810/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3811/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3812/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3813/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3814/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3815/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3816/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3817/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3818/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3819/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 3820/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3821/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3822/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3823/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3824/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3825/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3826/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3827/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3828/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3829/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3830/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3831/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3832/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3833/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3834/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3835/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3836/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3837/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3838/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3839/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3840/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3841/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3842/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3843/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 3844/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3845/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3846/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3847/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3848/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3849/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3850/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3851/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3852/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3853/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3854/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3855/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3856/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3857/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3858/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3859/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3860/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3861/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3862/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3863/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3864/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3865/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3866/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3867/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3868/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3869/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3870/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 3871/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3872/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3873/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3874/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3875/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3876/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3877/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3878/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3879/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 3880/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3881/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3882/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3883/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3884/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3885/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 3886/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3887/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3888/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3889/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3890/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3891/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3892/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3893/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3894/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3895/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3896/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3897/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3898/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3899/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3900/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3901/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3902/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3903/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 3904/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3905/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3906/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3907/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3908/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3909/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3910/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3911/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3912/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3913/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3914/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3915/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3916/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3917/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3918/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3919/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3920/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3921/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3922/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 3923/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3924/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3925/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3926/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3927/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3928/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3929/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3930/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3931/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3932/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3933/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3934/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3935/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3936/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3937/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3938/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3939/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3940/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3941/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3942/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3943/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3944/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3945/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3946/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3947/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3948/5000\n","23/23 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3949/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3950/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3951/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3952/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3953/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3954/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3955/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 3956/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3957/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 3958/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 3959/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3960/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3961/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3962/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3963/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3964/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3965/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3966/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 3967/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3968/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3969/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3970/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3971/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3972/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3973/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3974/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3975/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3976/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3977/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3978/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 3979/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 3980/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3981/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3982/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3983/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3984/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3985/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3986/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3987/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3988/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 3989/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 3990/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3991/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3992/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 3993/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3994/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 3995/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3996/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 3997/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 3998/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 3999/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4000/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4001/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4002/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4003/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4004/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4005/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4006/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4007/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4008/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4009/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4010/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4011/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4012/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4013/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4014/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4015/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4016/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4017/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4018/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4019/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4020/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4021/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4022/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4023/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4024/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4025/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4026/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4027/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4028/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4029/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4030/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4031/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4032/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4033/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4034/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4035/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4036/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4037/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4038/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4039/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4040/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4041/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4042/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4043/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4044/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4045/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4046/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4047/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4048/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4049/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4050/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4051/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4052/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4053/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4054/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4055/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4056/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4057/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4058/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4059/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4060/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4061/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4062/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4063/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4064/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4065/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4066/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4067/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4068/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4069/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4070/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4071/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4072/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4073/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4074/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4075/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4076/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4077/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4078/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4079/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4080/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4081/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4082/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4083/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4084/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4085/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4086/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4087/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4088/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4089/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4090/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4091/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4092/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4093/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4094/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4095/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4096/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4097/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4098/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4099/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4100/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4101/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4102/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4103/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4104/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4105/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4106/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4107/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4108/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4109/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4110/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4111/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4112/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4113/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4114/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4115/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4116/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4117/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4118/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4119/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4120/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4121/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4122/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4123/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4124/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4125/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4126/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4127/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4128/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4129/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4130/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4131/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4132/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4133/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4134/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4135/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4136/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4137/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4138/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4139/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4140/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4141/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4142/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4143/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4144/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4145/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4146/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4147/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4148/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4149/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4150/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 4151/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4152/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4153/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4154/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4155/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4156/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4157/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4158/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4159/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4160/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4161/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4162/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4163/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4164/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4165/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4166/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4167/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4168/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4169/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4170/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4171/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4172/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4173/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4174/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4175/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4176/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4177/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4178/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4179/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4180/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4181/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4182/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4183/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4184/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4185/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4186/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4187/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4188/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4189/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4190/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4191/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4192/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4193/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4194/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4195/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4196/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4197/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4198/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4199/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4200/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4201/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4202/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4203/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4204/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4205/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4206/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4207/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4208/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.0014\n","Epoch 4209/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.0014\n","Epoch 4210/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4211/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4212/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4213/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4214/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4215/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4216/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4217/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4218/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4219/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4220/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4221/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4222/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4223/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4224/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4225/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4226/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4227/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4228/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4229/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4230/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4231/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4232/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4233/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4234/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4235/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4236/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4237/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4238/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4239/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4240/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4241/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4242/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4243/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4244/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4245/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4246/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4247/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4248/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4249/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4250/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4251/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4252/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4253/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4254/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4255/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4256/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4257/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4258/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4259/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4260/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4261/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4262/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4263/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4264/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4265/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4266/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4267/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4268/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4269/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4270/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4271/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4272/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4273/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4274/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4275/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4276/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4277/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4278/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4279/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4280/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 4281/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4282/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4283/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4284/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4285/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4286/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4287/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4288/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4289/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4290/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4291/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4292/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4293/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4294/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4295/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4296/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4297/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4298/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4299/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4300/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4301/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4302/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4303/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4304/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4305/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4306/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4307/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4308/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4309/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4310/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4311/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4312/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4313/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 4314/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4315/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4316/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4317/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4318/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4319/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4320/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4321/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4322/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4323/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4324/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4325/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4326/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4327/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4328/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4329/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4330/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4331/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4332/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4333/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4334/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4335/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4336/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4337/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4338/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4339/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4340/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4341/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4342/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4343/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4344/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4345/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4346/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4347/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4348/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4349/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4350/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4351/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4352/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4353/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4354/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4355/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4356/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4357/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4358/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4359/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4360/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4361/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4362/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4363/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4364/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4365/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4366/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4367/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4368/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4369/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4370/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4371/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4372/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4373/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4374/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4375/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4376/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4377/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4378/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4379/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4380/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4381/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4382/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4383/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4384/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4385/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4386/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4387/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4388/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4389/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4390/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4391/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4392/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4393/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4394/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4395/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4396/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4397/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4398/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4399/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4400/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4401/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4402/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4403/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4404/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4405/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4406/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4407/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4408/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4409/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4410/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4411/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4412/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4413/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4414/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 4415/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4416/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4417/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4418/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4419/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4420/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4421/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4422/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4423/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4424/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4425/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4426/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4427/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4428/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4429/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4430/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4431/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4432/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4433/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4434/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4435/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4436/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4437/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4438/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4439/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4440/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4441/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4442/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4443/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.0014\n","Epoch 4444/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4445/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4446/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4447/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4448/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4449/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4450/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4451/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4452/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4453/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4454/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4455/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4456/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4457/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.0014\n","Epoch 4458/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4459/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4460/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4461/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4462/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4463/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4464/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4465/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4466/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4467/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4468/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4469/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4470/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4471/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4472/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4473/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4474/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4475/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4476/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4477/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4478/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4479/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 4480/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4481/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4482/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4483/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4484/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4485/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4486/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4487/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4488/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4489/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4490/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4491/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4492/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4493/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4494/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4495/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4496/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4497/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4498/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4499/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4500/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4501/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4502/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4503/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4504/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4505/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4506/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4507/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4508/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4509/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4510/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4511/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4512/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4513/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4514/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4515/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4516/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4517/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4518/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4519/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4520/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4521/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4522/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4523/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4524/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4525/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4526/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4527/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4528/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4529/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4530/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4531/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4532/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4533/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4534/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4535/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4536/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4537/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4538/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4539/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4540/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4541/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4542/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4543/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4544/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4545/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4546/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4547/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4548/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4549/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4550/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4551/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4552/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4553/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4554/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4555/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4556/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4557/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4558/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4559/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4560/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4561/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4562/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4563/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4564/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4565/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4566/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4567/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4568/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4569/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4570/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4571/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4572/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4573/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4574/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4575/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4576/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4577/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4578/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4579/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4580/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4581/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4582/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4583/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 4584/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4585/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4586/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4587/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4588/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4589/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4590/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4591/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4592/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4593/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4594/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4595/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4596/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4597/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4598/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4599/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4600/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4601/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4602/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4603/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4604/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4605/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4606/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4607/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4608/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4609/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4610/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4611/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4612/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4613/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4614/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4615/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4616/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4617/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4618/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4619/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4620/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4621/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4622/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4623/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4624/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4625/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4626/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4627/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4628/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4629/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4630/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4631/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4632/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4633/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4634/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4635/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4636/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4637/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4638/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4639/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4640/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4641/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4642/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4643/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4644/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4645/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4646/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4647/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4648/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4649/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4650/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4651/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4652/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4653/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4654/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4655/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4656/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4657/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4658/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4659/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4660/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4661/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4662/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4663/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4664/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4665/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4666/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4667/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4668/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4669/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4670/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4671/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4672/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4673/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4674/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4675/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4676/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4677/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4678/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4679/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4680/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4681/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4682/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4683/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4684/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4685/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4686/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4687/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4688/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4689/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4690/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4691/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4692/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4693/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4694/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4695/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4696/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4697/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4698/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0014\n","Epoch 4699/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4700/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4701/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4702/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4703/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0014\n","Epoch 4704/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4705/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4706/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4707/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4708/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4709/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4710/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4711/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4712/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4713/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4714/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4715/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4716/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4717/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4718/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4719/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4720/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4721/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4722/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4723/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4724/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4725/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4726/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4727/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4728/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4729/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4730/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4731/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4732/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4733/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4734/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4735/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4736/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4737/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4738/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4739/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4740/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4741/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4742/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4743/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4744/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4745/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4746/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4747/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4748/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4749/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4750/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4751/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4752/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4753/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 4754/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 4755/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4756/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4757/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4758/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4759/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4760/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4761/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4762/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4763/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4764/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4765/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4766/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4767/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4768/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4769/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4770/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4771/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4772/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4773/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4774/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4775/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4776/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4777/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4778/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4779/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4780/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4781/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4782/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4783/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4784/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4785/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4786/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4787/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4788/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4789/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4790/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4791/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4792/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4793/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4794/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4795/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4796/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4797/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4798/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4799/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4800/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4801/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4802/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4803/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4804/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4805/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4806/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4807/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4808/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4809/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4810/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4811/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4812/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4813/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4814/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4815/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4816/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4817/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4818/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4819/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4820/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4821/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4822/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4823/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4824/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4825/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4826/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4827/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4828/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4829/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4830/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4831/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4832/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4833/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4834/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4835/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4836/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4837/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4838/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4839/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4840/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4841/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4842/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 4843/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4844/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0014\n","Epoch 4845/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4846/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4847/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4848/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4849/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4850/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4851/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4852/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4853/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4854/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4855/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4856/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4857/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4858/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4859/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4860/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4861/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4862/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4863/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4864/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4865/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4866/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4867/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4868/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4869/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4870/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4871/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4872/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4873/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4874/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4875/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4876/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4877/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4878/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4879/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4880/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4881/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4882/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4883/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4884/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4885/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4886/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4887/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4888/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4889/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4890/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4891/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4892/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4893/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4894/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4895/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4896/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4897/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4898/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4899/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4900/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4901/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4902/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4903/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4904/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4905/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4906/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4907/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4908/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4909/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4910/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4911/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4912/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4913/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4914/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4915/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4916/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4917/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4918/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4919/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4920/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4921/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4922/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4923/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4924/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4925/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4926/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4927/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4928/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4929/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4930/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4931/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4932/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4933/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4934/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4935/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4936/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0014\n","Epoch 4937/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4938/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4939/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4940/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4941/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4942/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4943/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4944/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0277 - accuracy: 0.0014\n","Epoch 4945/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4946/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4947/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0014\n","Epoch 4948/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4949/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4950/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4951/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4952/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4953/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4954/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.0014\n","Epoch 4955/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0014\n","Epoch 4956/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4957/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4958/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4959/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4960/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4961/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4962/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4963/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4964/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4965/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4966/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4967/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4968/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4969/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4970/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4971/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4972/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4973/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4974/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 0.0014\n","Epoch 4975/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4976/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4977/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4978/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4979/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4980/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0014\n","Epoch 4981/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4982/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0014\n","Epoch 4983/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.0014\n","Epoch 4984/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0014\n","Epoch 4985/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4986/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4987/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4988/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4989/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4990/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4991/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4992/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4993/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4994/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 4995/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0014\n","Epoch 4996/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0014\n","Epoch 4997/5000\n","23/23 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0014\n","Epoch 4998/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0014\n","Epoch 4999/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n","Epoch 5000/5000\n","23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.0014\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f9312159190>"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["### **VALIDACION DEL MODELO MLP**"],"metadata":{"id":"9iF0kHJmO5GM"}},{"cell_type":"code","source":["RMSE = (np.sqrt(mean_squared_error(Y_test, (model_RL_1.predict(X_test)))))\n","regr = LinearRegression()\n","regr.fit(Y_test,(model_RL_1.predict(X_test)))\n","coef = regr.coef_\n","\n","print((model_RL_1.predict(X_train))[0])\n","print(RMSE)\n","print(coef)"],"metadata":{"id":"-MdpPqSEZTXt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239879144,"user_tz":300,"elapsed":517,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"c06f0930-747e-4136-ed32-6663f10e4723"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.04086182]\n","0.20521056622445075\n","[[0.66874165]]\n"]}]},{"cell_type":"markdown","source":["## **MODELO A PARTIR DE MLP SKLEARN**"],"metadata":{"id":"osjvcV90w8Gg"}},{"cell_type":"markdown","source":["### **DIVISION DEL DATASET Y PREPROCESAMIENTO DE LOS DATOS**"],"metadata":{"id":"E8HsRkWMxuqZ"}},{"cell_type":"code","source":["X = datos.iloc[:,0:6]\n","Y = datos.iloc[:,6]\n","\n","scaler1 = MinMaxScaler(feature_range=(-1, 1))\n","scaler1.fit(X)\n","X_n=scaler1.transform(X)\n","X_n = np.array(X_n)\n","\n","scaler2 = MinMaxScaler(feature_range=(-1, 1))\n","Y_n = Y.values\n","Y_n = Y_n.reshape(-1,1)\n","\n","scaler2.fit(Y_n)\n","Y_n=scaler2.transform(Y_n)\n","Y_n = np.array(Y_n)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X_n, Y_n, test_size = 0.2, random_state=5)"],"metadata":{"id":"AkoL3lmPxmmJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **CREACION DEL MODELO DE SKLEARN**"],"metadata":{"id":"XxPTz4umxGVJ"}},{"cell_type":"code","source":["from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import QuantileTransformer\n","from sklearn.neural_network import MLPRegressor\n","import time\n","\n","print(\"Training MLPRegressor...\")\n","model_skl = make_pipeline(\n","    QuantileTransformer(),\n","    MLPRegressor(\n","        hidden_layer_sizes=(10, 8,5),\n","        learning_rate_init=0.01,\n","        early_stopping=True,\n","        random_state=0,\n","    ),\n",")"],"metadata":{"id":"3a0YCYp_6bup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664239308957,"user_tz":300,"elapsed":5,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"e6490757-9ae3-42bb-99eb-b99b11aa645f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training MLPRegressor...\n"]}]},{"cell_type":"markdown","source":["### **ENTRENAMIENTO DEL MODELO**"],"metadata":{"id":"SWERAjrwxKjI"}},{"cell_type":"code","source":["model_skl.fit(X_train, Y_train)\n","y_pred_skl_MLP = model_skl.predict(X_test)\n","print(y_pred_skl_MLP.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K6HH-xeoxCQq","executionInfo":{"status":"ok","timestamp":1664239309186,"user_tz":300,"elapsed":232,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"2c34cda5-cf89-45e2-8c25-1aa2b4a8d57a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(182,)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py:2593: UserWarning: n_quantiles (1000) is greater than the total number of samples (726). n_quantiles is set to n_samples.\n","  \"n_samples.\" % (self.n_quantiles, n_samples)\n","/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1599: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]}]},{"cell_type":"markdown","source":["### **VALIDACION DEL MODELO DE SKLEARN**"],"metadata":{"id":"FkVNfNTWySZb"}},{"cell_type":"code","source":["regr = LinearRegression()\n","regr.fit(Y_test,y_pred_skl_MLP)\n","coef = regr.coef_\n","print(coef)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnS_gC4FxQNQ","executionInfo":{"status":"ok","timestamp":1664239309187,"user_tz":300,"elapsed":8,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"2ccd92b6-3996-4b21-959c-ec49788cd355"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.59506617]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"dA-OY0FaxBih"}}]}