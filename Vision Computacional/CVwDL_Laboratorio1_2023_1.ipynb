{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qKQcWPsvonK1"},"source":["# <font color='blue'>**LABORATORIO 1. Visión computacional Tradicional y CNN´s aplicadas a problemas de clasificación de imágenes**\n","\n","<font color='red'>Integrantes (Máximo 3 personas):\n","\n","<font color='red'>Nombres y Apellidos\n","\n","<font color='red'>Codigo\n","\n","Partiendo de la idea general que asegura que una arquitectura de red neuronal que trabaja bien para un problema de visión computacional trabajará bien para cualquier otro problema en la misma area, en este laboratorio, usted podrá basarse en alguna de estas arquitecturas (i.e, LeNet, VGG, AlexNet), para crear una CNN que le permita clasificar los digitos obtenidos del lenguajes de señas.\n","\n","Usted deberá seguir las instrucciones (<font color='red'>resaltadas en rojo </font>), las cuales además de guiarlo, generarán en suma el puntaje final obtenido para este laboratorio.\n","\n","Cabe resaltar que a través de este documento se estará promoviendo el uso de algunas funciones propias de Keras, ya que esta es una excelente API que permite reducir considerablemente los tiempos de experimentación.\n","\n"]},{"cell_type":"code","metadata":{"id":"A4a2HzpMM27y","executionInfo":{"status":"ok","timestamp":1677700765182,"user_tz":300,"elapsed":6,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}}},"source":["#Para dar un orden al trabajo realizado, coloque aqui todas las librerias que van a ser usadas. \n","import numpy as np\n","from matplotlib import pyplot as plt\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"AXofP_DqrIzi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677700768581,"user_tz":300,"elapsed":1905,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"96200962-fe4c-4362-86cb-6b96443aef51"},"source":["# Vamos a usar el Sign Language Digits Dataset\n","#By Turkey Ankara Ayrancı Anadolu High School Students.\n","\n","!git clone https://github.com/ardamavi/Sign-Language-Digits-Dataset.git\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Sign-Language-Digits-Dataset'...\n","remote: Enumerating objects: 2095, done.\u001b[K\n","remote: Counting objects: 100% (6/6), done.\u001b[K\n","remote: Compressing objects: 100% (6/6), done.\u001b[K\n","remote: Total 2095 (delta 2), reused 0 (delta 0), pack-reused 2089\u001b[K\n","Receiving objects: 100% (2095/2095), 15.07 MiB | 22.66 MiB/s, done.\n","Resolving deltas: 100% (660/660), done.\n"]}]},{"cell_type":"code","metadata":{"id":"SrxiJZFxyLt3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677700793213,"user_tz":300,"elapsed":295,"user":{"displayName":"JOSE DAVID SANTACRUZ GUERRERO","userId":"13056226680869022468"}},"outputId":"98af50e8-7a16-4b79-f954-57663c85a68b"},"source":["#Esta parte esta hecha para ustedes. \n","#Se les pide que intenten seguir el código y entender\n","#lo que se hace y el porque se hace. Que tan importante es usar pathlib en este caso?\n","\n","import os\n","import pathlib\n","sld_dir = pathlib.Path('Sign-Language-Digits-Dataset/Dataset/')\n","print(len(list(sld_dir.glob('*/*.JPG'))))\n","\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2062\n"]}]},{"cell_type":"markdown","metadata":{"id":"H8pow6p77Eaj"},"source":["<font color='red'>Punto 1 (0.25). Lea la ayuda de la utilidad llamada **preprocessing.image_dataset_from_directory** de keras y utilicela para crear su dataset </font>. \n","\n","Lea atentamente que parametros usted puede ingresarle a la función y tambien que es lo que la misma retorna.\n","\n","Use el 80% de las imagenes para entrenamiento y el 20% para validación. \n","\n","Cree los dos conjuntos: entrenamiento y validación.\n","\n"]},{"cell_type":"code","metadata":{"id":"8a-tAM7v7kb0"},"source":["#Pegue aquí el código para crear  el dataset de entrenamiento\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26-95aO59heO"},"source":["#Pegue aquí el código para crear  el dataset de validación\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nQfPAC90-XjM"},"source":["<font color='red'>Punto 2 (0.25). Observando en que consiste el conjunto de datos </font>. \n","\n","Los dos conjuntos de datos creados en el punto 1, tienen una estructura especifíca para el manejo de las imagenes y las etiquetas.\n","\n","Tome como base el código que se hizo en clase para mostrar las primeras 5 imagenes del dataset de MNIST y modifiquelo, para mostrar 9 imágenes del dataset de entrenamiento (cada una de las imagenes debe presentar su etiqueta en la parte superior).\n","\n","A continuación algunos Tips:\n","\n","\n","*   Suponiendo que su conjunto de entrenamiento se llame: ***sld_train***. Una forma de saber los nombres de las clases es: **nombres_clases = sld_train.class_names**\n","\n","*   Si usted hace un ciclo for usando como rango por ejemplo el ***sld_train*** este devuelve dos arreglos: imagenes y etiquetas\n","\n","*   Tenga en cuenta que para poder visualizar este tipo de imágenes  usted debe convertirlas, por ejemplo: **imagen[i].numpy().astype('uint8')**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"d-wcE9iG97YA"},"source":["#Pegue aquí su código para visualizar imágenes\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jTAcGVtCWDM"},"source":["<font color='red'>Punto 3 (0.25). Lea la ayuda de la utilidad llamada **experimental.preprocessing.Rescaling** de keras y utilicela para normalizar sus conjuntos de datos</font>. \n","\n","Lea atentamente que parametros usted puede ingresarle a la función y tambien que es lo que la misma retorna.\n","\n","Aunque no es algo imprescindible, si usted desea puede iterar sobre el conjunto de datos normalizados e irlos guardando en las variables x_train, y_train, x_test y y_test usadas en clase. Para lo que podría necesitar una línea de código parecida a esto: **x_train, y_train = next(iter(dato_normalizado))**"]},{"cell_type":"code","metadata":{"id":"MBZqLgn2IeGg"},"source":["#Pegue aquí su código para normalizar los datos de entrenamiento"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='red'>Punto 4 (1.5). Cree un clasificador tradicional para el dataset dado. \n","\n","Utilice los conocimientos adquiridos en clase acerca de visión computacional tradicional: detectores/descriptores, BoVW y ANN´s para que cree, entrene y valide un CLASIFICADOR TRADICIONAL.\n","Recuerde obtener métricas que pueda compara con el clasificador CNN.\n"],"metadata":{"id":"gKt1YXfVlg8x"}},{"cell_type":"code","metadata":{"id":"7yXaGpX5IkyX"},"source":["#Pegue aquí su código del entrenamiento, validar y medir su clasificador tradicional"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1G-Vpw2q8I9I"},"source":["<font color='red'>Punto 5 (0.25). Creando su arquitectura básica de CNN</font>. \n","\n","Cree una arquitectura basado en alguno de los modelos propuestos. Para esta parte su modelo debe estar completo (todas las capas convolucionales, pooling y fully conected), pero no deben tener ningún tipo de regularización.\n","\n","De una explicación corta acerca (No más de 10 líneas) de la arquitectura usada y el porqué de su selección.\n","\n","Muestre al final el resumen de su modelo y escoja el método de optimización que va a ser usado (este puede ser modificado en el transcurso de los entrenamientos, si usted lo considera necesario)"]},{"cell_type":"code","metadata":{"id":"DqQ4U_A1A7Xs"},"source":["#Realice aqui su modelo de CNN\n","\n","modelo.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zy7WmWB2H4vQ"},"source":["optimo= ¿?\n","modelo.compile(optimizer=optimo, loss='¿?',metrics=['accuracy'] )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gtyC8FXaFAPF"},"source":["<font color='red'>Punto *6* (0.5). Entrenando y validando su modelo CNN</font>. \n","\n","Entrene su modelo básico, obtenga los gráficos de accuracy y loss y saque sus propias conclusiones.\n","\n"]},{"cell_type":"code","metadata":{"id":"deTdg45iI_Qk"},"source":["#Muestre todo el proceso aquí"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mo-zAhZqJFIF"},"source":["#Muestre las gráficas y métricas aquí"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v89NXu9YFkgw"},"source":["<font color='red'>Punto **7** (1.0). Mejorando su CNN</font>. \n","\n","Haga uso de los metodos de regularización que usted considere convenientes (puede ser más de uno), de tal forma que consiga reducir considerablemente el overfitting.\n","\n","Usted puede entrenar la red las veces que usted desee cambiando los metodos de regularización, pero asegurese de dejar plasmados en este trabajo dos modelos diferentes (por lógica deben ser los dos mejores)\n","\n","Realice una tabla comparativa mostrando las diferencias entre su modelo CNN base, los dos modelos con regularización y el modelo tradicional."]},{"cell_type":"markdown","metadata":{"id":"oTE9zYBvJy9P"},"source":["<font color='red'>Punto 8 (1.0). Pruebas adicionales de su mejor Clasificador</font>. \n","\n","Realice un conjunto de 20 imagenes, mostrando diferentes digitos del lenguaje de señas. Realice un programa que permita leer las imagenes, pre-procesarlas y dejarlas listas para servir de entrada a su mejor clasificador.  Muestre las imagenes procesadas y el valor de predicción de su red con porcentaje. "]}]}